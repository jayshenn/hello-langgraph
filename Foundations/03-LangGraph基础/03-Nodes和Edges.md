# Nodes å’Œ Edgesï¼šLangGraph çš„æ„å»ºè¦ç´ 

> ğŸ¯ **å­¦ä¹ ç›®æ ‡**ï¼šæŒæ¡èŠ‚ç‚¹å’Œè¾¹çš„ä½¿ç”¨æ–¹æ³•ï¼Œç†è§£å›¾çš„æ„å»ºè§„åˆ™å’Œæ•°æ®æµæ¨¡å¼

## ğŸ”µ Nodesï¼ˆèŠ‚ç‚¹ï¼‰ï¼šå›¾çš„å¤„ç†å•å…ƒ

### èŠ‚ç‚¹çš„æœ¬è´¨

åœ¨ LangGraph ä¸­ï¼Œ**èŠ‚ç‚¹ï¼ˆNodeï¼‰** æ˜¯å›¾çš„åŸºæœ¬å¤„ç†å•å…ƒï¼Œæ¯ä¸ªèŠ‚ç‚¹ï¼š
- æ¥æ”¶å½“å‰çŠ¶æ€ä½œä¸ºè¾“å…¥
- æ‰§è¡Œç‰¹å®šçš„ä¸šåŠ¡é€»è¾‘
- è¿”å›æ›´æ–°åçš„çŠ¶æ€

```python
from typing import TypedDict

class ChatState(TypedDict):
    user_input: str
    response: str
    confidence: float

# æ ‡å‡†èŠ‚ç‚¹å‡½æ•°ç­¾å
def my_node(state: ChatState) -> ChatState:
    """
    èŠ‚ç‚¹å‡½æ•°çš„æ ‡å‡†æ ¼å¼ï¼š
    - è¾“å…¥ï¼šå½“å‰çŠ¶æ€
    - è¾“å‡ºï¼šæ›´æ–°åçš„çŠ¶æ€
    """
    # ä¸šåŠ¡é€»è¾‘å¤„ç†
    processed_input = process_input(state["user_input"])

    # è¿”å›æ›´æ–°åçš„çŠ¶æ€
    return {
        **state,  # ä¿æŒå…¶ä»–å­—æ®µä¸å˜
        "response": processed_input,
        "confidence": 0.85
    }
```

### èŠ‚ç‚¹å‡½æ•°çš„è§„èŒƒ

#### 1. çº¯å‡½æ•°ç‰¹æ€§

```python
# âœ… å¥½çš„èŠ‚ç‚¹ï¼šçº¯å‡½æ•°ï¼Œå¯é¢„æµ‹
def good_node(state: ChatState) -> ChatState:
    # ä¸ä¾èµ–å¤–éƒ¨çŠ¶æ€
    result = analyze_text(state["user_input"])

    return {
        **state,
        "analysis_result": result
    }

# âŒ ä¸å¥½çš„èŠ‚ç‚¹ï¼šæœ‰å‰¯ä½œç”¨
global_counter = 0

def bad_node(state: ChatState) -> ChatState:
    global global_counter
    global_counter += 1  # å‰¯ä½œç”¨ï¼šä¿®æ”¹å…¨å±€çŠ¶æ€

    # ä¾èµ–å¤–éƒ¨å¯å˜çŠ¶æ€
    result = f"å¤„ç†æ¬¡æ•°: {global_counter}"

    return {**state, "result": result}
```

#### 2. é”™è¯¯å¤„ç†

```python
def robust_node(state: ChatState) -> ChatState:
    """å¥å£®çš„èŠ‚ç‚¹å®ç°"""
    try:
        # ä¸»è¦ä¸šåŠ¡é€»è¾‘
        result = risky_operation(state["user_input"])

        return {
            **state,
            "result": result,
            "error": None,
            "status": "success"
        }

    except ValueError as e:
        # ä¸šåŠ¡é€»è¾‘é”™è¯¯
        return {
            **state,
            "error": f"è¾“å…¥é”™è¯¯: {str(e)}",
            "status": "input_error"
        }

    except Exception as e:
        # ç³»ç»Ÿé”™è¯¯
        return {
            **state,
            "error": f"ç³»ç»Ÿé”™è¯¯: {str(e)}",
            "status": "system_error"
        }
```

#### 3. æ¡ä»¶å¤„ç†

```python
def conditional_node(state: ChatState) -> ChatState:
    """æ ¹æ®çŠ¶æ€æ¡ä»¶è¿›è¡Œä¸åŒå¤„ç†"""
    user_input = state["user_input"]

    if not user_input or not user_input.strip():
        return {
            **state,
            "error": "è¾“å…¥ä¸èƒ½ä¸ºç©º",
            "needs_input": True
        }

    if len(user_input) > 1000:
        # è¾“å…¥è¿‡é•¿ï¼Œéœ€è¦æˆªæ–­
        return {
            **state,
            "user_input": user_input[:1000],
            "truncated": True,
            "original_length": len(user_input)
        }

    # æ­£å¸¸å¤„ç†
    return {
        **state,
        "processed": True,
        "processing_time": time.time()
    }
```

### ç‰¹æ®ŠèŠ‚ç‚¹ç±»å‹

#### 1. å·¥å…·è°ƒç”¨èŠ‚ç‚¹

```python
from langchain.tools import Tool

def tool_calling_node(state: ChatState) -> ChatState:
    """è°ƒç”¨å¤–éƒ¨å·¥å…·çš„èŠ‚ç‚¹"""
    user_query = state["user_input"]

    # åˆ¤æ–­æ˜¯å¦éœ€è¦ä½¿ç”¨å·¥å…·
    if "å¤©æ°”" in user_query:
        # è°ƒç”¨å¤©æ°”å·¥å…·
        weather_result = weather_tool.run(user_query)
        return {
            **state,
            "tool_result": weather_result,
            "tool_used": "weather",
            "needs_response_generation": True
        }

    elif "æœç´¢" in user_query:
        # è°ƒç”¨æœç´¢å·¥å…·
        search_result = search_tool.run(user_query)
        return {
            **state,
            "tool_result": search_result,
            "tool_used": "search",
            "needs_response_generation": True
        }

    else:
        # ä¸éœ€è¦å·¥å…·
        return {
            **state,
            "tool_result": None,
            "tool_used": None,
            "needs_response_generation": True
        }
```

#### 2. LLM è°ƒç”¨èŠ‚ç‚¹

```python
from langchain_openai import ChatOpenAI

def llm_node(state: ChatState) -> ChatState:
    """è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹çš„èŠ‚ç‚¹"""
    llm = ChatOpenAI(model="gpt-4")

    # æ„å»ºæç¤º
    messages = build_messages(state)

    # è°ƒç”¨ LLM
    response = llm.invoke(messages)

    return {
        **state,
        "llm_response": response.content,
        "tokens_used": response.usage_metadata["total_tokens"] if response.usage_metadata else 0
    }

def build_messages(state: ChatState) -> list:
    """æ„å»ºæ¶ˆæ¯åˆ—è¡¨"""
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹"}
    ]

    # æ·»åŠ å†å²å¯¹è¯
    for msg in state.get("conversation_history", []):
        messages.append(msg)

    # æ·»åŠ å½“å‰ç”¨æˆ·è¾“å…¥
    messages.append({"role": "user", "content": state["user_input"]})

    return messages
```

#### 3. æ•°æ®å¤„ç†èŠ‚ç‚¹

```python
def data_processing_node(state: ChatState) -> ChatState:
    """æ•°æ®å¤„ç†å’Œè½¬æ¢èŠ‚ç‚¹"""
    raw_data = state.get("raw_data", [])

    # æ•°æ®æ¸…æ´—
    cleaned_data = [
        item for item in raw_data
        if item and isinstance(item, str) and len(item.strip()) > 0
    ]

    # æ•°æ®è½¬æ¢
    processed_data = [
        {
            "original": item,
            "cleaned": item.strip().lower(),
            "length": len(item),
            "word_count": len(item.split())
        }
        for item in cleaned_data
    ]

    # æ•°æ®ç»Ÿè®¡
    statistics = {
        "total_items": len(processed_data),
        "avg_length": sum(item["length"] for item in processed_data) / len(processed_data) if processed_data else 0,
        "total_words": sum(item["word_count"] for item in processed_data)
    }

    return {
        **state,
        "processed_data": processed_data,
        "data_statistics": statistics,
        "processing_completed": True
    }
```

## ğŸ”— Edgesï¼ˆè¾¹ï¼‰ï¼šå›¾çš„æ§åˆ¶æµ

### è¾¹çš„ç±»å‹

#### 1. æ™®é€šè¾¹ï¼ˆRegular Edgesï¼‰

```python
from langgraph.graph import StateGraph

# åˆ›å»ºå›¾
graph = StateGraph(ChatState)

# æ·»åŠ èŠ‚ç‚¹
graph.add_node("input_processing", process_input_node)
graph.add_node("llm_call", llm_node)
graph.add_node("output_formatting", format_output_node)

# æ·»åŠ æ™®é€šè¾¹ï¼šå›ºå®šçš„æ‰§è¡Œé¡ºåº
graph.add_edge("input_processing", "llm_call")      # input_processing -> llm_call
graph.add_edge("llm_call", "output_formatting")    # llm_call -> output_formatting
```

#### 2. æ¡ä»¶è¾¹ï¼ˆConditional Edgesï¼‰

```python
def route_by_input_type(state: ChatState) -> str:
    """è·¯ç”±å‡½æ•°ï¼šæ ¹æ®çŠ¶æ€å†³å®šä¸‹ä¸€ä¸ªèŠ‚ç‚¹"""
    user_input = state["user_input"].lower()

    if "æœç´¢" in user_input or "æŸ¥æ‰¾" in user_input:
        return "search_node"
    elif "å¤©æ°”" in user_input:
        return "weather_node"
    elif "è®¡ç®—" in user_input or "æ•°å­¦" in user_input:
        return "calculation_node"
    else:
        return "general_chat_node"

# æ·»åŠ æ¡ä»¶è¾¹
graph.add_conditional_edges(
    "input_analysis",           # æºèŠ‚ç‚¹
    route_by_input_type,        # è·¯ç”±å‡½æ•°
    {
        "search_node": "search_node",
        "weather_node": "weather_node",
        "calculation_node": "calculation_node",
        "general_chat_node": "general_chat_node"
    }
)
```

#### 3. åŠ¨æ€è¾¹ï¼ˆSend APIï¼‰

```python
from langgraph.constants import Send

def fan_out_processing(state: ChatState) -> list[Send]:
    """åŠ¨æ€åˆ›å»ºå¤šä¸ªå¹¶è¡Œä»»åŠ¡"""
    queries = state.get("parallel_queries", [])
    sends = []

    for i, query in enumerate(queries):
        # ä¸ºæ¯ä¸ªæŸ¥è¯¢åˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„å¤„ç†ä»»åŠ¡
        sends.append(
            Send("process_single_query", {
                "query": query,
                "query_id": i,
                "timestamp": time.time()
            })
        )

    return sends

def process_single_query(state: dict) -> ChatState:
    """å¤„ç†å•ä¸ªæŸ¥è¯¢"""
    query = state["query"]
    query_id = state["query_id"]

    result = process_query(query)

    return {
        "parallel_results": [{
            "query_id": query_id,
            "query": query,
            "result": result,
            "processed_at": time.time()
        }]
    }

# æ·»åŠ åŠ¨æ€è¾¹
graph.add_conditional_edges("fan_out", fan_out_processing)
graph.add_node("process_single_query", process_single_query)
```

### å¤æ‚è·¯ç”±æ¨¡å¼

#### 1. å¤šæ¡ä»¶è·¯ç”±

```python
def complex_router(state: ChatState) -> str:
    """å¤æ‚çš„è·¯ç”±é€»è¾‘"""
    user_input = state["user_input"]
    user_history = state.get("conversation_history", [])
    user_preferences = state.get("user_preferences", {})

    # æ¡ä»¶ 1ï¼šç´§æ€¥æƒ…å†µæ£€æµ‹
    if any(keyword in user_input.lower() for keyword in ["ç´§æ€¥", "æ€¥", "help", "sos"]):
        return "emergency_handler"

    # æ¡ä»¶ 2ï¼šåŸºäºå†å²å¯¹è¯çš„ä¸Šä¸‹æ–‡
    if len(user_history) > 0 and "ç»§ç»­" in user_input:
        last_topic = extract_topic(user_history[-1])
        if last_topic:
            return f"continue_{last_topic}"

    # æ¡ä»¶ 3ï¼šåŸºäºç”¨æˆ·åå¥½
    preferred_style = user_preferences.get("response_style", "standard")
    if preferred_style == "detailed":
        return "detailed_response"
    elif preferred_style == "concise":
        return "concise_response"

    # æ¡ä»¶ 4ï¼šå†…å®¹ç±»å‹æ£€æµ‹
    content_type = detect_content_type(user_input)
    if content_type == "question":
        return "qa_handler"
    elif content_type == "request":
        return "request_handler"

    # é»˜è®¤è·¯ç”±
    return "general_handler"

graph.add_conditional_edges(
    "input_analysis",
    complex_router,
    {
        "emergency_handler": "emergency_handler",
        "continue_research": "research_continuation",
        "continue_coding": "coding_continuation",
        "detailed_response": "detailed_processor",
        "concise_response": "concise_processor",
        "qa_handler": "question_answering",
        "request_handler": "request_processing",
        "general_handler": "general_processing"
    }
)
```

#### 2. å¾ªç¯å’Œé‡è¯•é€»è¾‘

```python
def retry_logic(state: ChatState) -> str:
    """é‡è¯•é€»è¾‘è·¯ç”±"""
    error_count = state.get("error_count", 0)
    max_retries = state.get("max_retries", 3)
    last_error = state.get("last_error")

    # å¦‚æœæ²¡æœ‰é”™è¯¯ï¼Œç»§ç»­æ­£å¸¸æµç¨‹
    if not last_error:
        return "next_step"

    # å¦‚æœé‡è¯•æ¬¡æ•°å·²è¾¾ä¸Šé™ï¼Œè¿›å…¥é”™è¯¯å¤„ç†
    if error_count >= max_retries:
        return "error_handler"

    # æ ¹æ®é”™è¯¯ç±»å‹å†³å®šé‡è¯•ç­–ç•¥
    if "network" in last_error.lower():
        return "network_retry"
    elif "rate_limit" in last_error.lower():
        return "rate_limit_retry"
    else:
        return "general_retry"

def processing_node_with_retry(state: ChatState) -> ChatState:
    """å¸¦é‡è¯•æœºåˆ¶çš„å¤„ç†èŠ‚ç‚¹"""
    try:
        result = risky_operation(state["user_input"])
        return {
            **state,
            "result": result,
            "error_count": 0,  # é‡ç½®é”™è¯¯è®¡æ•°
            "last_error": None
        }
    except Exception as e:
        return {
            **state,
            "error_count": state.get("error_count", 0) + 1,
            "last_error": str(e)
        }

# æ·»åŠ é‡è¯•å¾ªç¯
graph.add_conditional_edges(
    "processing_node",
    retry_logic,
    {
        "next_step": "success_handler",
        "error_handler": "final_error_handler",
        "network_retry": "processing_node",     # å¾ªç¯å›è‡ªå·±
        "rate_limit_retry": "wait_and_retry",
        "general_retry": "processing_node"
    }
)
```

## ğŸš€ START å’Œ END èŠ‚ç‚¹

### START èŠ‚ç‚¹

```python
from langgraph.graph import START, END

# æ–¹å¼ 1ï¼šä½¿ç”¨ set_entry_point
graph.set_entry_point("first_node")

# æ–¹å¼ 2ï¼šä½¿ç”¨ START å¸¸é‡ï¼ˆæ¨èï¼‰
graph.add_edge(START, "first_node")

# æ–¹å¼ 3ï¼šå¤šä¸ªå…¥å£ç‚¹
def entry_router(state: ChatState) -> str:
    """å…¥å£è·¯ç”±"""
    if state.get("is_admin"):
        return "admin_flow"
    else:
        return "user_flow"

graph.add_conditional_edges(
    START,
    entry_router,
    {
        "admin_flow": "admin_node",
        "user_flow": "user_node"
    }
)
```

### END èŠ‚ç‚¹

```python
# æ–¹å¼ 1ï¼šä½¿ç”¨ set_finish_point
graph.set_finish_point("final_node")

# æ–¹å¼ 2ï¼šä½¿ç”¨ END å¸¸é‡ï¼ˆæ¨èï¼‰
graph.add_edge("final_node", END)

# æ–¹å¼ 3ï¼šæ¡ä»¶ç»“æŸ
def should_continue(state: ChatState) -> str:
    """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç»§ç»­"""
    if state.get("is_complete"):
        return "end"
    elif state.get("needs_human_review"):
        return "human_review"
    else:
        return "continue_processing"

graph.add_conditional_edges(
    "decision_node",
    should_continue,
    {
        "end": END,
        "human_review": "human_review_node",
        "continue_processing": "next_processing_node"
    }
)
```

## ğŸ”„ æ•°æ®æµæ¨¡å¼

### 1. çº¿æ€§æµç¨‹

```python
"""
ç”¨æˆ·è¾“å…¥ â†’ é¢„å¤„ç† â†’ LLMè°ƒç”¨ â†’ åå¤„ç† â†’ è¾“å‡º
"""

graph.add_edge(START, "preprocessing")
graph.add_edge("preprocessing", "llm_call")
graph.add_edge("llm_call", "postprocessing")
graph.add_edge("postprocessing", END)
```

### 2. åˆ†æ”¯æµç¨‹

```python
"""
              â†’ æœç´¢å¤„ç† â†’
ç”¨æˆ·è¾“å…¥ â†’ åˆ†æ â†’ å¤©æ°”å¤„ç† â†’ ç»“æœæ±‡æ€» â†’ è¾“å‡º
              â†’ è®¡ç®—å¤„ç† â†’
"""

def route_by_type(state: ChatState) -> str:
    intent = state["intent"]
    return f"{intent}_handler"

graph.add_edge(START, "intent_analysis")
graph.add_conditional_edges(
    "intent_analysis",
    route_by_type,
    {
        "search_handler": "search_handler",
        "weather_handler": "weather_handler",
        "calculation_handler": "calculation_handler"
    }
)

# æ‰€æœ‰åˆ†æ”¯æ±‡æ€»åˆ°åŒä¸€ä¸ªèŠ‚ç‚¹
graph.add_edge("search_handler", "result_aggregation")
graph.add_edge("weather_handler", "result_aggregation")
graph.add_edge("calculation_handler", "result_aggregation")
graph.add_edge("result_aggregation", END)
```

### 3. å¹¶è¡Œæµç¨‹

```python
"""
ç”¨æˆ·è¾“å…¥ â†’ åˆ†å‘ â†’ æœç´¢API â†˜
                â†’ æ•°æ®åº“æŸ¥è¯¢ â†’ ç»“æœåˆå¹¶ â†’ è¾“å‡º
                â†’ ç¼“å­˜æŸ¥æ‰¾ â†—
"""

def parallel_dispatch(state: ChatState) -> list[Send]:
    """å¹¶è¡Œåˆ†å‘åˆ°å¤šä¸ªå¤„ç†èŠ‚ç‚¹"""
    query = state["user_input"]

    return [
        Send("search_api", {"query": query, "source": "api"}),
        Send("search_database", {"query": query, "source": "database"}),
        Send("search_cache", {"query": query, "source": "cache"})
    ]

graph.add_edge(START, "input_validation")
graph.add_conditional_edges("input_validation", parallel_dispatch)

# å¹¶è¡ŒèŠ‚ç‚¹è‡ªåŠ¨æ±‡æ€»ç»“æœï¼ˆé€šè¿‡ reducerï¼‰
graph.add_edge("search_api", "result_merger")
graph.add_edge("search_database", "result_merger")
graph.add_edge("search_cache", "result_merger")
graph.add_edge("result_merger", END)
```

### 4. å¾ªç¯æµç¨‹

```python
"""
ç”¨æˆ·è¾“å…¥ â†’ å¤„ç† â†’ éªŒè¯ â†’ [å¤±è´¥] â†’ é‡æ–°å¤„ç†
                    â†“
                  [æˆåŠŸ] â†’ è¾“å‡º
"""

def validation_router(state: ChatState) -> str:
    """éªŒè¯ç»“æœè·¯ç”±"""
    if state.get("validation_passed"):
        return "success"
    elif state.get("retry_count", 0) < 3:
        return "retry"
    else:
        return "failed"

graph.add_edge(START, "initial_processing")
graph.add_edge("initial_processing", "validation")

graph.add_conditional_edges(
    "validation",
    validation_router,
    {
        "success": "output_formatting",
        "retry": "retry_processing",      # å¾ªç¯å›åˆ°å¤„ç†
        "failed": "error_handling"
    }
)

graph.add_edge("retry_processing", "validation")  # é‡è¯•åé‡æ–°éªŒè¯
graph.add_edge("output_formatting", END)
graph.add_edge("error_handling", END)
```

## ğŸª å®æˆ˜æ¡ˆä¾‹ï¼šæ™ºèƒ½å®¢æœç³»ç»Ÿ

```python
from typing import Literal
from langgraph.graph import StateGraph, START, END

class CustomerServiceState(TypedDict):
    user_input: str
    intent: Literal["order", "technical", "billing", "general"]
    confidence: float
    user_id: str
    session_id: str

    # å¤„ç†ç»“æœ
    query_result: Optional[dict]
    response: str
    escalate_to_human: bool

    # ä¸Šä¸‹æ–‡ä¿¡æ¯
    conversation_history: List[dict]
    user_profile: Optional[dict]

def analyze_intent(state: CustomerServiceState) -> CustomerServiceState:
    """åˆ†æç”¨æˆ·æ„å›¾"""
    intent, confidence = intent_classifier(state["user_input"])

    return {
        **state,
        "intent": intent,
        "confidence": confidence
    }

def handle_order_inquiry(state: CustomerServiceState) -> CustomerServiceState:
    """å¤„ç†è®¢å•æŸ¥è¯¢"""
    user_id = state["user_id"]
    query = state["user_input"]

    order_info = order_system.query(user_id, query)

    return {
        **state,
        "query_result": order_info,
        "response": format_order_response(order_info)
    }

def handle_technical_support(state: CustomerServiceState) -> CustomerServiceState:
    """å¤„ç†æŠ€æœ¯æ”¯æŒ"""
    query = state["user_input"]
    kb_result = knowledge_base.search(query)

    if kb_result["confidence"] > 0.8:
        response = generate_technical_response(kb_result)
        escalate = False
    else:
        response = "è®©æˆ‘ä¸ºæ‚¨è½¬æ¥æŠ€æœ¯ä¸“å®¶..."
        escalate = True

    return {
        **state,
        "query_result": kb_result,
        "response": response,
        "escalate_to_human": escalate
    }

def route_by_intent(state: CustomerServiceState) -> str:
    """æ ¹æ®æ„å›¾è·¯ç”±"""
    intent = state["intent"]
    confidence = state["confidence"]

    # ä½ç½®ä¿¡åº¦éœ€è¦æ¾„æ¸…
    if confidence < 0.6:
        return "clarification"

    # æ ¹æ®æ„å›¾è·¯ç”±
    if intent == "order":
        return "order_handling"
    elif intent == "technical":
        return "technical_handling"
    elif intent == "billing":
        return "billing_handling"
    else:
        return "general_handling"

def escalation_check(state: CustomerServiceState) -> str:
    """æ£€æŸ¥æ˜¯å¦éœ€è¦äººå·¥ä»‹å…¥"""
    if state.get("escalate_to_human"):
        return "human_handoff"
    else:
        return "response_delivery"

# æ„å»ºå®¢æœå›¾
customer_service_graph = StateGraph(CustomerServiceState)

# æ·»åŠ èŠ‚ç‚¹
customer_service_graph.add_node("intent_analysis", analyze_intent)
customer_service_graph.add_node("clarification", ask_clarification)
customer_service_graph.add_node("order_handling", handle_order_inquiry)
customer_service_graph.add_node("technical_handling", handle_technical_support)
customer_service_graph.add_node("billing_handling", handle_billing_inquiry)
customer_service_graph.add_node("general_handling", handle_general_query)
customer_service_graph.add_node("human_handoff", escalate_to_human)
customer_service_graph.add_node("response_delivery", deliver_response)

# æ·»åŠ è¾¹
customer_service_graph.add_edge(START, "intent_analysis")

customer_service_graph.add_conditional_edges(
    "intent_analysis",
    route_by_intent,
    {
        "clarification": "clarification",
        "order_handling": "order_handling",
        "technical_handling": "technical_handling",
        "billing_handling": "billing_handling",
        "general_handling": "general_handling"
    }
)

# æ‰€æœ‰å¤„ç†èŠ‚ç‚¹éƒ½éœ€è¦æ£€æŸ¥æ˜¯å¦å‡çº§
for node in ["order_handling", "technical_handling", "billing_handling", "general_handling"]:
    customer_service_graph.add_conditional_edges(
        node,
        escalation_check,
        {
            "human_handoff": "human_handoff",
            "response_delivery": "response_delivery"
        }
    )

customer_service_graph.add_edge("clarification", "intent_analysis")  # æ¾„æ¸…åé‡æ–°åˆ†æ
customer_service_graph.add_edge("human_handoff", END)
customer_service_graph.add_edge("response_delivery", END)

# ç¼–è¯‘å›¾
customer_service_app = customer_service_graph.compile()
```

## ğŸ’¡ æœ€ä½³å®è·µ

### âœ… èŠ‚ç‚¹è®¾è®¡æœ€ä½³å®è·µ

1. **å•ä¸€èŒè´£**ï¼šæ¯ä¸ªèŠ‚ç‚¹åªè´Ÿè´£ä¸€ä¸ªæ˜ç¡®çš„åŠŸèƒ½
2. **çº¯å‡½æ•°**ï¼šé¿å…å‰¯ä½œç”¨ï¼Œä¾¿äºæµ‹è¯•å’Œè°ƒè¯•
3. **é”™è¯¯å¤„ç†**ï¼šä¼˜é›…åœ°å¤„ç†å¼‚å¸¸æƒ…å†µ
4. **çŠ¶æ€å®Œæ•´æ€§**ï¼šç¡®ä¿è¿”å›çš„çŠ¶æ€åŒ…å«å¿…è¦çš„å­—æ®µ
5. **æ–‡æ¡£åŒ–**ï¼šä¸ºèŠ‚ç‚¹æ·»åŠ æ¸…æ™°çš„æ–‡æ¡£è¯´æ˜

### âœ… è¾¹è®¾è®¡æœ€ä½³å®è·µ

1. **æ¸…æ™°çš„è·¯ç”±é€»è¾‘**ï¼šè·¯ç”±å‡½æ•°åº”è¯¥ç®€å•æ˜äº†
2. **å®Œæ•´çš„è·¯ç”±è¦†ç›–**ï¼šç¡®ä¿æ‰€æœ‰å¯èƒ½çš„çŠ¶æ€éƒ½æœ‰å¯¹åº”çš„è·¯ç”±
3. **åˆç†çš„é»˜è®¤è·¯ç”±**ï¼šæä¾›åˆç†çš„é»˜è®¤å¤„ç†è·¯å¾„
4. **é¿å…æ­»å¾ªç¯**ï¼šåœ¨å¾ªç¯è¾¹ä¸­åŒ…å«é€€å‡ºæ¡ä»¶
5. **æ€§èƒ½è€ƒè™‘**ï¼šé¿å…è¿‡äºå¤æ‚çš„è·¯ç”±è®¡ç®—

### âŒ å¸¸è§é™·é˜±

1. **èŠ‚ç‚¹é—´çš„ç´§è€¦åˆ**ï¼šé¿å…èŠ‚ç‚¹ç›´æ¥ä¾èµ–å…¶ä»–èŠ‚ç‚¹çš„å†…éƒ¨å®ç°
2. **çŠ¶æ€æ±¡æŸ“**ï¼šé¿å…åœ¨èŠ‚ç‚¹ä¸­æ·»åŠ ä¸ç›¸å…³çš„çŠ¶æ€å­—æ®µ
3. **è¿‡åº¦å¤æ‚çš„è·¯ç”±**ï¼šä¿æŒè·¯ç”±é€»è¾‘çš„ç®€å•æ€§
4. **ç¼ºå°‘é”™è¯¯å¤„ç†**ï¼šå¿½ç•¥å¼‚å¸¸æƒ…å†µçš„å¤„ç†
5. **çŠ¶æ€ä¸ä¸€è‡´**ï¼šè¿”å›çš„çŠ¶æ€ç»“æ„ä¸å®šä¹‰ä¸ç¬¦

## ğŸš€ ä¸‹ä¸€æ­¥

æŒæ¡äº†èŠ‚ç‚¹å’Œè¾¹çš„ä½¿ç”¨åï¼Œæ¥ä¸‹æ¥å­¦ä¹ ï¼š
- `04-Graphç¼–è¯‘ä¸æ‰§è¡Œ.md` - äº†è§£å›¾çš„è¿è¡Œæœºåˆ¶
- `ç¤ºä¾‹ä»£ç /simple_graph.py` - æŸ¥çœ‹å®Œæ•´çš„å®ç°ç¤ºä¾‹

---

*èŠ‚ç‚¹å’Œè¾¹æ˜¯æ„å»º LangGraph åº”ç”¨çš„åŸºç¡€ï¼Œç†è§£å®ƒä»¬çš„ä½¿ç”¨æ¨¡å¼æ˜¯æˆåŠŸçš„å…³é”®ï¼* ğŸ”§