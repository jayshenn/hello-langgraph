# Graph ç¼–è¯‘ä¸æ‰§è¡Œï¼šè®©å›¾è¿è¡Œèµ·æ¥

> ğŸ¯ **å­¦ä¹ ç›®æ ‡**ï¼šæŒæ¡å›¾çš„ç¼–è¯‘è¿‡ç¨‹å’Œæ‰§è¡Œæ–¹å¼ï¼Œç†è§£ä¸åŒæ‰§è¡Œæ¨¡å¼çš„é€‚ç”¨åœºæ™¯

## âš™ï¸ compile()ï¼šå›¾çš„ç¼–è¯‘è¿‡ç¨‹

### ç¼–è¯‘çš„ä½œç”¨

åœ¨ LangGraph ä¸­ï¼Œ`compile()` æ–¹æ³•å°†å›¾çš„å®šä¹‰è½¬æ¢ä¸ºå¯æ‰§è¡Œçš„åº”ç”¨ï¼š

```python
from langgraph.graph import StateGraph
from typing import TypedDict

class ChatState(TypedDict):
    user_input: str
    response: str

def process_node(state: ChatState) -> ChatState:
    return {**state, "response": f"å¤„ç†ç»“æœ: {state['user_input']}"}

# 1. å®šä¹‰å›¾ç»“æ„
graph = StateGraph(ChatState)
graph.add_node("process", process_node)
graph.add_edge("START", "process")
graph.add_edge("process", "END")

# 2. ç¼–è¯‘æˆå¯æ‰§è¡Œåº”ç”¨
app = graph.compile()

# 3. ç°åœ¨å¯ä»¥è¿è¡Œäº†
result = app.invoke({"user_input": "Hello World"})
print(result)  # {"user_input": "Hello World", "response": "å¤„ç†ç»“æœ: Hello World"}
```

### ç¼–è¯‘æ—¶éªŒè¯

```python
# ç¼–è¯‘è¿‡ç¨‹ä¸­ä¼šè¿›è¡Œå¤šé¡¹éªŒè¯
try:
    app = graph.compile()
except ValueError as e:
    print(f"å›¾ç»“æ„é”™è¯¯: {e}")

# å¸¸è§çš„ç¼–è¯‘é”™è¯¯ï¼š
# 1. ç¼ºå°‘å…¥å£ç‚¹
# 2. å­˜åœ¨å­¤ç«‹èŠ‚ç‚¹ï¼ˆæ— æ³•åˆ°è¾¾çš„èŠ‚ç‚¹ï¼‰
# 3. ç¼ºå°‘ç»“æŸè·¯å¾„
# 4. å¾ªç¯è·¯å¾„æ²¡æœ‰é€€å‡ºæ¡ä»¶
```

## ğŸ”§ ç¼–è¯‘é…ç½®é€‰é¡¹

### Checkpointerï¼šçŠ¶æ€æŒä¹…åŒ–

```python
from langgraph.checkpoint.memory import MemorySaver
from langgraph.checkpoint.sqlite import SqliteSaver

# 1. å†…å­˜æ£€æŸ¥ç‚¹ï¼ˆé€‚ç”¨äºå¼€å‘å’Œæµ‹è¯•ï¼‰
memory_checkpointer = MemorySaver()
app_with_memory = graph.compile(checkpointer=memory_checkpointer)

# 2. SQLite æ£€æŸ¥ç‚¹ï¼ˆé€‚ç”¨äºç”Ÿäº§ç¯å¢ƒï¼‰
sqlite_checkpointer = SqliteSaver.from_conn_string("checkpoints.db")
app_with_persistence = graph.compile(checkpointer=sqlite_checkpointer)

# ä½¿ç”¨æ£€æŸ¥ç‚¹çš„å¥½å¤„ï¼š
# - çŠ¶æ€æŒä¹…åŒ–
# - æ”¯æŒæ—¶é—´æ—…è¡Œ
# - äººæœºåä½œ
# - é”™è¯¯æ¢å¤
```

### Interrupt é…ç½®

```python
# é…ç½®ä¸­æ–­ç‚¹
app = graph.compile(
    checkpointer=memory_checkpointer,
    interrupt_before=["human_review"],  # åœ¨æŒ‡å®šèŠ‚ç‚¹å‰ä¸­æ–­
    interrupt_after=["risky_operation"]  # åœ¨æŒ‡å®šèŠ‚ç‚¹åä¸­æ–­
)

# åŠ¨æ€è®¾ç½®ä¸­æ–­ç‚¹
app = graph.compile(checkpointer=memory_checkpointer)
config = {
    "configurable": {
        "thread_id": "conversation_1"
    },
    "interrupt_before": ["sensitive_operation"]
}

result = app.invoke(initial_state, config=config)
```

### è°ƒè¯•é…ç½®

```python
# å¯ç”¨è°ƒè¯•æ¨¡å¼
app = graph.compile(
    debug=True,  # å¯ç”¨è°ƒè¯•ä¿¡æ¯
    checkpointer=memory_checkpointer
)

# è‡ªå®šä¹‰ç¼–è¯‘å™¨é…ç½®
from langgraph.pregel import Pregel

custom_app = graph.compile(
    checkpointer=memory_checkpointer,
    compiler=Pregel,  # è‡ªå®šä¹‰ç¼–è¯‘å™¨
    debug=True
)
```

## ğŸƒ æ‰§è¡Œæ¨¡å¼

### 1. invoke()ï¼šåŒæ­¥æ‰§è¡Œ

```python
# åŸºæœ¬åŒæ­¥æ‰§è¡Œ
result = app.invoke({"user_input": "Hello"})
print(result)

# å¸¦é…ç½®çš„åŒæ­¥æ‰§è¡Œ
config = {
    "configurable": {
        "thread_id": "user_123",
        "model_name": "gpt-4"
    }
}

result = app.invoke(
    {"user_input": "Hello"},
    config=config
)
```

### 2. ainvoke()ï¼šå¼‚æ­¥æ‰§è¡Œ

```python
import asyncio

async def run_async():
    # å¼‚æ­¥æ‰§è¡Œ
    result = await app.ainvoke({"user_input": "Hello"})
    print(result)

    # å¹¶å‘æ‰§è¡Œå¤šä¸ªè¯·æ±‚
    tasks = [
        app.ainvoke({"user_input": f"Query {i}"})
        for i in range(5)
    ]

    results = await asyncio.gather(*tasks)
    return results

# è¿è¡Œå¼‚æ­¥ä»£ç 
asyncio.run(run_async())
```

### 3. stream()ï¼šæµå¼æ‰§è¡Œ

```python
# åŒæ­¥æµå¼æ‰§è¡Œ
for chunk in app.stream({"user_input": "Hello"}):
    print(f"æ”¶åˆ°æ›´æ–°: {chunk}")

# å¼‚æ­¥æµå¼æ‰§è¡Œ
async def stream_async():
    async for chunk in app.astream({"user_input": "Hello"}):
        print(f"å¼‚æ­¥æ›´æ–°: {chunk}")

asyncio.run(stream_async())
```

## ğŸ“Š æµå¼æ‰§è¡Œè¯¦è§£

### æµå¼æ¨¡å¼

```python
# 1. æµå¼èŠ‚ç‚¹æ›´æ–°ï¼ˆé»˜è®¤ï¼‰
for chunk in app.stream({"user_input": "Hello"}):
    print(chunk)
    # è¾“å‡ºæ ¼å¼: {"node_name": {"updated_state": ...}}

# 2. æµå¼å€¼æ›´æ–°
for chunk in app.stream({"user_input": "Hello"}, stream_mode="values"):
    print(chunk)
    # è¾“å‡ºæ ¼å¼: {"user_input": "Hello", "response": "..."}

# 3. æµå¼è°ƒè¯•ä¿¡æ¯
for chunk in app.stream({"user_input": "Hello"}, stream_mode="debug"):
    print(chunk)
    # è¾“å‡ºæ ¼å¼: {"type": "task", "timestamp": ..., "step": ...}
```

### å¤šç§æµå¼æ¨¡å¼ç»„åˆ

```python
# åŒæ—¶æµå¼å¤šç§ç±»å‹çš„æ•°æ®
for chunk in app.stream(
    {"user_input": "Hello"},
    stream_mode=["values", "updates", "debug"]
):
    if "values" in chunk:
        print(f"çŠ¶æ€æ›´æ–°: {chunk['values']}")
    if "updates" in chunk:
        print(f"èŠ‚ç‚¹æ›´æ–°: {chunk['updates']}")
    if "debug" in chunk:
        print(f"è°ƒè¯•ä¿¡æ¯: {chunk['debug']}")
```

### è‡ªå®šä¹‰æµå¼æ•°æ®

```python
def custom_streaming_node(state: ChatState) -> ChatState:
    """æ”¯æŒè‡ªå®šä¹‰æµå¼è¾“å‡ºçš„èŠ‚ç‚¹"""
    from langgraph.prebuilt import stream

    # æµå¼è¾“å‡ºå¤„ç†è¿›åº¦
    stream("å¼€å§‹å¤„ç†ç”¨æˆ·è¾“å…¥...")

    processed_input = process_input(state["user_input"])
    stream(f"è¾“å…¥å¤„ç†å®Œæˆ: {processed_input[:50]}...")

    result = generate_response(processed_input)
    stream("å“åº”ç”Ÿæˆå®Œæˆ!")

    return {**state, "response": result}

# æ¥æ”¶è‡ªå®šä¹‰æµå¼æ•°æ®
for chunk in app.stream({"user_input": "Hello"}, stream_mode="custom"):
    if isinstance(chunk, str):
        print(f"è¿›åº¦æ›´æ–°: {chunk}")
    else:
        print(f"çŠ¶æ€æ›´æ–°: {chunk}")
```

## ğŸ® æ‰§è¡Œé…ç½®

### åŸºæœ¬é…ç½®é€‰é¡¹

```python
config = {
    # çº¿ç¨‹é…ç½®
    "configurable": {
        "thread_id": "conversation_123",  # å¯¹è¯çº¿ç¨‹ID
        "user_id": "user_456",           # ç”¨æˆ·ID
    },

    # æ‰§è¡Œé…ç½®
    "recursion_limit": 100,              # é€’å½’é™åˆ¶
    "max_execution_time": 300,           # æœ€å¤§æ‰§è¡Œæ—¶é—´ï¼ˆç§’ï¼‰

    # ä¸­æ–­é…ç½®
    "interrupt_before": ["human_review"],
    "interrupt_after": ["sensitive_operation"],

    # è°ƒè¯•é…ç½®
    "debug": True,
    "verbose": True
}

result = app.invoke(initial_state, config=config)
```

### åŠ¨æ€é…ç½®

```python
def get_dynamic_config(user_id: str, session_id: str) -> dict:
    """æ ¹æ®ç”¨æˆ·å’Œä¼šè¯åŠ¨æ€ç”Ÿæˆé…ç½®"""
    user_profile = get_user_profile(user_id)

    config = {
        "configurable": {
            "thread_id": f"{user_id}_{session_id}",
            "model_name": user_profile.get("preferred_model", "gpt-3.5-turbo"),
            "temperature": user_profile.get("creativity_level", 0.7),
            "max_tokens": user_profile.get("response_length", 150)
        }
    }

    # æ ¹æ®ç”¨æˆ·ç­‰çº§è®¾ç½®ä¸åŒçš„é™åˆ¶
    if user_profile.get("is_premium"):
        config["recursion_limit"] = 200
        config["max_execution_time"] = 600
    else:
        config["recursion_limit"] = 50
        config["max_execution_time"] = 120

    return config

# ä½¿ç”¨åŠ¨æ€é…ç½®
config = get_dynamic_config("user_123", "session_456")
result = app.invoke({"user_input": "Hello"}, config=config)
```

## ğŸ”„ æ£€æŸ¥ç‚¹å’ŒçŠ¶æ€ç®¡ç†

### åŸºæœ¬æ£€æŸ¥ç‚¹æ“ä½œ

```python
from langgraph.checkpoint.memory import MemorySaver

checkpointer = MemorySaver()
app = graph.compile(checkpointer=checkpointer)

config = {"configurable": {"thread_id": "conversation_1"}}

# ç¬¬ä¸€æ¬¡æ‰§è¡Œ
result1 = app.invoke({"user_input": "Hello"}, config=config)

# ç¬¬äºŒæ¬¡æ‰§è¡Œï¼ˆåœ¨åŒä¸€çº¿ç¨‹ä¸­ï¼Œä¼šç»§ç»­ä¹‹å‰çš„çŠ¶æ€ï¼‰
result2 = app.invoke({"user_input": "How are you?"}, config=config)

# æŸ¥çœ‹æ£€æŸ¥ç‚¹å†å²
checkpoints = list(checkpointer.list(config))
for checkpoint in checkpoints:
    print(f"æ£€æŸ¥ç‚¹ {checkpoint.checkpoint_id}: {checkpoint.state}")
```

### é«˜çº§æ£€æŸ¥ç‚¹æ“ä½œ

```python
# ä»ç‰¹å®šæ£€æŸ¥ç‚¹æ¢å¤
specific_config = {
    "configurable": {
        "thread_id": "conversation_1",
        "checkpoint_id": "specific_checkpoint_id"
    }
}

result = app.invoke({"user_input": "Continue"}, config=specific_config)

# è·å–å½“å‰çŠ¶æ€
current_state = app.get_state(config)
print(f"å½“å‰çŠ¶æ€: {current_state.values}")
print(f"ä¸‹ä¸€ä¸ªèŠ‚ç‚¹: {current_state.next}")

# æ›´æ–°çŠ¶æ€
new_state = {**current_state.values, "custom_field": "new_value"}
app.update_state(config, new_state)
```

## ğŸ› ï¸ è°ƒè¯•å’Œç›‘æ§

### å†…ç½®è°ƒè¯•å·¥å…·

```python
# å¯ç”¨è¯¦ç»†è°ƒè¯•
app = graph.compile(checkpointer=checkpointer, debug=True)

# è°ƒè¯•æ‰§è¡Œè¿‡ç¨‹
import logging
logging.basicConfig(level=logging.DEBUG)

for chunk in app.stream({"user_input": "Hello"}, stream_mode="debug"):
    print(f"è°ƒè¯•ä¿¡æ¯: {chunk}")
```

### è‡ªå®šä¹‰ç›‘æ§

```python
class ExecutionMonitor:
    """è‡ªå®šä¹‰æ‰§è¡Œç›‘æ§å™¨"""

    def __init__(self):
        self.execution_log = []
        self.start_time = None

    def log_node_start(self, node_name: str, state: dict):
        """è®°å½•èŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ"""
        self.execution_log.append({
            "event": "node_start",
            "node": node_name,
            "timestamp": time.time(),
            "state_keys": list(state.keys())
        })

    def log_node_end(self, node_name: str, result: dict):
        """è®°å½•èŠ‚ç‚¹æ‰§è¡Œå®Œæˆ"""
        self.execution_log.append({
            "event": "node_end",
            "node": node_name,
            "timestamp": time.time(),
            "result_keys": list(result.keys())
        })

    def get_report(self):
        """ç”Ÿæˆæ‰§è¡ŒæŠ¥å‘Š"""
        total_time = time.time() - self.start_time
        node_count = len([log for log in self.execution_log if log["event"] == "node_start"])

        return {
            "total_execution_time": total_time,
            "nodes_executed": node_count,
            "execution_log": self.execution_log
        }

# ä½¿ç”¨ç›‘æ§å™¨
monitor = ExecutionMonitor()

def monitored_node(state: ChatState) -> ChatState:
    """å¸¦ç›‘æ§çš„èŠ‚ç‚¹"""
    monitor.log_node_start("monitored_node", state)

    # å®é™…å¤„ç†é€»è¾‘
    result = {**state, "processed": True}

    monitor.log_node_end("monitored_node", result)
    return result
```

### æ€§èƒ½åˆ†æ

```python
import time
from functools import wraps

def performance_monitor(func):
    """æ€§èƒ½ç›‘æ§è£…é¥°å™¨"""
    @wraps(func)
    def wrapper(state):
        start_time = time.time()
        result = func(state)
        end_time = time.time()

        # è®°å½•æ‰§è¡Œæ—¶é—´
        execution_time = end_time - start_time
        print(f"èŠ‚ç‚¹ {func.__name__} æ‰§è¡Œæ—¶é—´: {execution_time:.3f}ç§’")

        # æ·»åŠ æ€§èƒ½ä¿¡æ¯åˆ°çŠ¶æ€
        performance_info = result.get("_performance", {})
        performance_info[func.__name__] = execution_time

        return {**result, "_performance": performance_info}

    return wrapper

@performance_monitor
def slow_processing_node(state: ChatState) -> ChatState:
    """è€—æ—¶çš„å¤„ç†èŠ‚ç‚¹"""
    time.sleep(2)  # æ¨¡æ‹Ÿè€—æ—¶æ“ä½œ
    return {**state, "processed": True}
```

## ğŸš¨ é”™è¯¯å¤„ç†å’Œæ¢å¤

### å¼‚å¸¸å¤„ç†

```python
def robust_execution(app, initial_state, config):
    """å¥å£®çš„æ‰§è¡Œå‡½æ•°"""
    max_retries = 3
    retry_count = 0

    while retry_count < max_retries:
        try:
            return app.invoke(initial_state, config)

        except Exception as e:
            retry_count += 1
            print(f"æ‰§è¡Œå¤±è´¥ (å°è¯• {retry_count}/{max_retries}): {e}")

            if retry_count >= max_retries:
                print("è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ‰§è¡Œå¤±è´¥")
                raise

            # æŒ‡æ•°é€€é¿
            wait_time = 2 ** retry_count
            print(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
            time.sleep(wait_time)

# ä½¿ç”¨å¥å£®æ‰§è¡Œ
try:
    result = robust_execution(app, {"user_input": "Hello"}, config)
except Exception as e:
    print(f"æœ€ç»ˆæ‰§è¡Œå¤±è´¥: {e}")
```

### çŠ¶æ€å›æ»š

```python
def safe_execution_with_rollback(app, initial_state, config):
    """æ”¯æŒå›æ»šçš„å®‰å…¨æ‰§è¡Œ"""
    # ä¿å­˜åˆå§‹æ£€æŸ¥ç‚¹
    initial_checkpoint = app.get_state(config)

    try:
        result = app.invoke(initial_state, config)
        return result

    except Exception as e:
        print(f"æ‰§è¡Œå‡ºé”™ï¼Œå›æ»šåˆ°åˆå§‹çŠ¶æ€: {e}")

        # å›æ»šåˆ°åˆå§‹çŠ¶æ€
        if initial_checkpoint:
            app.update_state(config, initial_checkpoint.values)

        raise
```

## ğŸª å®æˆ˜æ¡ˆä¾‹ï¼šå®Œæ•´çš„æ‰§è¡Œæµç¨‹

```python
import asyncio
import time
from typing import AsyncGenerator

class AdvancedChatApp:
    """é«˜çº§èŠå¤©åº”ç”¨ç¤ºä¾‹"""

    def __init__(self, graph: StateGraph):
        self.checkpointer = MemorySaver()
        self.app = graph.compile(
            checkpointer=self.checkpointer,
            debug=True
        )
        self.execution_stats = {}

    async def chat(self, user_id: str, message: str) -> AsyncGenerator[dict, None]:
        """å¼‚æ­¥èŠå¤©æ¥å£"""
        config = {
            "configurable": {
                "thread_id": f"user_{user_id}",
                "user_id": user_id
            }
        }

        start_time = time.time()
        token_count = 0

        try:
            # æµå¼æ‰§è¡Œ
            async for chunk in self.app.astream(
                {"user_input": message},
                config=config,
                stream_mode=["values", "updates"]
            ):
                # è®¡ç®—tokenä½¿ç”¨é‡
                if "values" in chunk and "tokens_used" in chunk["values"]:
                    token_count += chunk["values"]["tokens_used"]

                yield {
                    "type": "update",
                    "data": chunk,
                    "timestamp": time.time()
                }

            # è®°å½•ç»Ÿè®¡ä¿¡æ¯
            execution_time = time.time() - start_time
            self.execution_stats[user_id] = {
                "last_execution_time": execution_time,
                "total_tokens": token_count,
                "timestamp": time.time()
            }

            yield {
                "type": "complete",
                "execution_time": execution_time,
                "tokens_used": token_count
            }

        except Exception as e:
            yield {
                "type": "error",
                "error": str(e),
                "timestamp": time.time()
            }

    def get_conversation_history(self, user_id: str) -> list:
        """è·å–å¯¹è¯å†å²"""
        config = {"configurable": {"thread_id": f"user_{user_id}"}}

        try:
            checkpoints = list(self.checkpointer.list(config))
            history = []

            for checkpoint in checkpoints:
                if "conversation_history" in checkpoint.state:
                    history.extend(checkpoint.state["conversation_history"])

            return history
        except Exception:
            return []

    def reset_conversation(self, user_id: str):
        """é‡ç½®å¯¹è¯"""
        config = {"configurable": {"thread_id": f"user_{user_id}"}}

        # æ¸…é™¤æ£€æŸ¥ç‚¹å†å²
        current_state = self.app.get_state(config)
        if current_state:
            # é‡ç½®ä¸ºåˆå§‹çŠ¶æ€
            self.app.update_state(config, {
                "conversation_history": [],
                "user_context": {}
            })

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    # å‡è®¾å·²ç»æœ‰å®šä¹‰å¥½çš„å›¾
    chat_app = AdvancedChatApp(chat_graph)

    # å¼‚æ­¥èŠå¤©
    async for update in chat_app.chat("user_123", "Hello, how are you?"):
        if update["type"] == "update":
            print(f"æ›´æ–°: {update['data']}")
        elif update["type"] == "complete":
            print(f"å®Œæˆ - è€—æ—¶: {update['execution_time']:.2f}ç§’")
        elif update["type"] == "error":
            print(f"é”™è¯¯: {update['error']}")

    # æŸ¥çœ‹å†å²
    history = chat_app.get_conversation_history("user_123")
    print(f"å¯¹è¯å†å²: {history}")

if __name__ == "__main__":
    asyncio.run(main())
```

## ğŸ’¡ æœ€ä½³å®è·µ

### âœ… ç¼–è¯‘æœ€ä½³å®è·µ

1. **åˆç†çš„æ£€æŸ¥ç‚¹ç­–ç•¥**ï¼šæ ¹æ®åº”ç”¨éœ€æ±‚é€‰æ‹©åˆé€‚çš„æ£€æŸ¥ç‚¹å™¨
2. **é€‚å½“çš„ä¸­æ–­ç‚¹**ï¼šåœ¨å…³é”®èŠ‚ç‚¹è®¾ç½®ä¸­æ–­ï¼Œæ”¯æŒäººæœºåä½œ
3. **é”™è¯¯å¤„ç†**ï¼šä¸ºå…³é”®èŠ‚ç‚¹æ·»åŠ é”™è¯¯å¤„ç†é€»è¾‘
4. **æ€§èƒ½ç›‘æ§**ï¼šç›‘æ§æ‰§è¡Œæ—¶é—´å’Œèµ„æºä½¿ç”¨
5. **çŠ¶æ€éªŒè¯**ï¼šåœ¨å…³é”®æ­¥éª¤éªŒè¯çŠ¶æ€çš„å®Œæ•´æ€§

### âœ… æ‰§è¡Œæœ€ä½³å®è·µ

1. **é€‰æ‹©åˆé€‚çš„æ‰§è¡Œæ¨¡å¼**ï¼šæ ¹æ®åœºæ™¯é€‰æ‹©åŒæ­¥ã€å¼‚æ­¥æˆ–æµå¼æ‰§è¡Œ
2. **é…ç½®ç®¡ç†**ï¼šä½¿ç”¨åŠ¨æ€é…ç½®é€‚åº”ä¸åŒç”¨æˆ·å’Œåœºæ™¯
3. **èµ„æºç®¡ç†**ï¼šåˆç†è®¾ç½®æ‰§è¡Œé™åˆ¶å’Œè¶…æ—¶
4. **ç›‘æ§å’Œæ—¥å¿—**ï¼šè®°å½•æ‰§è¡Œè¿‡ç¨‹ï¼Œä¾¿äºè°ƒè¯•å’Œä¼˜åŒ–
5. **ä¼˜é›…é™çº§**ï¼šåœ¨å¼‚å¸¸æƒ…å†µä¸‹æä¾›åˆç†çš„é™çº§æ–¹æ¡ˆ

### âŒ å¸¸è§é™·é˜±

1. **æ— é™é€’å½’**ï¼šç¼ºå°‘é€€å‡ºæ¡ä»¶çš„å¾ªç¯
2. **å†…å­˜æ³„æ¼**ï¼šé•¿æ—¶é—´è¿è¡Œæ—¶æ£€æŸ¥ç‚¹æ•°æ®ç§¯ç´¯
3. **é˜»å¡æ“ä½œ**ï¼šåœ¨å¼‚æ­¥ç¯å¢ƒä¸­ä½¿ç”¨åŒæ­¥é˜»å¡æ“ä½œ
4. **é…ç½®é”™è¯¯**ï¼šä¸æ­£ç¡®çš„çº¿ç¨‹IDæˆ–æ£€æŸ¥ç‚¹é…ç½®
5. **å¼‚å¸¸åæ²¡**ï¼šå¿½ç•¥æˆ–ä¸å½“å¤„ç†å¼‚å¸¸

## ğŸš€ ä¸‹ä¸€æ­¥

æŒæ¡äº†å›¾çš„ç¼–è¯‘å’Œæ‰§è¡Œåï¼Œä½ å¯ä»¥ï¼š
- æŸ¥çœ‹ `ç¤ºä¾‹ä»£ç /simple_graph.py` äº†è§£å®Œæ•´å®ç°
- å­¦ä¹  `04-è¿›é˜¶ç‰¹æ€§` ä¸­çš„é«˜çº§åŠŸèƒ½
- å®è·µæ„å»ºè‡ªå·±çš„ LangGraph åº”ç”¨

---

*ç†è§£ç¼–è¯‘å’Œæ‰§è¡Œæœºåˆ¶ï¼Œè®©ä½ çš„ LangGraph åº”ç”¨ç¨³å®šé«˜æ•ˆåœ°è¿è¡Œï¼* âš¡