# ä¸ºä»€ä¹ˆéœ€è¦ LangGraphï¼Ÿ

> ğŸ¯ **å­¦ä¹ ç›®æ ‡**ï¼šç†è§£ LangGraph çš„è®¾è®¡ç†å¿µï¼Œæ˜ç™½å®ƒè§£å†³äº†ä»€ä¹ˆé—®é¢˜ï¼Œä»¥åŠä¸ LangChain çš„åŒºåˆ«

## ğŸ¤” AI åº”ç”¨å¼€å‘çš„ç—›ç‚¹

åœ¨æ·±å…¥ LangGraph ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆçœ‹çœ‹å¼€å‘ AI åº”ç”¨æ—¶é‡åˆ°çš„é—®é¢˜ï¼š

### é—®é¢˜ 1ï¼šå¤æ‚çš„æ§åˆ¶æµ

```python
# ä¼ ç»Ÿçš„çº¿æ€§ AI åº”ç”¨
def simple_chatbot(user_input):
    # 1. å¤„ç†è¾“å…¥
    processed_input = preprocess(user_input)

    # 2. è°ƒç”¨ LLM
    response = llm.invoke(processed_input)

    # 3. åå¤„ç†
    final_response = postprocess(response)

    return final_response

# ğŸ˜• é—®é¢˜ï¼šå¦‚ä½•å¤„ç†è¿™äº›æƒ…å†µï¼Ÿ
# - éœ€è¦è°ƒç”¨å¤–éƒ¨å·¥å…·ï¼Ÿ
# - ç”¨æˆ·è¾“å…¥ä¸æ¸…æ¥šï¼Œéœ€è¦æ¾„æ¸…ï¼Ÿ
# - ç”Ÿæˆçš„å†…å®¹éœ€è¦å¤šè½®ä¼˜åŒ–ï¼Ÿ
# - éœ€è¦è®°ä½å¯¹è¯å†å²ï¼Ÿ
```

### é—®é¢˜ 2ï¼šç¼ºä¹çŠ¶æ€ç®¡ç†

```python
# æ²¡æœ‰çŠ¶æ€ç®¡ç†çš„å¤šè½®å¯¹è¯
def bad_chatbot():
    while True:
        user_input = input("ç”¨æˆ·: ")

        # ğŸ˜° æ¯æ¬¡éƒ½æ˜¯å…¨æ–°å¼€å§‹ï¼Œæ²¡æœ‰è®°å¿†
        response = llm.invoke(user_input)
        print(f"AI: {response}")

# é—®é¢˜ï¼š
# - æ— æ³•è®°ä½ä¹‹å‰çš„å¯¹è¯
# - æ— æ³•å¤„ç†ä¸Šä¸‹æ–‡å¼•ç”¨
# - æ— æ³•è¿›è¡Œå¤æ‚çš„å¤šæ­¥æ¨ç†
```

### é—®é¢˜ 3ï¼šå·¥å…·è°ƒç”¨çš„å¤æ‚æ€§

```python
# æ‰‹åŠ¨ç®¡ç†å·¥å…·è°ƒç”¨
def manual_tool_calling(user_input):
    # åˆ¤æ–­æ˜¯å¦éœ€è¦å·¥å…·
    if "å¤©æ°”" in user_input:
        # è°ƒç”¨å¤©æ°”API
        weather_data = weather_api.get_weather()
        # å†æ¬¡è°ƒç”¨LLMç”Ÿæˆå“åº”
        response = llm.invoke(f"åŸºäºå¤©æ°”æ•°æ® {weather_data} å›ç­”: {user_input}")
    elif "æœç´¢" in user_input:
        # è°ƒç”¨æœç´¢API
        search_results = search_api.search(user_input)
        # å†æ¬¡è°ƒç”¨LLM
        response = llm.invoke(f"åŸºäºæœç´¢ç»“æœ {search_results} å›ç­”: {user_input}")
    else:
        response = llm.invoke(user_input)

    # ğŸ˜µ ä»£ç å˜å¾—éå¸¸å¤æ‚ï¼Œéš¾ä»¥ç»´æŠ¤
    return response
```

## ğŸ†š LangChain vs LangGraph

### LangChain çš„ä¼˜åŠ¿å’Œå±€é™

**LangChain æ“…é•¿çš„ï¼š**
```python
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

# ç®€å•çš„é“¾å¼æ“ä½œ
prompt = PromptTemplate(template="ç¿»è¯‘ä»¥ä¸‹æ–‡æœ¬åˆ°è‹±æ–‡: {text}")
chain = LLMChain(llm=llm, prompt=prompt)
result = chain.run(text="ä½ å¥½ï¼Œä¸–ç•Œ")  # çº¿æ€§ã€é¢„å®šä¹‰çš„æµç¨‹
```

**LangChain çš„å±€é™ï¼š**
```python
# ğŸ˜• å¤æ‚æ¡ä»¶é€»è¾‘å¾ˆéš¾è¡¨è¾¾
def complex_chain():
    # å¦‚æœç”¨æˆ·é—®å¤©æ°”ï¼Œè°ƒç”¨å¤©æ°”API
    # å¦‚æœç»“æœä¸ç¡®å®šï¼Œè¦æ±‚æ¾„æ¸…
    # å¦‚æœç”¨æˆ·ä¸æ»¡æ„ï¼Œé‡æ–°ç”Ÿæˆ
    # å¦‚æœé”™è¯¯æ¬¡æ•°è¿‡å¤šï¼Œè½¬äººå·¥
    # ...è¿™ç§é€»è¾‘åœ¨ LangChain ä¸­å¾ˆéš¾ä¼˜é›…åœ°å®ç°
    pass
```

### LangGraph çš„è§£å†³æ–¹æ¡ˆ

**LangGraph çš„è®¾è®¡ç†å¿µï¼š**
1. **å›¾ç»“æ„**ï¼šç”¨å›¾æ¥è¡¨ç¤ºå¤æ‚çš„æ§åˆ¶æµ
2. **çŠ¶æ€ç®¡ç†**ï¼šç»Ÿä¸€çš„çŠ¶æ€åœ¨èŠ‚ç‚¹é—´ä¼ é€’
3. **æ¡ä»¶è·¯ç”±**ï¼šæ ¹æ®çŠ¶æ€åŠ¨æ€å†³å®šä¸‹ä¸€æ­¥
4. **å¾ªç¯æ”¯æŒ**ï¼šæ”¯æŒè¿­ä»£å’Œé‡è¯•
5. **äººæœºåä½œ**ï¼šå¯ä»¥æš‚åœç­‰å¾…äººå·¥å¹²é¢„

```python
from langgraph.graph import StateGraph
from typing import TypedDict

class ChatState(TypedDict):
    user_input: str
    context: list[str]
    needs_clarification: bool
    tool_results: dict
    confidence: float

# ğŸ¯ æ¸…æ™°çš„å›¾ç»“æ„
graph = StateGraph(ChatState)

# æ·»åŠ å¤„ç†èŠ‚ç‚¹
graph.add_node("understand", understand_user_input)
graph.add_node("clarify", ask_for_clarification)
graph.add_node("use_tools", call_external_tools)
graph.add_node("generate", generate_response)

# æ·»åŠ æ¡ä»¶è·¯ç”±
graph.add_conditional_edges(
    "understand",
    lambda state: "clarify" if state["needs_clarification"] else "use_tools"
)

# æ”¯æŒå¾ªç¯
graph.add_conditional_edges(
    "generate",
    lambda state: "understand" if state["confidence"] < 0.7 else END
)
```

## ğŸ¯ LangGraph çš„æ ¸å¿ƒä¼˜åŠ¿

### 1. å¯è§†åŒ–çš„æ§åˆ¶æµ

```python
# ä¼ ç»Ÿä»£ç ï¼šæ§åˆ¶æµéšè—åœ¨ if/else ä¸­
def traditional_agent(user_input):
    if needs_search(user_input):
        if search_successful():
            if result_relevant():
                return generate_response()
            else:
                return refine_search()
        else:
            return handle_error()
    else:
        return direct_response()

# LangGraphï¼šæ§åˆ¶æµæ¸…æ™°å¯è§
"""
ç”¨æˆ·è¾“å…¥ â†’ æ„å›¾åˆ†æ â†’ [éœ€è¦æœç´¢] â†’ æ‰§è¡Œæœç´¢ â†’ [æˆåŠŸ] â†’ ç”Ÿæˆå“åº”
                    â†“                      â†“
                 ç›´æ¥å“åº”              é”™è¯¯å¤„ç†
"""
```

### 2. çŠ¶æ€çš„ä¸€è‡´æ€§ç®¡ç†

```python
# âœ… LangGraph çš„çŠ¶æ€ç®¡ç†
class AgentState(TypedDict):
    user_input: str
    search_results: list[str]
    conversation_history: list[str]
    current_tool: str
    error_count: int

def search_node(state: AgentState) -> AgentState:
    """æ¯ä¸ªèŠ‚ç‚¹éƒ½æ¥æ”¶å®Œæ•´çŠ¶æ€ï¼Œè¿”å›æ›´æ–°åçš„çŠ¶æ€"""
    results = search_api.search(state["user_input"])

    return {
        **state,  # ä¿æŒå…¶ä»–çŠ¶æ€ä¸å˜
        "search_results": results,
        "current_tool": "search",
        "conversation_history": state["conversation_history"] + [f"æœç´¢: {state['user_input']}"]
    }
```

### 3. äººæœºåä½œï¼ˆHuman-in-the-Loopï¼‰

```python
from langgraph.prebuilt import interrupt

def review_node(state: AgentState) -> AgentState:
    """éœ€è¦äººå·¥å®¡æ ¸çš„èŠ‚ç‚¹"""
    if state["confidence"] < 0.5:
        # æš‚åœæ‰§è¡Œï¼Œç­‰å¾…äººå·¥å¹²é¢„
        human_feedback = interrupt("è¯·å®¡æ ¸ä»¥ä¸‹å†…å®¹æ˜¯å¦åˆé€‚...")

        return {
            **state,
            "human_feedback": human_feedback,
            "reviewed": True
        }

    return state
```

### 4. é”™è¯¯æ¢å¤å’Œé‡è¯•

```python
def robust_node(state: AgentState) -> AgentState:
    """æ”¯æŒé”™è¯¯æ¢å¤çš„èŠ‚ç‚¹"""
    try:
        result = risky_operation(state["user_input"])
        return {**state, "result": result, "error_count": 0}
    except Exception as e:
        error_count = state.get("error_count", 0) + 1

        if error_count < 3:
            # é‡è¯•
            return {**state, "error_count": error_count}
        else:
            # è½¬åˆ°é”™è¯¯å¤„ç†
            return {**state, "error": str(e), "needs_fallback": True}

# åœ¨å›¾ä¸­æ·»åŠ é‡è¯•é€»è¾‘
graph.add_conditional_edges(
    "robust_node",
    lambda state: "robust_node" if state.get("error_count", 0) > 0 and state.get("error_count", 0) < 3 else "next_node"
)
```

## ğŸŒŸ LangGraph çš„åº”ç”¨åœºæ™¯

### 1. å¤æ‚çš„å¯¹è¯ç³»ç»Ÿ

```python
"""
ç”¨æˆ·è¾“å…¥ â†’ æ„å›¾è¯†åˆ« â†’ [è¯¢é—®äº§å“] â†’ äº§å“æœç´¢ â†’ ç»“æœç­›é€‰ â†’ æ¨èç”Ÿæˆ
              â†“          [è¯¢é—®è®¢å•] â†’ è®¢å•æŸ¥è¯¢ â†’ çŠ¶æ€æ›´æ–°
           é—²èŠå¤„ç†      [æŠ•è¯‰] â†’ æƒ…æ„Ÿåˆ†æ â†’ é—®é¢˜åˆ†ç±» â†’ [ä¸¥é‡] â†’ è½¬äººå·¥
                                               â†“
                                            è‡ªåŠ¨å¤„ç†
"""
```

### 2. å¤šæ­¥éª¤çš„ä»»åŠ¡å¤„ç†

```python
"""
ä»£ç ç”Ÿæˆä»»åŠ¡ï¼š
ç”¨æˆ·éœ€æ±‚ â†’ éœ€æ±‚åˆ†æ â†’ æ¶æ„è®¾è®¡ â†’ ä»£ç ç”Ÿæˆ â†’ ä»£ç æµ‹è¯• â†’ [æµ‹è¯•å¤±è´¥] â†’ ä»£ç ä¿®å¤
                                              â†“                    â†‘
                                         [æµ‹è¯•é€šè¿‡] â†’ æ–‡æ¡£ç”Ÿæˆ â†-------â”˜
"""
```

### 3. å·¥å…·é“¾ç¼–æ’

```python
"""
æ•°æ®åˆ†æå·¥ä½œæµï¼š
æ•°æ®æº â†’ æ•°æ®æ¸…æ´— â†’ [éœ€è¦é¢å¤–æ•°æ®] â†’ æ•°æ®è¡¥å…… â†’ æ•°æ®åˆ†æ â†’ ç»“æœéªŒè¯ â†’ æŠ¥å‘Šç”Ÿæˆ
          â†“              â†“                              â†“
      [æ•°æ®è´¨é‡å·®] â†’ äººå·¥ä»‹å…¥            [ç»“æœå¼‚å¸¸] â†’ é‡æ–°åˆ†æ
```

## ğŸª å®é™…å¯¹æ¯”ç¤ºä¾‹

è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå…·ä½“ä¾‹å­æ¥çœ‹ LangChain å’Œ LangGraph çš„åŒºåˆ«ï¼š

### åœºæ™¯ï¼šæ™ºèƒ½å®¢æœç³»ç»Ÿ

**éœ€æ±‚ï¼š**
- ç†è§£ç”¨æˆ·é—®é¢˜
- å¦‚æœæ˜¯è®¢å•æŸ¥è¯¢ï¼Œè°ƒç”¨è®¢å•API
- å¦‚æœæ˜¯æŠ€æœ¯é—®é¢˜ï¼Œæœç´¢çŸ¥è¯†åº“
- å¦‚æœç½®ä¿¡åº¦ä½ï¼Œè¦æ±‚æ¾„æ¸…
- å¦‚æœå¤šæ¬¡å¤±è´¥ï¼Œè½¬äººå·¥

**LangChain å®ç°ï¼š**
```python
# ğŸ˜• å¤æ‚çš„åµŒå¥—é€»è¾‘
from langchain.chains import SequentialChain

def langchain_customer_service(user_input):
    # æ­¥éª¤ 1: æ„å›¾è¯†åˆ«
    intent_chain = LLMChain(llm=llm, prompt=intent_prompt)
    intent = intent_chain.run(user_input)

    if intent == "order_inquiry":
        # è®¢å•æŸ¥è¯¢é“¾
        order_chain = LLMChain(llm=llm, prompt=order_prompt)
        result = order_chain.run(user_input)
    elif intent == "technical":
        # æŠ€æœ¯æ”¯æŒé“¾
        tech_chain = LLMChain(llm=llm, prompt=tech_prompt)
        result = tech_chain.run(user_input)
    else:
        # ğŸ˜° å¦‚ä½•å¤„ç†éœ€è¦æ¾„æ¸…çš„æƒ…å†µï¼Ÿ
        # ğŸ˜° å¦‚ä½•å¤„ç†å¤šè½®å¯¹è¯ï¼Ÿ
        # ğŸ˜° å¦‚ä½•å¤„ç†é‡è¯•é€»è¾‘ï¼Ÿ
        pass

    return result
```

**LangGraph å®ç°ï¼š**
```python
from langgraph.graph import StateGraph, END
from typing import TypedDict

class CustomerServiceState(TypedDict):
    user_input: str
    intent: str
    confidence: float
    attempts: int
    clarification_needed: bool
    result: str

def analyze_intent(state: CustomerServiceState) -> CustomerServiceState:
    # æ„å›¾åˆ†æé€»è¾‘
    intent, confidence = analyze_user_intent(state["user_input"])

    return {
        **state,
        "intent": intent,
        "confidence": confidence,
        "clarification_needed": confidence < 0.7
    }

def handle_order_inquiry(state: CustomerServiceState) -> CustomerServiceState:
    # å¤„ç†è®¢å•æŸ¥è¯¢
    result = query_order_system(state["user_input"])
    return {**state, "result": result}

def handle_technical_issue(state: CustomerServiceState) -> CustomerServiceState:
    # å¤„ç†æŠ€æœ¯é—®é¢˜
    result = search_knowledge_base(state["user_input"])
    return {**state, "result": result}

def ask_clarification(state: CustomerServiceState) -> CustomerServiceState:
    # è¦æ±‚æ¾„æ¸…
    return {
        **state,
        "result": "æŠ±æ­‰ï¼Œæˆ‘éœ€è¦æ›´å¤šä¿¡æ¯ã€‚æ‚¨èƒ½è¯¦ç»†è¯´æ˜ä¸€ä¸‹å—ï¼Ÿ",
        "clarification_needed": False
    }

def route_by_intent(state: CustomerServiceState) -> str:
    """æ ¹æ®æ„å›¾è·¯ç”±"""
    if state["clarification_needed"]:
        return "clarify"
    elif state["intent"] == "order_inquiry":
        return "handle_order"
    elif state["intent"] == "technical":
        return "handle_tech"
    else:
        return "general_response"

# æ„å»ºå›¾
graph = StateGraph(CustomerServiceState)
graph.add_node("analyze", analyze_intent)
graph.add_node("handle_order", handle_order_inquiry)
graph.add_node("handle_tech", handle_technical_issue)
graph.add_node("clarify", ask_clarification)

graph.set_entry_point("analyze")
graph.add_conditional_edges("analyze", route_by_intent, {
    "handle_order": "handle_order",
    "handle_tech": "handle_tech",
    "clarify": "clarify",
    "general_response": END
})

graph.add_edge("handle_order", END)
graph.add_edge("handle_tech", END)
graph.add_edge("clarify", END)

app = graph.compile()
```

## âœ… é€‰æ‹©æŒ‡å—

### ä½¿ç”¨ LangChain å½“ï¼š
- ç®€å•çš„çº¿æ€§å¤„ç†æµç¨‹
- æ ‡å‡†çš„ RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰åº”ç”¨
- å¿«é€ŸåŸå‹éªŒè¯
- ä¸éœ€è¦å¤æ‚çš„æ§åˆ¶æµ

### ä½¿ç”¨ LangGraph å½“ï¼š
- éœ€è¦å¤æ‚çš„æ¡ä»¶é€»è¾‘
- å¤šæ­¥éª¤ã€å¤šå·¥å…·çš„ç¼–æ’
- éœ€è¦äººæœºåä½œ
- è¦æ±‚é«˜å¯é æ€§å’Œé”™è¯¯æ¢å¤
- éœ€è¦çŠ¶æ€ç®¡ç†å’Œè®°å¿†

## ğŸ’¡ å…³é”®è¦ç‚¹

1. **LangGraph ä¸æ˜¯ LangChain çš„æ›¿ä»£å“**ï¼šå®ƒä»¬è§£å†³ä¸åŒå±‚æ¬¡çš„é—®é¢˜
2. **å›¾ vs é“¾**ï¼šå›¾èƒ½è¡¨è¾¾æ›´å¤æ‚çš„æ§åˆ¶æµ
3. **çŠ¶æ€ä¸­å¿ƒ**ï¼šç»Ÿä¸€çš„çŠ¶æ€ç®¡ç†æ˜¯ LangGraph çš„æ ¸å¿ƒ
4. **å¯è§†åŒ–**ï¼šå›¾ç»“æ„è®©å¤æ‚é€»è¾‘å˜å¾—ç›´è§‚
5. **ç”Ÿäº§å°±ç»ª**ï¼šå†…ç½®é”™è¯¯å¤„ç†ã€é‡è¯•ã€äººæœºåä½œ

## ğŸš€ ä¸‹ä¸€æ­¥

ç†è§£äº† LangGraph çš„ä»·å€¼åï¼Œæ¥ä¸‹æ¥å­¦ä¹ ï¼š
- `02-Stateè¯¦è§£.md` - æ·±å…¥ç†è§£çŠ¶æ€è®¾è®¡
- `03-Nodeså’ŒEdges.md` - æŒæ¡å›¾çš„æ„å»ºè¦ç´ 

---

*ç°åœ¨ä½ æ˜ç™½äº†ä¸ºä»€ä¹ˆéœ€è¦ LangGraphï¼Œä»¥åŠå®ƒå¦‚ä½•è§£å†³å¤æ‚ AI åº”ç”¨çš„å¼€å‘éš¾é¢˜ï¼* ğŸ‰