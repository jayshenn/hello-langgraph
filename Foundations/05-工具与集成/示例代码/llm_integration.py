#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLM é›†æˆç¤ºä¾‹

æœ¬æ–‡ä»¶æ¼”ç¤ºäº†å¦‚ä½•åœ¨ LangGraph ä¸­é›†æˆå’Œé…ç½®å„ç§ LLM æ¨¡å‹ï¼š
1. å¤šç§æ¨¡å‹æä¾›å•†é…ç½®
2. Tool Calling æœºåˆ¶
3. æµå¼è¾“å‡ºå¤„ç†
4. æ¨¡å‹é™çº§ç­–ç•¥
5. Token ç®¡ç†å’Œæˆæœ¬æ§åˆ¶

è¿è¡Œæ–¹å¼:
    python llm_integration.py
"""

import os
import time
import asyncio
import tiktoken
from typing import Dict, Any, List, Optional, Union
from dataclasses import dataclass
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.tools import tool
from langchain_core.runnables import RunnableWithFallbacks


# ================================
# 1. æ¨¡å‹é…ç½®å·¥å‚
# ================================

class ModelFactory:
    """LLM æ¨¡å‹å·¥å‚"""

    @staticmethod
    def create_openai_model(model_name: str = "gpt-3.5-turbo", **kwargs):
        """åˆ›å»º OpenAI æ¨¡å‹"""
        try:
            from langchain_openai import ChatOpenAI

            default_config = {
                "model": model_name,
                "temperature": 0.1,
                "max_tokens": 1000,
                "timeout": 30,
                "max_retries": 2
            }
            default_config.update(kwargs)

            return ChatOpenAI(**default_config)
        except ImportError:
            print("è­¦å‘Š: langchain_openai æœªå®‰è£…ï¼Œè¿”å›æ¨¡æ‹Ÿæ¨¡å‹")
            return MockLLM("OpenAI", model_name)

    @staticmethod
    def create_anthropic_model(model_name: str = "claude-3-haiku-20240307", **kwargs):
        """åˆ›å»º Anthropic æ¨¡å‹"""
        try:
            from langchain_anthropic import ChatAnthropic

            default_config = {
                "model": model_name,
                "temperature": 0.1,
                "max_tokens": 1000
            }
            default_config.update(kwargs)

            return ChatAnthropic(**default_config)
        except ImportError:
            print("è­¦å‘Š: langchain_anthropic æœªå®‰è£…ï¼Œè¿”å›æ¨¡æ‹Ÿæ¨¡å‹")
            return MockLLM("Anthropic", model_name)

    @staticmethod
    def create_azure_model(deployment_name: str = "gpt-35-turbo", **kwargs):
        """åˆ›å»º Azure OpenAI æ¨¡å‹"""
        try:
            from langchain_openai import AzureChatOpenAI

            default_config = {
                "deployment_name": deployment_name,
                "temperature": 0.1,
                "max_tokens": 1000,
                "api_version": "2024-02-15-preview"
            }
            default_config.update(kwargs)

            return AzureChatOpenAI(**default_config)
        except ImportError:
            print("è­¦å‘Š: Azure OpenAI é…ç½®æœªå®Œæˆï¼Œè¿”å›æ¨¡æ‹Ÿæ¨¡å‹")
            return MockLLM("Azure", deployment_name)

    @staticmethod
    def create_google_model(model_name: str = "gemini-pro", **kwargs):
        """åˆ›å»º Google æ¨¡å‹"""
        try:
            from langchain_google_genai import ChatGoogleGenerativeAI

            default_config = {
                "model": model_name,
                "temperature": 0.1
            }
            default_config.update(kwargs)

            return ChatGoogleGenerativeAI(**default_config)
        except ImportError:
            print("è­¦å‘Š: langchain_google_genai æœªå®‰è£…ï¼Œè¿”å›æ¨¡æ‹Ÿæ¨¡å‹")
            return MockLLM("Google", model_name)


# ================================
# 2. æ¨¡æ‹Ÿ LLMï¼ˆç”¨äºæ¼”ç¤ºï¼‰
# ================================

class MockLLM:
    """æ¨¡æ‹Ÿ LLM ç±»"""

    def __init__(self, provider: str, model: str):
        self.provider = provider
        self.model = model
        self.call_count = 0

    def invoke(self, messages: List) -> AIMessage:
        """æ¨¡æ‹Ÿè°ƒç”¨"""
        self.call_count += 1

        # è§£ææ¶ˆæ¯
        if isinstance(messages, list):
            last_message = messages[-1]
            if hasattr(last_message, 'content'):
                content = last_message.content
            else:
                content = str(last_message)
        else:
            content = str(messages)

        # ç”Ÿæˆæ¨¡æ‹Ÿå“åº”
        responses = {
            "hello": "Hello! I'm a mock LLM response.",
            "å¤©æ°”": f"ä»Šå¤©å¤©æ°”æ™´æœ—ï¼Œæ¸©åº¦é€‚å®œã€‚({self.provider} {self.model})",
            "è®¡ç®—": "æˆ‘å¯ä»¥å¸®æ‚¨è®¡ç®—æ•°å­¦é—®é¢˜ã€‚",
            "å·¥å…·": "æˆ‘ç†è§£æ‚¨éœ€è¦ä½¿ç”¨å·¥å…·æ¥å®Œæˆä»»åŠ¡ã€‚",
            "é»˜è®¤": f"è¿™æ˜¯æ¥è‡ª {self.provider} {self.model} çš„æ¨¡æ‹Ÿå“åº” #{self.call_count}"
        }

        # é€‰æ‹©å“åº”
        response_text = responses.get("é»˜è®¤", responses["é»˜è®¤"])
        for key, response in responses.items():
            if key in content.lower():
                response_text = response
                break

        return AIMessage(content=response_text)

    async def ainvoke(self, messages: List) -> AIMessage:
        """å¼‚æ­¥æ¨¡æ‹Ÿè°ƒç”¨"""
        await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
        return self.invoke(messages)

    def stream(self, messages: List):
        """æ¨¡æ‹Ÿæµå¼è¾“å‡º"""
        response = self.invoke(messages)
        words = response.content.split()

        for word in words:
            chunk = AIMessage(content=word + " ")
            yield chunk
            time.sleep(0.1)  # æ¨¡æ‹Ÿæµå¼å»¶è¿Ÿ

    def bind_tools(self, tools: List):
        """ç»‘å®šå·¥å…·"""
        self.tools = tools
        return MockLLMWithTools(self, tools)


class MockLLMWithTools:
    """å¸¦å·¥å…·çš„æ¨¡æ‹Ÿ LLM"""

    def __init__(self, llm: MockLLM, tools: List):
        self.llm = llm
        self.tools = tools

    def invoke(self, messages: List) -> AIMessage:
        """å¸¦å·¥å…·è°ƒç”¨çš„æ¨¡æ‹Ÿå“åº”"""
        last_message = messages[-1] if messages else ""
        content = last_message.content if hasattr(last_message, 'content') else str(last_message)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·
        tool_keywords = {
            "è®¡ç®—": "calculator",
            "å¤©æ°”": "weather",
            "æ—¶é—´": "current_time",
            "éšæœº": "random_number"
        }

        for keyword, tool_name in tool_keywords.items():
            if keyword in content:
                # æ¨¡æ‹Ÿå·¥å…·è°ƒç”¨
                tool_call = {
                    "name": tool_name,
                    "args": {"input": content},
                    "id": f"call_{int(time.time())}"
                }

                response = AIMessage(
                    content="",
                    tool_calls=[tool_call]
                )
                return response

        # æ™®é€šå“åº”
        return self.llm.invoke(messages)


# ================================
# 3. å·¥å…·å®šä¹‰
# ================================

@tool
def calculator(expression: str) -> str:
    """è®¡ç®—æ•°å­¦è¡¨è¾¾å¼

    Args:
        expression: æ•°å­¦è¡¨è¾¾å¼

    Returns:
        è®¡ç®—ç»“æœ
    """
    try:
        # å®‰å…¨çš„æ•°å­¦è®¡ç®—
        allowed_chars = set("0123456789+-*/()., ")
        if not all(c in allowed_chars for c in expression):
            return "é”™è¯¯: è¡¨è¾¾å¼åŒ…å«ä¸å…è®¸çš„å­—ç¬¦"

        result = eval(expression)
        return f"{expression} = {result}"
    except Exception as e:
        return f"è®¡ç®—é”™è¯¯: {str(e)}"


@tool
def weather_info(city: str) -> str:
    """è·å–å¤©æ°”ä¿¡æ¯

    Args:
        city: åŸå¸‚åç§°

    Returns:
        å¤©æ°”ä¿¡æ¯
    """
    # æ¨¡æ‹Ÿå¤©æ°”æ•°æ®
    weather_data = {
        "åŒ—äº¬": "æ™´å¤©ï¼Œ15Â°Cï¼Œå¾®é£",
        "ä¸Šæµ·": "å¤šäº‘ï¼Œ18Â°Cï¼Œä¸œå—é£",
        "å¹¿å·": "å°é›¨ï¼Œ22Â°Cï¼Œæ¹¿åº¦è¾ƒé«˜",
        "æ·±åœ³": "æ™´å¤©ï¼Œ25Â°Cï¼Œæµ·é£ä¹ ä¹ "
    }

    return weather_data.get(city, f"{city}çš„å¤©æ°”ä¿¡æ¯æš‚ä¸å¯ç”¨")


@tool
def current_time() -> str:
    """è·å–å½“å‰æ—¶é—´

    Returns:
        å½“å‰æ—¶é—´å­—ç¬¦ä¸²
    """
    return time.strftime("%Y-%m-%d %H:%M:%S")


@tool
def random_number(min_val: int = 1, max_val: int = 100) -> int:
    """ç”Ÿæˆéšæœºæ•°

    Args:
        min_val: æœ€å°å€¼
        max_val: æœ€å¤§å€¼

    Returns:
        éšæœºæ•°
    """
    import random
    return random.randint(min_val, max_val)


# ================================
# 4. æ¨¡å‹é…ç½®ç®¡ç†å™¨
# ================================

@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®"""
    provider: str
    model: str
    temperature: float = 0.1
    max_tokens: int = 1000
    streaming: bool = False
    tools: List = None


class ModelManager:
    """æ¨¡å‹ç®¡ç†å™¨"""

    def __init__(self):
        self.models = {}
        self.current_model = None
        self.tools = [calculator, weather_info, current_time, random_number]

    def register_model(self, name: str, config: ModelConfig):
        """æ³¨å†Œæ¨¡å‹"""
        factory_methods = {
            "openai": ModelFactory.create_openai_model,
            "anthropic": ModelFactory.create_anthropic_model,
            "azure": ModelFactory.create_azure_model,
            "google": ModelFactory.create_google_model
        }

        if config.provider not in factory_methods:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹æä¾›å•†: {config.provider}")

        # åˆ›å»ºæ¨¡å‹
        model_kwargs = {
            "temperature": config.temperature,
            "max_tokens": config.max_tokens
        }

        if config.streaming:
            model_kwargs["streaming"] = True

        model = factory_methods[config.provider](config.model, **model_kwargs)

        # ç»‘å®šå·¥å…·
        if config.tools or self.tools:
            tools_to_bind = config.tools or self.tools
            model = model.bind_tools(tools_to_bind)

        self.models[name] = {
            "model": model,
            "config": config
        }

        print(f"æ¨¡å‹ '{name}' æ³¨å†ŒæˆåŠŸ ({config.provider} {config.model})")

    def get_model(self, name: str):
        """è·å–æ¨¡å‹"""
        if name not in self.models:
            return None
        return self.models[name]["model"]

    def set_current_model(self, name: str):
        """è®¾ç½®å½“å‰æ¨¡å‹"""
        if name in self.models:
            self.current_model = name
            print(f"å½“å‰æ¨¡å‹è®¾ç½®ä¸º: {name}")
        else:
            print(f"æ¨¡å‹ '{name}' ä¸å­˜åœ¨")

    def list_models(self) -> List[Dict[str, Any]]:
        """åˆ—å‡ºæ‰€æœ‰æ¨¡å‹"""
        return [
            {
                "name": name,
                "provider": info["config"].provider,
                "model": info["config"].model,
                "current": name == self.current_model
            }
            for name, info in self.models.items()
        ]

    def create_fallback_model(self, primary: str, fallbacks: List[str]):
        """åˆ›å»ºå¸¦é™çº§çš„æ¨¡å‹"""
        primary_model = self.get_model(primary)
        fallback_models = [self.get_model(name) for name in fallbacks]

        if not primary_model:
            raise ValueError(f"ä¸»æ¨¡å‹ '{primary}' ä¸å­˜åœ¨")

        fallback_models = [model for model in fallback_models if model is not None]

        if not fallback_models:
            print("è­¦å‘Š: æ²¡æœ‰æœ‰æ•ˆçš„é™çº§æ¨¡å‹")
            return primary_model

        return RunnableWithFallbacks(
            runnable=primary_model,
            fallbacks=fallback_models
        )


# ================================
# 5. å¯¹è¯ç®¡ç†å™¨
# ================================

class ConversationManager:
    """å¯¹è¯ç®¡ç†å™¨"""

    def __init__(self, model_manager: ModelManager):
        self.model_manager = model_manager
        self.conversation_history = []
        self.system_prompt = "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ï¼Œèƒ½å¤Ÿä½¿ç”¨å·¥å…·æ¥å¸®åŠ©ç”¨æˆ·è§£å†³é—®é¢˜ã€‚"

    def set_system_prompt(self, prompt: str):
        """è®¾ç½®ç³»ç»Ÿæç¤ºè¯"""
        self.system_prompt = prompt

    def add_message(self, message):
        """æ·»åŠ æ¶ˆæ¯åˆ°å†å²"""
        self.conversation_history.append(message)

    def clear_history(self):
        """æ¸…ç©ºå¯¹è¯å†å²"""
        self.conversation_history = []

    def get_messages(self) -> List:
        """è·å–å®Œæ•´æ¶ˆæ¯åˆ—è¡¨"""
        messages = []

        # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯
        if self.system_prompt:
            messages.append(SystemMessage(content=self.system_prompt))

        # æ·»åŠ å¯¹è¯å†å²
        messages.extend(self.conversation_history)

        return messages

    def chat(self, user_input: str, model_name: str = None) -> Dict[str, Any]:
        """è¿›è¡Œå¯¹è¯"""
        # ä½¿ç”¨æŒ‡å®šæ¨¡å‹æˆ–å½“å‰æ¨¡å‹
        if model_name:
            model = self.model_manager.get_model(model_name)
        elif self.model_manager.current_model:
            model = self.model_manager.get_model(self.model_manager.current_model)
        else:
            return {"error": "æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹"}

        if not model:
            return {"error": f"æ¨¡å‹ä¸å­˜åœ¨: {model_name or self.model_manager.current_model}"}

        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        user_message = HumanMessage(content=user_input)
        self.add_message(user_message)

        try:
            # è·å–æ¨¡å‹å“åº”
            messages = self.get_messages()
            start_time = time.time()
            response = model.invoke(messages)
            duration = time.time() - start_time

            # æ£€æŸ¥å·¥å…·è°ƒç”¨
            if hasattr(response, 'tool_calls') and response.tool_calls:
                return self._handle_tool_calls(response, model, duration)
            else:
                # æ™®é€šå“åº”
                self.add_message(response)
                return {
                    "type": "text",
                    "content": response.content,
                    "duration": duration,
                    "model": model_name or self.model_manager.current_model
                }

        except Exception as e:
            return {"error": f"å¯¹è¯å¤±è´¥: {str(e)}"}

    def _handle_tool_calls(self, response, model, initial_duration: float) -> Dict[str, Any]:
        """å¤„ç†å·¥å…·è°ƒç”¨"""
        tool_results = []

        for tool_call in response.tool_calls:
            tool_name = tool_call["name"]
            tool_args = tool_call["args"]

            # æŸ¥æ‰¾å¹¶æ‰§è¡Œå·¥å…·
            tool_result = self._execute_tool(tool_name, tool_args)
            tool_results.append({
                "tool": tool_name,
                "args": tool_args,
                "result": tool_result
            })

        # å°†å·¥å…·è°ƒç”¨å’Œç»“æœæ·»åŠ åˆ°å†å²
        self.add_message(response)

        # æ„å»ºæœ€ç»ˆå“åº”
        final_response = f"æˆ‘ä½¿ç”¨äº†ä»¥ä¸‹å·¥å…·:\n"
        for result in tool_results:
            final_response += f"â€¢ {result['tool']}: {result['result']}\n"

        return {
            "type": "tool_call",
            "content": final_response,
            "tool_calls": tool_results,
            "duration": initial_duration
        }

    def _execute_tool(self, tool_name: str, args: Dict[str, Any]) -> str:
        """æ‰§è¡Œå·¥å…·"""
        tools_map = {tool.name: tool for tool in self.model_manager.tools}

        if tool_name not in tools_map:
            return f"å·¥å…· {tool_name} ä¸å­˜åœ¨"

        try:
            tool = tools_map[tool_name]
            result = tool.invoke(args)
            return str(result)
        except Exception as e:
            return f"å·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}"

    async def stream_chat(self, user_input: str, model_name: str = None):
        """æµå¼å¯¹è¯"""
        # ä½¿ç”¨æŒ‡å®šæ¨¡å‹æˆ–å½“å‰æ¨¡å‹
        if model_name:
            model = self.model_manager.get_model(model_name)
        elif self.model_manager.current_model:
            model = self.model_manager.get_model(self.model_manager.current_model)
        else:
            yield {"error": "æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹"}
            return

        if not model:
            yield {"error": f"æ¨¡å‹ä¸å­˜åœ¨: {model_name or self.model_manager.current_model}"}
            return

        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        user_message = HumanMessage(content=user_input)
        self.add_message(user_message)

        try:
            messages = self.get_messages()
            full_response = ""

            # æµå¼è·å–å“åº”
            for chunk in model.stream(messages):
                if hasattr(chunk, 'content') and chunk.content:
                    full_response += chunk.content
                    yield {
                        "type": "stream",
                        "content": chunk.content,
                        "full_content": full_response
                    }

            # æ·»åŠ å®Œæ•´å“åº”åˆ°å†å²
            self.add_message(AIMessage(content=full_response))

            yield {
                "type": "complete",
                "content": full_response
            }

        except Exception as e:
            yield {"error": f"æµå¼å¯¹è¯å¤±è´¥: {str(e)}"}


# ================================
# 6. Token ç®¡ç†å™¨
# ================================

class TokenManager:
    """Token ç®¡ç†å™¨"""

    def __init__(self):
        self.costs = {
            "gpt-4": {"input": 0.03, "output": 0.06},
            "gpt-3.5-turbo": {"input": 0.001, "output": 0.002},
            "claude-3-haiku": {"input": 0.00025, "output": 0.00125},
            "claude-3-sonnet": {"input": 0.003, "output": 0.015}
        }

    def count_tokens(self, text: str, model: str = "gpt-3.5-turbo") -> int:
        """è®¡ç®— token æ•°é‡"""
        try:
            # å°è¯•ä½¿ç”¨ tiktoken
            encoding = tiktoken.encoding_for_model(model.split("-")[0] + "-" + model.split("-")[1])
            return len(encoding.encode(text))
        except:
            # ç®€å•ä¼°ç®—ï¼šä¸­æ–‡å­—ç¬¦=1.5 tokensï¼Œè‹±æ–‡å•è¯=1 token
            chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
            other_chars = len(text) - chinese_chars
            words = len(text.split())
            return int(chinese_chars * 1.5 + words)

    def estimate_cost(self, input_tokens: int, output_tokens: int, model: str) -> Dict[str, float]:
        """ä¼°ç®—æˆæœ¬"""
        # æ¨¡ç³ŠåŒ¹é…æ¨¡å‹åç§°
        model_key = None
        for key in self.costs:
            if key in model.lower():
                model_key = key
                break

        if not model_key:
            return {"error": f"æœªçŸ¥æ¨¡å‹: {model}"}

        cost_config = self.costs[model_key]
        input_cost = (input_tokens / 1000) * cost_config["input"]
        output_cost = (output_tokens / 1000) * cost_config["output"]

        return {
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "input_cost": input_cost,
            "output_cost": output_cost,
            "total_cost": input_cost + output_cost,
            "model": model_key,
            "currency": "USD"
        }

    def track_usage(self, messages: List, response: str, model: str) -> Dict[str, Any]:
        """è·Ÿè¸ªä½¿ç”¨æƒ…å†µ"""
        # è®¡ç®—è¾“å…¥ tokens
        input_text = "\n".join([
            msg.content if hasattr(msg, 'content') else str(msg)
            for msg in messages
        ])
        input_tokens = self.count_tokens(input_text, model)

        # è®¡ç®—è¾“å‡º tokens
        output_tokens = self.count_tokens(response, model)

        # ä¼°ç®—æˆæœ¬
        cost = self.estimate_cost(input_tokens, output_tokens, model)

        return {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "model": model,
            "usage": cost,
            "messages_count": len(messages)
        }


# ================================
# 7. ç¤ºä¾‹å’Œæµ‹è¯•
# ================================

def setup_demo_models():
    """è®¾ç½®æ¼”ç¤ºæ¨¡å‹"""
    manager = ModelManager()

    # æ³¨å†Œå¤šä¸ªæ¨¡å‹é…ç½®
    configs = [
        ModelConfig("openai", "gpt-3.5-turbo", temperature=0.1, max_tokens=500),
        ModelConfig("openai", "gpt-4", temperature=0.2, max_tokens=1000),
        ModelConfig("anthropic", "claude-3-haiku-20240307", temperature=0.1),
        ModelConfig("google", "gemini-pro", temperature=0.1),
    ]

    for i, config in enumerate(configs):
        name = f"{config.provider}_{config.model.replace('-', '_')}"
        try:
            manager.register_model(name, config)
        except Exception as e:
            print(f"æ³¨å†Œæ¨¡å‹ {name} å¤±è´¥: {e}")

    # è®¾ç½®é»˜è®¤æ¨¡å‹
    available_models = manager.list_models()
    if available_models:
        manager.set_current_model(available_models[0]["name"])

    return manager


def test_basic_chat():
    """æµ‹è¯•åŸºç¡€å¯¹è¯"""
    print("\nğŸ’¬ åŸºç¡€å¯¹è¯æµ‹è¯•")
    print("=" * 50)

    # è®¾ç½®æ¨¡å‹
    manager = setup_demo_models()
    conversation = ConversationManager(manager)

    # è®¾ç½®ç³»ç»Ÿæç¤ºè¯
    conversation.set_system_prompt("ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ï¼Œæ“…é•¿æ•°å­¦è®¡ç®—å’Œæä¾›å®ç”¨ä¿¡æ¯ã€‚")

    # æµ‹è¯•å¯¹è¯
    test_inputs = [
        "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±",
        "è¯·è®¡ç®— 15 * 23",
        "åŒ—äº¬ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
        "ç°åœ¨å‡ ç‚¹äº†ï¼Ÿ"
    ]

    for user_input in test_inputs:
        print(f"\nğŸ‘¤ ç”¨æˆ·: {user_input}")
        response = conversation.chat(user_input)

        if "error" in response:
            print(f"âŒ é”™è¯¯: {response['error']}")
        else:
            print(f"ğŸ¤– åŠ©æ‰‹: {response['content']}")
            if response["type"] == "tool_call":
                print(f"ğŸ”§ ä½¿ç”¨äº† {len(response['tool_calls'])} ä¸ªå·¥å…·")


async def test_streaming_chat():
    """æµ‹è¯•æµå¼å¯¹è¯"""
    print("\nğŸŒŠ æµå¼å¯¹è¯æµ‹è¯•")
    print("=" * 50)

    manager = setup_demo_models()
    conversation = ConversationManager(manager)

    user_input = "è¯·ç”¨200å­—å·¦å³ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹"
    print(f"\nğŸ‘¤ ç”¨æˆ·: {user_input}")
    print("ğŸ¤– åŠ©æ‰‹: ", end="", flush=True)

    async for chunk in conversation.stream_chat(user_input):
        if chunk.get("type") == "stream":
            print(chunk["content"], end="", flush=True)
        elif chunk.get("type") == "complete":
            print("\nâœ… æµå¼è¾“å‡ºå®Œæˆ")
        elif "error" in chunk:
            print(f"\nâŒ é”™è¯¯: {chunk['error']}")


def test_model_comparison():
    """æµ‹è¯•æ¨¡å‹å¯¹æ¯”"""
    print("\nğŸ”„ æ¨¡å‹å¯¹æ¯”æµ‹è¯•")
    print("=" * 50)

    manager = setup_demo_models()
    conversation = ConversationManager(manager)

    available_models = manager.list_models()
    test_question = "è¯·ç®€å•è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"

    print(f"æµ‹è¯•é—®é¢˜: {test_question}\n")

    for model_info in available_models:
        model_name = model_info["name"]
        print(f"ğŸ“± {model_name} ({model_info['provider']} {model_info['model']}):")

        # æ¸…ç©ºå†å²ä»¥ç¡®ä¿å…¬å¹³å¯¹æ¯”
        conversation.clear_history()

        response = conversation.chat(test_question, model_name)

        if "error" in response:
            print(f"âŒ {response['error']}")
        else:
            print(f"ğŸ’­ {response['content'][:100]}...")
            print(f"â±ï¸  è€—æ—¶: {response['duration']:.2f}s")

        print()


def test_token_management():
    """æµ‹è¯• Token ç®¡ç†"""
    print("\nğŸ“Š Token ç®¡ç†æµ‹è¯•")
    print("=" * 50)

    token_manager = TokenManager()

    # æµ‹è¯•æ–‡æœ¬
    test_texts = [
        "Hello, world!",
        "ä½ å¥½ï¼Œä¸–ç•Œï¼",
        "è¿™æ˜¯ä¸€ä¸ªæ›´é•¿çš„æµ‹è¯•æ–‡æœ¬ï¼ŒåŒ…å«ä¸­è‹±æ–‡æ··åˆå†…å®¹ã€‚This is a longer test text with mixed Chinese and English content."
    ]

    for text in test_texts:
        tokens = token_manager.count_tokens(text)
        print(f"æ–‡æœ¬: {text[:30]}...")
        print(f"Token æ•°é‡: {tokens}")

        # æ¨¡æ‹Ÿå“åº”
        response = "è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„AIå“åº”ã€‚"
        cost = token_manager.estimate_cost(tokens, token_manager.count_tokens(response), "gpt-3.5-turbo")
        print(f"ä¼°ç®—æˆæœ¬: ${cost.get('total_cost', 0):.6f} USD")
        print()


def test_fallback_model():
    """æµ‹è¯•æ¨¡å‹é™çº§"""
    print("\nğŸ”„ æ¨¡å‹é™çº§æµ‹è¯•")
    print("=" * 50)

    manager = setup_demo_models()
    available_models = [model["name"] for model in manager.list_models()]

    if len(available_models) < 2:
        print("éœ€è¦è‡³å°‘2ä¸ªæ¨¡å‹æ¥æµ‹è¯•é™çº§åŠŸèƒ½")
        return

    try:
        # åˆ›å»ºå¸¦é™çº§çš„æ¨¡å‹
        fallback_model = manager.create_fallback_model(
            primary=available_models[0],
            fallbacks=available_models[1:]
        )

        print(f"ä¸»æ¨¡å‹: {available_models[0]}")
        print(f"é™çº§æ¨¡å‹: {available_models[1:]}")

        # æµ‹è¯•è°ƒç”¨
        response = fallback_model.invoke([HumanMessage(content="ä½ å¥½")])
        print(f"å“åº”: {response.content}")

    except Exception as e:
        print(f"é™çº§æµ‹è¯•å¤±è´¥: {e}")


async def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ğŸš€ LLM é›†æˆç¤ºä¾‹æµ‹è¯•")
    print("=" * 60)

    # åŸºç¡€å¯¹è¯æµ‹è¯•
    test_basic_chat()

    # æµå¼å¯¹è¯æµ‹è¯•
    await test_streaming_chat()

    # æ¨¡å‹å¯¹æ¯”æµ‹è¯•
    test_model_comparison()

    # Token ç®¡ç†æµ‹è¯•
    test_token_management()

    # é™çº§æµ‹è¯•
    test_fallback_model()

    print("\n" + "=" * 60)
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    asyncio.run(run_all_tests())