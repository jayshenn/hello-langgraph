#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAG ç³»ç»Ÿå®ç°ç¤ºä¾‹

æœ¬æ–‡ä»¶æ¼”ç¤ºäº†å®Œæ•´çš„ RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ç³»ç»Ÿå®ç°ï¼š
1. æ–‡æ¡£å¤„ç†å’Œå‘é‡åŒ–
2. æ£€ç´¢å™¨é…ç½®å’Œä¼˜åŒ–
3. åŸºç¡€ RAG å®ç°
4. é«˜çº§ RAG æŠ€æœ¯
5. Agentic RAG ç³»ç»Ÿ

è¿è¡Œæ–¹å¼:
    python rag_system.py
"""

import os
import json
import time
import hashlib
import asyncio
from typing import Dict, Any, List, Optional, Union
from dataclasses import dataclass
from collections import deque, defaultdict

# æ¨¡æ‹Ÿ LangChain ç»„ä»¶ï¼ˆå®é™…ä½¿ç”¨æ—¶è¯·å®‰è£…ç›¸åº”åŒ…ï¼‰
try:
    from langchain_community.document_loaders import TextLoader
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_community.vectorstores import Chroma
    from langchain_openai import OpenAIEmbeddings, ChatOpenAI
    from langchain_core.documents import Document
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False
    print("è­¦å‘Š: LangChain ç»„ä»¶æœªå®‰è£…ï¼Œä½¿ç”¨æ¨¡æ‹Ÿå®ç°")


# ================================
# 1. æ¨¡æ‹Ÿç»„ä»¶ï¼ˆå½“ LangChain ä¸å¯ç”¨æ—¶ï¼‰
# ================================

class MockDocument:
    """æ¨¡æ‹Ÿæ–‡æ¡£ç±»"""
    def __init__(self, page_content: str, metadata: Dict[str, Any] = None):
        self.page_content = page_content
        self.metadata = metadata or {}


class MockEmbeddings:
    """æ¨¡æ‹ŸåµŒå…¥æ¨¡å‹"""
    def __init__(self):
        self.call_count = 0

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """åµŒå…¥æ–‡æ¡£"""
        self.call_count += len(texts)
        # ç”Ÿæˆæ¨¡æ‹Ÿå‘é‡ï¼ˆåŸºäºæ–‡æœ¬å“ˆå¸Œï¼‰
        embeddings = []
        for text in texts:
            # ç®€å•çš„æ–‡æœ¬å‘é‡åŒ–ï¼ˆå®é™…åº”ç”¨ä¸­ä¸è¦è¿™æ ·åšï¼‰
            hash_obj = hashlib.md5(text.encode())
            vector = [float(int(hash_obj.hexdigest()[i:i+2], 16)) / 255.0 for i in range(0, 32, 2)]
            embeddings.append(vector)
        return embeddings

    def embed_query(self, text: str) -> List[float]:
        """åµŒå…¥æŸ¥è¯¢"""
        return self.embed_documents([text])[0]


class MockVectorStore:
    """æ¨¡æ‹Ÿå‘é‡å­˜å‚¨"""
    def __init__(self, documents: List[MockDocument] = None, embeddings=None):
        self.documents = documents or []
        self.embeddings = embeddings or MockEmbeddings()
        self.vectors = []
        self.doc_embeddings = {}

        if documents:
            self._build_index()

    def _build_index(self):
        """æ„å»ºç´¢å¼•"""
        texts = [doc.page_content for doc in self.documents]
        self.vectors = self.embeddings.embed_documents(texts)

        for i, doc in enumerate(self.documents):
            self.doc_embeddings[i] = self.vectors[i]

    def similarity_search(self, query: str, k: int = 4) -> List[MockDocument]:
        """ç›¸ä¼¼æ€§æœç´¢"""
        if not self.documents:
            return []

        query_vector = self.embeddings.embed_query(query)

        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = []
        for i, doc_vector in enumerate(self.vectors):
            similarity = self._cosine_similarity(query_vector, doc_vector)
            similarities.append((similarity, i))

        # æ’åºå¹¶è¿”å›å‰kä¸ª
        similarities.sort(reverse=True)
        results = []
        for _, doc_idx in similarities[:k]:
            results.append(self.documents[doc_idx])

        return results

    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        norm1 = sum(a * a for a in vec1) ** 0.5
        norm2 = sum(b * b for b in vec2) ** 0.5

        if norm1 == 0 or norm2 == 0:
            return 0
        return dot_product / (norm1 * norm2)

    def add_documents(self, documents: List[MockDocument]):
        """æ·»åŠ æ–‡æ¡£"""
        self.documents.extend(documents)
        self._build_index()

    def as_retriever(self, search_kwargs: Dict[str, Any] = None):
        """åˆ›å»ºæ£€ç´¢å™¨"""
        search_kwargs = search_kwargs or {}
        return MockRetriever(self, **search_kwargs)


class MockRetriever:
    """æ¨¡æ‹Ÿæ£€ç´¢å™¨"""
    def __init__(self, vectorstore: MockVectorStore, k: int = 4):
        self.vectorstore = vectorstore
        self.k = k

    def invoke(self, query: str) -> List[MockDocument]:
        """æ£€ç´¢æ–‡æ¡£"""
        return self.vectorstore.similarity_search(query, k=self.k)


class MockLLM:
    """æ¨¡æ‹Ÿå¤§è¯­è¨€æ¨¡å‹"""
    def __init__(self, model: str = "mock-gpt"):
        self.model = model

    def invoke(self, messages: List) -> Any:
        """è°ƒç”¨æ¨¡å‹"""
        # æ¨¡æ‹Ÿå“åº”
        if isinstance(messages, list) and messages:
            content = messages[-1].get("content", "") if isinstance(messages[-1], dict) else str(messages[-1])
        else:
            content = str(messages)

        # ç”ŸæˆåŸºäºå†…å®¹çš„å“åº”
        if "ä»€ä¹ˆæ˜¯" in content or "ä»‹ç»" in content:
            return type('Response', (), {'content': f"åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ï¼Œ{content.replace('ä»€ä¹ˆæ˜¯', '').replace('ä»‹ç»', '')}æ˜¯ä¸€ä¸ªé‡è¦çš„æ¦‚å¿µ..."})()
        elif "å¦‚ä½•" in content:
            return type('Response', (), {'content': f"è¦{content.replace('å¦‚ä½•', '')}ï¼Œæ‚¨å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œ..."})()
        else:
            return type('Response', (), {'content': f"æ ¹æ®ç›¸å…³ä¿¡æ¯ï¼Œ{content}çš„ç­”æ¡ˆæ˜¯..."})()


# ================================
# 2. æ–‡æ¡£å¤„ç†ç³»ç»Ÿ
# ================================

class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨"""

    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.processed_docs = []

    def create_sample_documents(self) -> List[MockDocument]:
        """åˆ›å»ºç¤ºä¾‹æ–‡æ¡£"""
        documents = [
            MockDocument(
                page_content="""
                LangGraph æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºæœ‰çŠ¶æ€ã€å¤šå‚ä¸è€…åº”ç”¨ç¨‹åºçš„å¼ºå¤§åº“ã€‚å®ƒåŸºäº LangChain è¡¨è¾¾å¼è¯­è¨€ï¼Œ
                æä¾›äº†å®šä¹‰åŒ…å«å¾ªç¯å’Œæ¡ä»¶çš„å¤æ‚å›¾ç»“æ„çš„èƒ½åŠ›ã€‚LangGraph ç‰¹åˆ«é€‚åˆæ„å»ºéœ€è¦å¾ªç¯æ‰§è¡Œã€
                çŠ¶æ€ç®¡ç†å’Œå¤æ‚å†³ç­–æµç¨‹çš„ AI åº”ç”¨ç¨‹åºã€‚å…¶æ ¸å¿ƒç‰¹æ€§åŒ…æ‹¬çŠ¶æ€ç®¡ç†ã€å›¾å½¢å·¥ä½œæµã€
                æ¡ä»¶æ‰§è¡Œå’ŒæŒä¹…åŒ–èƒ½åŠ›ã€‚
                """,
                metadata={"source": "langgraph_intro", "category": "introduction", "length": "medium"}
            ),
            MockDocument(
                page_content="""
                åœ¨ LangGraph ä¸­ï¼ŒçŠ¶æ€(State)æ˜¯é€šè¿‡ TypedDict å®šä¹‰çš„æ ¸å¿ƒæ¦‚å¿µã€‚çŠ¶æ€ä»£è¡¨åœ¨å›¾çš„èŠ‚ç‚¹ä¹‹é—´
                ä¼ é€’çš„æ‰€æœ‰ä¿¡æ¯ã€‚æ¯ä¸ªèŠ‚ç‚¹å‡½æ•°æ¥æ”¶å½“å‰çŠ¶æ€ä½œä¸ºè¾“å…¥ï¼Œå¯ä»¥è¯»å–ã€ä¿®æ”¹çŠ¶æ€ï¼Œå¹¶è¿”å›æ›´æ–°åçš„çŠ¶æ€ã€‚
                çŠ¶æ€ç®¡ç†ç¡®ä¿äº†æ•°æ®åœ¨æ•´ä¸ªå·¥ä½œæµä¸­çš„ä¸€è‡´æ€§å’Œå¯è®¿é—®æ€§ã€‚çŠ¶æ€å¯ä»¥åŒ…å«æ¶ˆæ¯å†å²ã€ç”¨æˆ·ä¿¡æ¯ã€
                å¤„ç†ç»“æœç­‰å„ç§æ•°æ®ç±»å‹ã€‚
                """,
                metadata={"source": "langgraph_state", "category": "core_concept", "length": "medium"}
            ),
            MockDocument(
                page_content="""
                å·¥å…·(Tools)æ˜¯ LangGraph ä¸­ Agent ä¸å¤–éƒ¨ä¸–ç•Œäº¤äº’çš„é‡è¦æœºåˆ¶ã€‚å·¥å…·å¯ä»¥æ˜¯å‡½æ•°ã€APIè°ƒç”¨ã€
                æ•°æ®åº“æŸ¥è¯¢æˆ–ä»»ä½•å…¶ä»–æ“ä½œã€‚é€šè¿‡ @tool è£…é¥°å™¨æˆ–ç»§æ‰¿ BaseTool ç±»ï¼Œå¼€å‘è€…å¯ä»¥åˆ›å»ºè‡ªå®šä¹‰å·¥å…·ã€‚
                Agent æ ¹æ®ç”¨æˆ·è¾“å…¥å’Œå½“å‰ä¸Šä¸‹æ–‡å†³å®šæ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·ä»¥åŠè°ƒç”¨å“ªä¸ªå·¥å…·ã€‚å·¥å…·çš„æ­£ç¡®è®¾è®¡å’Œ
                ä½¿ç”¨æ˜¯æ„å»ºå¼ºå¤§ AI åº”ç”¨çš„å…³é”®ã€‚
                """,
                metadata={"source": "langgraph_tools", "category": "core_concept", "length": "medium"}
            ),
            MockDocument(
                page_content="""
                æ¡ä»¶è·¯ç”±(Conditional Routing)æ˜¯ LangGraph çš„é«˜çº§ç‰¹æ€§ï¼Œå…è®¸æ ¹æ®å½“å‰çŠ¶æ€æˆ–å¤„ç†ç»“æœ
                åŠ¨æ€å†³å®šä¸‹ä¸€æ­¥çš„æ‰§è¡Œè·¯å¾„ã€‚é€šè¿‡ add_conditional_edges æ–¹æ³•ï¼Œå¯ä»¥å®šä¹‰å¤æ‚çš„æ¡ä»¶é€»è¾‘ã€‚
                æ¡ä»¶å‡½æ•°æ¥æ”¶å½“å‰çŠ¶æ€å¹¶è¿”å›å­—ç¬¦ä¸²é”®ï¼Œè¯¥é”®å†³å®šäº†ä¸‹ä¸€ä¸ªè¦æ‰§è¡Œçš„èŠ‚ç‚¹ã€‚è¿™ç§æœºåˆ¶ä½¿å¾—
                å›¾å¯ä»¥æ ¹æ®è¿è¡Œæ—¶çš„æƒ…å†µè¿›è¡ŒåŠ¨æ€è°ƒæ•´ï¼Œå®ç°æ™ºèƒ½çš„åˆ†æ”¯æ‰§è¡Œã€‚
                """,
                metadata={"source": "langgraph_routing", "category": "advanced", "length": "medium"}
            ),
            MockDocument(
                page_content="""
                Human-in-the-Loop (äººåœ¨å›è·¯ä¸­)æ˜¯ LangGraph æä¾›çš„é‡è¦åŠŸèƒ½ï¼Œå…è®¸åœ¨è‡ªåŠ¨åŒ–æµç¨‹ä¸­
                é€‚æ—¶å¼•å…¥äººå·¥å¹²é¢„ã€‚é€šè¿‡ interrupt å‡½æ•°ï¼Œå¯ä»¥åœ¨å…³é”®å†³ç­–ç‚¹æš‚åœæ‰§è¡Œï¼Œç­‰å¾…äººå·¥è¾“å…¥
                æˆ–ç¡®è®¤ã€‚è¿™ç§æœºåˆ¶åœ¨éœ€è¦äººå·¥å®¡æ ¸ã€å†³ç­–æˆ–æä¾›é¢å¤–ä¿¡æ¯çš„åœºæ™¯ä¸­éå¸¸æœ‰ç”¨ã€‚
                HIL åŠŸèƒ½å¢å¼ºäº†ç³»ç»Ÿçš„å¯æ§æ€§å’Œå¯é æ€§ã€‚
                """,
                metadata={"source": "langgraph_hil", "category": "advanced", "length": "medium"}
            ),
            MockDocument(
                page_content="""
                RAG(æ£€ç´¢å¢å¼ºç”Ÿæˆ)æ˜¯ä¸€ç§å°†ä¿¡æ¯æ£€ç´¢ä¸æ–‡æœ¬ç”Ÿæˆç›¸ç»“åˆçš„æŠ€æœ¯ã€‚å®ƒå…è®¸ AI æ¨¡å‹åœ¨ç”Ÿæˆå›ç­”æ—¶
                ä»å¤–éƒ¨çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼Œä»è€Œæä¾›æ›´å‡†ç¡®ã€æ›´åŠæ—¶çš„ç­”æ¡ˆã€‚RAG ç³»ç»Ÿé€šå¸¸åŒ…æ‹¬æ–‡æ¡£é¢„å¤„ç†ã€
                å‘é‡åŒ–ã€æ£€ç´¢å’Œç”Ÿæˆå››ä¸ªä¸»è¦ç»„ä»¶ã€‚è¿™ç§æ–¹æ³•è§£å†³äº†å¤§è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†æˆªæ­¢æ—¶é—´é™åˆ¶å’Œ
                å¯èƒ½å‡ºç°çš„å¹»è§‰é—®é¢˜ã€‚
                """,
                metadata={"source": "rag_concept", "category": "technology", "length": "medium"}
            )
        ]

        return documents

    def split_documents(self, documents: List[MockDocument]) -> List[MockDocument]:
        """åˆ†å‰²æ–‡æ¡£ä¸ºè¾ƒå°çš„å—"""
        chunks = []

        for doc in documents:
            content = doc.page_content.strip()

            # ç®€å•çš„æ–‡æœ¬åˆ†å‰²ï¼ˆå®é™…ä¸­åº”ä½¿ç”¨æ›´å¤æ‚çš„åˆ†å‰²ç­–ç•¥ï¼‰
            if len(content) <= self.chunk_size:
                chunks.append(MockDocument(
                    page_content=content,
                    metadata={**doc.metadata, "chunk_id": 0, "chunk_size": len(content)}
                ))
            else:
                # åˆ†å‰²é•¿æ–‡æ¡£
                start = 0
                chunk_id = 0
                while start < len(content):
                    end = min(start + self.chunk_size, len(content))

                    # å°è¯•åœ¨å•è¯è¾¹ç•Œåˆ†å‰²
                    if end < len(content):
                        while end > start and content[end] not in [' ', '\n', 'ã€‚', 'ï¼Œ']:
                            end -= 1
                        if end == start:  # å¦‚æœæ‰¾ä¸åˆ°åˆé€‚çš„åˆ†å‰²ç‚¹
                            end = min(start + self.chunk_size, len(content))

                    chunk_content = content[start:end].strip()
                    if chunk_content:
                        chunks.append(MockDocument(
                            page_content=chunk_content,
                            metadata={
                                **doc.metadata,
                                "chunk_id": chunk_id,
                                "chunk_size": len(chunk_content),
                                "original_length": len(content)
                            }
                        ))
                        chunk_id += 1

                    start = max(end - self.chunk_overlap, start + 1)

        self.processed_docs = chunks
        return chunks

    def enhance_metadata(self, documents: List[MockDocument]) -> List[MockDocument]:
        """å¢å¼ºæ–‡æ¡£å…ƒæ•°æ®"""
        enhanced_docs = []

        for doc in documents:
            content = doc.page_content
            enhanced_metadata = {
                **doc.metadata,
                "word_count": len(content.split()),
                "char_count": len(content),
                "has_chinese": any('\u4e00' <= c <= '\u9fff' for c in content),
                "has_technical_terms": any(term in content.lower() for term in [
                    'langgraph', 'rag', 'ai', 'agent', 'llm', 'api'
                ]),
                "processing_timestamp": time.time()
            }

            enhanced_docs.append(MockDocument(
                page_content=content,
                metadata=enhanced_metadata
            ))

        return enhanced_docs


# ================================
# 3. åŸºç¡€ RAG ç³»ç»Ÿ
# ================================

class BasicRAGSystem:
    """åŸºç¡€ RAG ç³»ç»Ÿ"""

    def __init__(self, documents: List[MockDocument] = None):
        self.documents = documents or []
        self.vectorstore = None
        self.retriever = None
        self.llm = MockLLM("rag-gpt")
        self.embeddings = MockEmbeddings()

        if documents:
            self._build_vectorstore()

    def _build_vectorstore(self):
        """æ„å»ºå‘é‡å­˜å‚¨"""
        print(f"æ„å»ºå‘é‡å­˜å‚¨ï¼Œæ–‡æ¡£æ•°é‡: {len(self.documents)}")

        self.vectorstore = MockVectorStore(self.documents, self.embeddings)
        self.retriever = self.vectorstore.as_retriever(search_kwargs={"k": 3})

        print("å‘é‡å­˜å‚¨æ„å»ºå®Œæˆ")

    def add_documents(self, documents: List[MockDocument]):
        """æ·»åŠ æ–°æ–‡æ¡£"""
        self.documents.extend(documents)
        if self.vectorstore:
            self.vectorstore.add_documents(documents)
        else:
            self._build_vectorstore()

    def retrieve(self, query: str, k: int = 3) -> List[MockDocument]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        if not self.retriever:
            return []

        try:
            docs = self.vectorstore.similarity_search(query, k=k)
            return docs
        except Exception as e:
            print(f"æ£€ç´¢å¤±è´¥: {e}")
            return []

    def generate_answer(self, query: str, context_docs: List[MockDocument]) -> str:
        """åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ"""
        if not context_docs:
            return "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"

        # æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join([
            f"æ–‡æ¡£{i+1}: {doc.page_content}"
            for i, doc in enumerate(context_docs)
        ])

        # æ„å»ºæç¤ºè¯
        prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ï¼š

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{query}

è¯·åŸºäºä¸Šä¸‹æ–‡æä¾›å‡†ç¡®ã€è¯¦ç»†çš„å›ç­”ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚

å›ç­”ï¼š"""

        try:
            response = self.llm.invoke([{"role": "user", "content": prompt}])
            return response.content
        except Exception as e:
            return f"ç”Ÿæˆå›ç­”å¤±è´¥: {str(e)}"

    def query(self, question: str) -> Dict[str, Any]:
        """å®Œæ•´çš„ RAG æŸ¥è¯¢æµç¨‹"""
        start_time = time.time()

        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        relevant_docs = self.retrieve(question)

        # 2. ç”Ÿæˆç­”æ¡ˆ
        answer = self.generate_answer(question, relevant_docs)

        # 3. å‡†å¤‡å“åº”
        response = {
            "question": question,
            "answer": answer,
            "retrieved_docs": len(relevant_docs),
            "sources": [
                {
                    "content": doc.page_content[:100] + "...",
                    "metadata": doc.metadata
                }
                for doc in relevant_docs
            ],
            "processing_time": time.time() - start_time
        }

        return response


# ================================
# 4. é«˜çº§ RAG ç³»ç»Ÿ
# ================================

class AdvancedRAGSystem(BasicRAGSystem):
    """é«˜çº§ RAG ç³»ç»Ÿ"""

    def __init__(self, documents: List[MockDocument] = None):
        super().__init__(documents)
        self.query_cache = {}
        self.retrieval_strategies = {
            "similarity": self._similarity_search,
            "mmr": self._mmr_search,
            "threshold": self._threshold_search
        }

    def _similarity_search(self, query: str, k: int = 3) -> List[MockDocument]:
        """ç›¸ä¼¼æ€§æœç´¢"""
        return self.vectorstore.similarity_search(query, k=k)

    def _mmr_search(self, query: str, k: int = 3) -> List[MockDocument]:
        """æœ€å¤§è¾¹é™…ç›¸å…³æ€§æœç´¢"""
        # ç®€åŒ–çš„ MMR å®ç°
        initial_docs = self.vectorstore.similarity_search(query, k=k*2)

        if not initial_docs:
            return []

        # é€‰æ‹©å¤šæ ·åŒ–çš„æ–‡æ¡£
        selected_docs = [initial_docs[0]]  # æœ€ç›¸å…³çš„æ–‡æ¡£
        remaining_docs = initial_docs[1:]

        while len(selected_docs) < k and remaining_docs:
            # ç®€å•çš„å¤šæ ·æ€§é€‰æ‹©ï¼šé€‰æ‹©ä¸å·²é€‰æ–‡æ¡£å·®å¼‚è¾ƒå¤§çš„
            best_doc = None
            max_diversity = -1

            for doc in remaining_docs:
                diversity = self._calculate_diversity(doc, selected_docs)
                if diversity > max_diversity:
                    max_diversity = diversity
                    best_doc = doc

            if best_doc:
                selected_docs.append(best_doc)
                remaining_docs.remove(best_doc)

        return selected_docs

    def _threshold_search(self, query: str, threshold: float = 0.7) -> List[MockDocument]:
        """é˜ˆå€¼æœç´¢"""
        # ç®€åŒ–å®ç°ï¼šè¿”å›ç›¸ä¼¼åº¦é«˜äºé˜ˆå€¼çš„æ–‡æ¡£
        docs = self.vectorstore.similarity_search(query, k=10)
        # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œåº”è¯¥åŸºäºå®é™…çš„ç›¸ä¼¼åº¦åˆ†æ•°è¿‡æ»¤
        return docs[:3]  # ç®€åŒ–å¤„ç†

    def _calculate_diversity(self, doc: MockDocument, selected_docs: List[MockDocument]) -> float:
        """è®¡ç®—æ–‡æ¡£å¤šæ ·æ€§"""
        # ç®€åŒ–çš„å¤šæ ·æ€§è®¡ç®—
        doc_words = set(doc.page_content.lower().split())

        total_overlap = 0
        for selected_doc in selected_docs:
            selected_words = set(selected_doc.page_content.lower().split())
            overlap = len(doc_words.intersection(selected_words))
            total_overlap += overlap

        # é‡å è¶Šå°‘ï¼Œå¤šæ ·æ€§è¶Šé«˜
        return 1.0 / (1.0 + total_overlap / len(doc_words))

    def enhanced_retrieve(self, query: str, strategy: str = "similarity", **kwargs) -> List[MockDocument]:
        """å¢å¼ºæ£€ç´¢"""
        if strategy not in self.retrieval_strategies:
            strategy = "similarity"

        retrieval_func = self.retrieval_strategies[strategy]
        return retrieval_func(query, **kwargs)

    def rerank_documents(self, query: str, documents: List[MockDocument]) -> List[MockDocument]:
        """é‡æ–°æ’åºæ–‡æ¡£"""
        if not documents:
            return documents

        # ç®€å•çš„é‡æ’åºï¼šåŸºäºæŸ¥è¯¢è¯åŒ¹é…
        query_words = set(query.lower().split())
        scored_docs = []

        for doc in documents:
            doc_words = set(doc.page_content.lower().split())
            match_score = len(query_words.intersection(doc_words))
            length_penalty = len(doc.page_content) / 1000  # åå¥½è¾ƒçŸ­çš„æ–‡æ¡£
            final_score = match_score - length_penalty
            scored_docs.append((final_score, doc))

        # æŒ‰åˆ†æ•°æ’åº
        scored_docs.sort(key=lambda x: x[0], reverse=True)
        return [doc for _, doc in scored_docs]

    def query_with_cache(self, question: str, use_cache: bool = True) -> Dict[str, Any]:
        """å¸¦ç¼“å­˜çš„æŸ¥è¯¢"""
        cache_key = hashlib.md5(question.encode()).hexdigest()

        # æ£€æŸ¥ç¼“å­˜
        if use_cache and cache_key in self.query_cache:
            cached_result = self.query_cache[cache_key]
            cached_result["from_cache"] = True
            return cached_result

        # æ‰§è¡ŒæŸ¥è¯¢
        result = self.query(question)

        # ç¼“å­˜ç»“æœ
        if use_cache:
            self.query_cache[cache_key] = result.copy()

        result["from_cache"] = False
        return result

    def multi_strategy_query(self, question: str) -> Dict[str, Any]:
        """å¤šç­–ç•¥æŸ¥è¯¢"""
        strategies = ["similarity", "mmr"]
        all_docs = []

        for strategy in strategies:
            docs = self.enhanced_retrieve(question, strategy=strategy, k=2)
            all_docs.extend(docs)

        # å»é‡
        unique_docs = []
        seen_content = set()
        for doc in all_docs:
            content_hash = hashlib.md5(doc.page_content.encode()).hexdigest()
            if content_hash not in seen_content:
                seen_content.add(content_hash)
                unique_docs.append(doc)

        # é‡æ’åº
        reranked_docs = self.rerank_documents(question, unique_docs)

        # ç”Ÿæˆç­”æ¡ˆ
        answer = self.generate_answer(question, reranked_docs[:3])

        return {
            "question": question,
            "answer": answer,
            "strategy": "multi_strategy",
            "total_docs_retrieved": len(all_docs),
            "unique_docs": len(unique_docs),
            "sources": [
                {
                    "content": doc.page_content[:100] + "...",
                    "metadata": doc.metadata
                }
                for doc in reranked_docs[:3]
            ]
        }


# ================================
# 5. Agentic RAG ç³»ç»Ÿ
# ================================

class AgenticRAGSystem:
    """æ™ºèƒ½ RAG ç³»ç»Ÿ"""

    def __init__(self, rag_system: AdvancedRAGSystem):
        self.rag_system = rag_system
        self.llm = MockLLM("agentic-rag")
        self.conversation_history = []

    def analyze_query(self, query: str) -> Dict[str, Any]:
        """åˆ†ææŸ¥è¯¢"""
        analysis = {
            "query_type": "factual",  # factual, how_to, comparison, etc.
            "complexity": "medium",   # low, medium, high
            "needs_retrieval": True,
            "suggested_strategy": "similarity",
            "keywords": self._extract_keywords(query)
        }

        # ç®€å•çš„æŸ¥è¯¢ç±»å‹åˆ†æ
        if any(word in query.lower() for word in ["å¦‚ä½•", "æ€ä¹ˆ", "how to"]):
            analysis["query_type"] = "how_to"
            analysis["suggested_strategy"] = "mmr"
        elif any(word in query.lower() for word in ["æ¯”è¾ƒ", "åŒºåˆ«", "vs", "difference"]):
            analysis["query_type"] = "comparison"
            analysis["suggested_strategy"] = "multi_strategy"
        elif any(word in query.lower() for word in ["ä»€ä¹ˆæ˜¯", "ä»‹ç»", "what is"]):
            analysis["query_type"] = "definition"

        return analysis

    def _extract_keywords(self, query: str) -> List[str]:
        """æå–å…³é”®è¯"""
        # ç®€åŒ–çš„å…³é”®è¯æå–
        stop_words = {"çš„", "æ˜¯", "åœ¨", "å’Œ", "æˆ–", "ä¸", "äº†", "å—", "å‘¢", "a", "an", "the", "is", "are", "in", "on", "at"}
        words = [word.lower() for word in query.split() if word.lower() not in stop_words]
        return words

    def iterative_retrieval(self, query: str, max_iterations: int = 3) -> Dict[str, Any]:
        """è¿­ä»£æ£€ç´¢"""
        all_results = []
        current_query = query
        confidence_scores = []

        for iteration in range(max_iterations):
            print(f"è¿­ä»£ {iteration + 1}: {current_query}")

            # æ£€ç´¢æ–‡æ¡£
            docs = self.rag_system.enhanced_retrieve(current_query, k=3)

            # è¯„ä¼°æ£€ç´¢è´¨é‡
            confidence = self._evaluate_retrieval_quality(current_query, docs)
            confidence_scores.append(confidence)

            result = {
                "iteration": iteration + 1,
                "query": current_query,
                "docs_found": len(docs),
                "confidence": confidence,
                "docs": docs
            }
            all_results.append(result)

            # å¦‚æœç½®ä¿¡åº¦è¶³å¤Ÿé«˜ï¼Œåœæ­¢è¿­ä»£
            if confidence > 0.8:
                break

            # æ”¹è¿›æŸ¥è¯¢
            if iteration < max_iterations - 1:
                current_query = self._refine_query(current_query, docs)

        # é€‰æ‹©æœ€ä½³ç»“æœ
        best_result = max(all_results, key=lambda x: x["confidence"])

        return {
            "final_query": best_result["query"],
            "best_docs": best_result["docs"],
            "iterations": len(all_results),
            "confidence_progression": confidence_scores,
            "all_results": all_results
        }

    def _evaluate_retrieval_quality(self, query: str, docs: List[MockDocument]) -> float:
        """è¯„ä¼°æ£€ç´¢è´¨é‡"""
        if not docs:
            return 0.0

        query_words = set(query.lower().split())
        total_relevance = 0

        for doc in docs:
            doc_words = set(doc.page_content.lower().split())
            relevance = len(query_words.intersection(doc_words)) / len(query_words)
            total_relevance += relevance

        return min(total_relevance / len(docs), 1.0)

    def _refine_query(self, original_query: str, docs: List[MockDocument]) -> str:
        """æ”¹è¿›æŸ¥è¯¢"""
        if not docs:
            return original_query

        # ä»æ–‡æ¡£ä¸­æå–ç›¸å…³æœ¯è¯­
        doc_text = " ".join([doc.page_content for doc in docs])
        doc_words = doc_text.lower().split()

        # æ‰¾åˆ°é¢‘ç¹å‡ºç°çš„æœ¯è¯­
        word_freq = defaultdict(int)
        for word in doc_words:
            if len(word) > 3:  # å¿½ç•¥çŸ­è¯
                word_freq[word] += 1

        # æ·»åŠ é«˜é¢‘æœ¯è¯­åˆ°æŸ¥è¯¢ä¸­
        frequent_terms = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:2]
        additional_terms = [term for term, _ in frequent_terms if term not in original_query.lower()]

        if additional_terms:
            refined_query = f"{original_query} {' '.join(additional_terms[:1])}"
            return refined_query

        return original_query

    def conversational_rag(self, query: str) -> Dict[str, Any]:
        """å¯¹è¯å¼ RAG"""
        # åˆ†ææŸ¥è¯¢
        analysis = self.analyze_query(query)

        # è€ƒè™‘å¯¹è¯å†å²
        context_aware_query = self._build_context_aware_query(query)

        # æ‰§è¡Œæ£€ç´¢
        if analysis["complexity"] == "high":
            retrieval_result = self.iterative_retrieval(context_aware_query)
            docs = retrieval_result["best_docs"]
        else:
            strategy = analysis["suggested_strategy"]
            docs = self.rag_system.enhanced_retrieve(context_aware_query, strategy=strategy)

        # ç”Ÿæˆç­”æ¡ˆ
        answer = self.rag_system.generate_answer(query, docs)

        # æ·»åŠ åˆ°å¯¹è¯å†å²
        self.conversation_history.append({
            "query": query,
            "answer": answer,
            "timestamp": time.time()
        })

        return {
            "query": query,
            "context_aware_query": context_aware_query,
            "answer": answer,
            "analysis": analysis,
            "docs_used": len(docs),
            "conversation_turn": len(self.conversation_history)
        }

    def _build_context_aware_query(self, query: str) -> str:
        """æ„å»ºä¸Šä¸‹æ–‡æ„ŸçŸ¥æŸ¥è¯¢"""
        if not self.conversation_history:
            return query

        # è·å–æœ€è¿‘çš„å¯¹è¯å†å²
        recent_history = self.conversation_history[-2:]  # æœ€è¿‘2è½®å¯¹è¯

        # æå–ä¸Šä¸‹æ–‡å…³é”®è¯
        context_keywords = []
        for turn in recent_history:
            keywords = self._extract_keywords(turn["query"])
            context_keywords.extend(keywords)

        # å¦‚æœå½“å‰æŸ¥è¯¢å¾ˆçŸ­ä¸”å¯èƒ½éœ€è¦ä¸Šä¸‹æ–‡
        if len(query.split()) <= 3 and context_keywords:
            # æ·»åŠ ç›¸å…³çš„ä¸Šä¸‹æ–‡å…³é”®è¯
            relevant_context = context_keywords[:2]  # é™åˆ¶ä¸Šä¸‹æ–‡å…³é”®è¯æ•°é‡
            enhanced_query = f"{query} {' '.join(relevant_context)}"
            return enhanced_query

        return query


# ================================
# 6. RAG è¯„ä¼°å’Œä¼˜åŒ–
# ================================

class RAGEvaluator:
    """RAG ç³»ç»Ÿè¯„ä¼°å™¨"""

    def __init__(self):
        self.test_queries = [
            "ä»€ä¹ˆæ˜¯ LangGraphï¼Ÿ",
            "å¦‚ä½•åœ¨ LangGraph ä¸­ç®¡ç†çŠ¶æ€ï¼Ÿ",
            "LangGraph çš„å·¥å…·æœºåˆ¶æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ",
            "æ¡ä»¶è·¯ç”±å’Œæ™®é€šè·¯ç”±çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ",
            "RAG ç³»ç»Ÿçš„ä¸»è¦ç»„ä»¶æœ‰å“ªäº›ï¼Ÿ"
        ]

    def evaluate_system(self, rag_system: BasicRAGSystem) -> Dict[str, Any]:
        """è¯„ä¼° RAG ç³»ç»Ÿæ€§èƒ½"""
        results = []

        for query in self.test_queries:
            start_time = time.time()

            # æ‰§è¡ŒæŸ¥è¯¢
            result = rag_system.query(query)

            # è®¡ç®—æŒ‡æ ‡
            metrics = {
                "query": query,
                "response_time": time.time() - start_time,
                "docs_retrieved": result["retrieved_docs"],
                "answer_length": len(result["answer"]),
                "has_answer": "æŠ±æ­‰" not in result["answer"] and "æ²¡æœ‰" not in result["answer"]
            }

            results.append(metrics)

        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        total_time = sum(r["response_time"] for r in results)
        avg_docs = sum(r["docs_retrieved"] for r in results) / len(results)
        success_rate = sum(1 for r in results if r["has_answer"]) / len(results)

        return {
            "individual_results": results,
            "summary": {
                "total_queries": len(self.test_queries),
                "avg_response_time": total_time / len(results),
                "avg_docs_retrieved": avg_docs,
                "success_rate": success_rate,
                "total_evaluation_time": total_time
            }
        }

    def compare_systems(self, systems: Dict[str, BasicRAGSystem]) -> Dict[str, Any]:
        """æ¯”è¾ƒå¤šä¸ª RAG ç³»ç»Ÿ"""
        comparison_results = {}

        for name, system in systems.items():
            print(f"è¯„ä¼°ç³»ç»Ÿ: {name}")
            comparison_results[name] = self.evaluate_system(system)

        # ç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š
        report = {
            "systems": comparison_results,
            "winner": self._determine_winner(comparison_results)
        }

        return report

    def _determine_winner(self, results: Dict[str, Dict[str, Any]]) -> Dict[str, str]:
        """ç¡®å®šæœ€ä½³ç³»ç»Ÿ"""
        winners = {
            "fastest": "",
            "most_accurate": "",
            "most_comprehensive": ""
        }

        fastest_time = float('inf')
        highest_success = 0
        most_docs = 0

        for name, result in results.items():
            summary = result["summary"]

            if summary["avg_response_time"] < fastest_time:
                fastest_time = summary["avg_response_time"]
                winners["fastest"] = name

            if summary["success_rate"] > highest_success:
                highest_success = summary["success_rate"]
                winners["most_accurate"] = name

            if summary["avg_docs_retrieved"] > most_docs:
                most_docs = summary["avg_docs_retrieved"]
                winners["most_comprehensive"] = name

        return winners


# ================================
# 7. ç¤ºä¾‹å’Œæµ‹è¯•
# ================================

def create_sample_rag_systems():
    """åˆ›å»ºç¤ºä¾‹ RAG ç³»ç»Ÿ"""
    # å‡†å¤‡æ–‡æ¡£
    processor = DocumentProcessor(chunk_size=800, chunk_overlap=100)
    documents = processor.create_sample_documents()
    enhanced_docs = processor.enhance_metadata(documents)
    chunks = processor.split_documents(enhanced_docs)

    print(f"å¤„ç†æ–‡æ¡£: {len(documents)} ä¸ªåŸå§‹æ–‡æ¡£, {len(chunks)} ä¸ªæ–‡æ¡£å—")

    # åˆ›å»ºä¸åŒçš„ RAG ç³»ç»Ÿ
    systems = {
        "åŸºç¡€RAG": BasicRAGSystem(chunks),
        "é«˜çº§RAG": AdvancedRAGSystem(chunks),
    }

    return systems


def test_basic_rag():
    """æµ‹è¯•åŸºç¡€ RAG åŠŸèƒ½"""
    print("\nğŸ” åŸºç¡€ RAG æµ‹è¯•")
    print("=" * 50)

    systems = create_sample_rag_systems()
    basic_rag = systems["åŸºç¡€RAG"]

    test_queries = [
        "ä»€ä¹ˆæ˜¯ LangGraphï¼Ÿ",
        "å¦‚ä½•ä½¿ç”¨å·¥å…·ï¼Ÿ",
        "çŠ¶æ€ç®¡ç†çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ"
    ]

    for query in test_queries:
        print(f"\nğŸ¤” é—®é¢˜: {query}")
        result = basic_rag.query(query)
        print(f"ğŸ“ å›ç­”: {result['answer'][:200]}...")
        print(f"ğŸ“š ä½¿ç”¨æ–‡æ¡£: {result['retrieved_docs']} ä¸ª")
        print(f"â±ï¸  å¤„ç†æ—¶é—´: {result['processing_time']:.2f}s")


def test_advanced_rag():
    """æµ‹è¯•é«˜çº§ RAG åŠŸèƒ½"""
    print("\nğŸš€ é«˜çº§ RAG æµ‹è¯•")
    print("=" * 50)

    systems = create_sample_rag_systems()
    advanced_rag = systems["é«˜çº§RAG"]

    # æµ‹è¯•å¤šç­–ç•¥æŸ¥è¯¢
    query = "LangGraph çš„æ ¸å¿ƒç‰¹æ€§æœ‰å“ªäº›ï¼Ÿ"
    print(f"\nğŸ¯ å¤šç­–ç•¥æŸ¥è¯¢: {query}")

    result = advanced_rag.multi_strategy_query(query)
    print(f"ğŸ“ å›ç­”: {result['answer'][:200]}...")
    print(f"ğŸ“Š ç­–ç•¥: {result['strategy']}")
    print(f"ğŸ“š æ£€ç´¢æ–‡æ¡£: {result['total_docs_retrieved']} ä¸ª (å»é‡å: {result['unique_docs']} ä¸ª)")

    # æµ‹è¯•ç¼“å­˜åŠŸèƒ½
    print(f"\nğŸ’¾ æµ‹è¯•ç¼“å­˜åŠŸèƒ½")
    start_time = time.time()
    result1 = advanced_rag.query_with_cache(query)
    time1 = time.time() - start_time

    start_time = time.time()
    result2 = advanced_rag.query_with_cache(query)
    time2 = time.time() - start_time

    print(f"é¦–æ¬¡æŸ¥è¯¢: {time1:.3f}s (ç¼“å­˜: {result1.get('from_cache', False)})")
    print(f"äºŒæ¬¡æŸ¥è¯¢: {time2:.3f}s (ç¼“å­˜: {result2.get('from_cache', False)})")


def test_agentic_rag():
    """æµ‹è¯•æ™ºèƒ½ RAG ç³»ç»Ÿ"""
    print("\nğŸ§  æ™ºèƒ½ RAG æµ‹è¯•")
    print("=" * 50)

    systems = create_sample_rag_systems()
    agentic_rag = AgenticRAGSystem(systems["é«˜çº§RAG"])

    # æµ‹è¯•æŸ¥è¯¢åˆ†æ
    query = "å¦‚ä½•åœ¨ LangGraph ä¸­å®ç°æ¡ä»¶è·¯ç”±ï¼Ÿ"
    print(f"\nğŸ” æŸ¥è¯¢åˆ†æ: {query}")

    analysis = agentic_rag.analyze_query(query)
    print(f"ğŸ“Š åˆ†æç»“æœ: {json.dumps(analysis, ensure_ascii=False, indent=2)}")

    # æµ‹è¯•å¯¹è¯å¼ RAG
    print(f"\nğŸ’¬ å¯¹è¯å¼ RAG æµ‹è¯•")
    conversation_queries = [
        "ä»€ä¹ˆæ˜¯ LangGraphï¼Ÿ",
        "å®ƒçš„ä¸»è¦ç‰¹æ€§æ˜¯ä»€ä¹ˆï¼Ÿ",  # åº”è¯¥åˆ©ç”¨ä¸Šä¸‹æ–‡
        "å¦‚ä½•ä½¿ç”¨å·¥å…·åŠŸèƒ½ï¼Ÿ"
    ]

    for i, conv_query in enumerate(conversation_queries):
        print(f"\nç¬¬{i+1}è½®å¯¹è¯: {conv_query}")
        result = agentic_rag.conversational_rag(conv_query)
        print(f"ğŸ¯ ä¸Šä¸‹æ–‡æ„ŸçŸ¥æŸ¥è¯¢: {result['context_aware_query']}")
        print(f"ğŸ“ å›ç­”: {result['answer'][:150]}...")

    # æµ‹è¯•è¿­ä»£æ£€ç´¢
    print(f"\nğŸ”„ è¿­ä»£æ£€ç´¢æµ‹è¯•")
    complex_query = "å¤æ‚çš„ AI ç³»ç»Ÿè®¾è®¡"
    iteration_result = agentic_rag.iterative_retrieval(complex_query, max_iterations=2)
    print(f"ğŸ¯ æœ€ç»ˆæŸ¥è¯¢: {iteration_result['final_query']}")
    print(f"ğŸ”¢ è¿­ä»£æ¬¡æ•°: {iteration_result['iterations']}")
    print(f"ğŸ“ˆ ç½®ä¿¡åº¦å˜åŒ–: {iteration_result['confidence_progression']}")


def test_rag_evaluation():
    """æµ‹è¯• RAG ç³»ç»Ÿè¯„ä¼°"""
    print("\nğŸ“Š RAG ç³»ç»Ÿè¯„ä¼°")
    print("=" * 50)

    systems = create_sample_rag_systems()
    evaluator = RAGEvaluator()

    # æ¯”è¾ƒç³»ç»Ÿæ€§èƒ½
    comparison = evaluator.compare_systems(systems)

    print("\nğŸ“ˆ è¯„ä¼°ç»“æœ:")
    for name, results in comparison["systems"].items():
        summary = results["summary"]
        print(f"\n{name}:")
        print(f"  å¹³å‡å“åº”æ—¶é—´: {summary['avg_response_time']:.3f}s")
        print(f"  å¹³å‡æ£€ç´¢æ–‡æ¡£æ•°: {summary['avg_docs_retrieved']:.1f}")
        print(f"  æˆåŠŸç‡: {summary['success_rate']:.1%}")

    print(f"\nğŸ† æœ€ä½³ç³»ç»Ÿ:")
    winners = comparison["winner"]
    for category, winner in winners.items():
        print(f"  {category}: {winner}")


async def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ğŸ¤– RAG ç³»ç»Ÿå®ç°ç¤ºä¾‹")
    print("=" * 60)

    test_basic_rag()
    test_advanced_rag()
    test_agentic_rag()
    test_rag_evaluation()

    print("\n" + "=" * 60)
    print("ğŸ‰ æ‰€æœ‰ RAG æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    # æ£€æŸ¥ä¾èµ–
    if not LANGCHAIN_AVAILABLE:
        print("ğŸ“ æ³¨æ„: æœ¬ç¤ºä¾‹ä½¿ç”¨æ¨¡æ‹Ÿç»„ä»¶ï¼Œå®é™…ä½¿ç”¨è¯·å®‰è£… LangChain ç›¸å…³åŒ…:")
        print("pip install langchain langchain-community langchain-openai chromadb")
        print()

    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    asyncio.run(run_all_tests())