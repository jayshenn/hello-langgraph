"""
RAGåº”ç”¨ç³»ç»Ÿ - LangGraphå®Œæ•´å®ç°

è¿™ä¸ªç³»ç»Ÿå±•ç¤ºäº†å¦‚ä½•æ„å»ºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åº”ç”¨ï¼š
- æ–‡æ¡£å‘é‡åŒ–å’Œå­˜å‚¨
- æ™ºèƒ½æ£€ç´¢å’Œæ’åº
- ä¸Šä¸‹æ–‡å¢å¼ºç”Ÿæˆ
- å¼•ç”¨è¿½è¸ªå’Œè´¨é‡æ§åˆ¶

é€‚åˆå­¦ä¹ ç°ä»£AIé—®ç­”ç³»ç»Ÿçš„æ ¸å¿ƒæŠ€æœ¯ã€‚
"""

import os
import asyncio
import datetime
import hashlib
from typing import TypedDict, List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path

import numpy as np
import pandas as pd
from dotenv import load_dotenv

# LangChainç›¸å…³
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_core.documents import Document
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain.schema import BaseRetriever

# LangGraph
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# =============================================================================
# æ•°æ®æ¨¡å‹
# =============================================================================

@dataclass
class RetrievalResult:
    """æ£€ç´¢ç»“æœ"""
    content: str
    source: str
    similarity_score: float
    metadata: Dict[str, Any]
    chunk_id: str


@dataclass
class RAGAnswer:
    """RAGå›ç­”"""
    content: str
    sources: List[RetrievalResult]
    confidence: float
    processing_time: float
    query_analysis: Dict[str, Any]
    generation_metadata: Dict[str, Any]


class RAGState(TypedDict):
    """RAGç³»ç»ŸçŠ¶æ€"""
    # ç”¨æˆ·è¾“å…¥
    user_query: str
    processed_query: str

    # æŸ¥è¯¢åˆ†æ
    query_type: str
    query_intent: str
    query_complexity: float
    keywords: List[str]

    # æ£€ç´¢ç»“æœ
    retrieved_documents: List[RetrievalResult]
    reranked_documents: List[RetrievalResult]
    selected_context: str

    # ç”Ÿæˆç»“æœ
    generated_answer: str
    answer_confidence: float
    citations: List[str]

    # ç³»ç»ŸçŠ¶æ€
    step_count: int
    processing_time: float
    error_message: str
    quality_scores: Dict[str, float]


# =============================================================================
# æ–‡æ¡£å¤„ç†å’Œå‘é‡åŒ–
# =============================================================================

class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨"""
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""]
        )

    def load_documents_from_directory(self, directory_path: str) -> List[Document]:
        """ä»ç›®å½•åŠ è½½æ–‡æ¡£"""
        documents = []
        directory = Path(directory_path)

        if not directory.exists():
            print(f"âš ï¸ ç›®å½•ä¸å­˜åœ¨: {directory_path}")
            return documents

        # æ”¯æŒçš„æ–‡ä»¶æ ¼å¼
        supported_extensions = {'.txt', '.md', '.py', '.js', '.html', '.css'}

        for file_path in directory.rglob("*"):
            if file_path.is_file() and file_path.suffix.lower() in supported_extensions:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()

                    # åˆ›å»ºæ–‡æ¡£å¯¹è±¡
                    doc = Document(
                        page_content=content,
                        metadata={
                            "source": str(file_path),
                            "filename": file_path.name,
                            "file_type": file_path.suffix,
                            "file_size": len(content),
                            "last_modified": datetime.datetime.fromtimestamp(
                                file_path.stat().st_mtime
                            ).isoformat()
                        }
                    )
                    documents.append(doc)

                except Exception as e:
                    print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")

        print(f"âœ… æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
        return documents

    def chunk_documents(self, documents: List[Document]) -> List[Document]:
        """æ–‡æ¡£åˆ†å—"""
        chunks = []

        for doc in documents:
            # ä½¿ç”¨æ–‡æœ¬åˆ†å‰²å™¨è¿›è¡Œåˆ†å—
            doc_chunks = self.text_splitter.split_documents([doc])

            # ä¸ºæ¯ä¸ªå—æ·»åŠ å”¯ä¸€ID
            for i, chunk in enumerate(doc_chunks):
                chunk_id = hashlib.md5(
                    f"{doc.metadata['source']}_{i}_{chunk.page_content[:100]}".encode()
                ).hexdigest()

                chunk.metadata.update({
                    "chunk_id": chunk_id,
                    "chunk_index": i,
                    "parent_doc": doc.metadata["source"]
                })

                chunks.append(chunk)

        print(f"âœ… æ–‡æ¡£åˆ†å—å®Œæˆï¼Œå…± {len(chunks)} ä¸ªå—")
        return chunks

    def create_sample_documents(self) -> List[Document]:
        """åˆ›å»ºç¤ºä¾‹æ–‡æ¡£"""
        sample_docs = [
            Document(
                page_content="""
                äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼ŒAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œæ—¨åœ¨åˆ›å»ºèƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»æ™ºèƒ½è¡Œä¸ºçš„ç³»ç»Ÿã€‚
                AIçš„ä¸»è¦ç›®æ ‡æ˜¯å¼€å‘èƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„è®¡ç®—æœºç¨‹åºï¼Œå¦‚å­¦ä¹ ã€æ¨ç†ã€æ„ŸçŸ¥ã€è¯­è¨€ç†è§£ç­‰ã€‚

                æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦å­é¢†åŸŸï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ”¹è¿›ã€‚
                æœºå™¨å­¦ä¹ ç®—æ³•é€šè¿‡åˆ†æå¤§é‡æ•°æ®æ¥è¯†åˆ«æ¨¡å¼ï¼Œå¹¶ä½¿ç”¨è¿™äº›æ¨¡å¼æ¥é¢„æµ‹æˆ–åˆ†ç±»æ–°æ•°æ®ã€‚

                æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œå®ƒä½¿ç”¨äººå·¥ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å­¦ä¹ è¿‡ç¨‹ã€‚
                æ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è¯­éŸ³è¯†åˆ«ç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚
                """,
                metadata={
                    "source": "ai_basics.md",
                    "title": "äººå·¥æ™ºèƒ½åŸºç¡€",
                    "category": "technology",
                    "author": "AIä¸“å®¶"
                }
            ),
            Document(
                page_content="""
                è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNatural Language Processingï¼ŒNLPï¼‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œ
                ä¸“æ³¨äºä½¿è®¡ç®—æœºèƒ½å¤Ÿç†è§£ã€è§£é‡Šå’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚

                NLPçš„ä¸»è¦åº”ç”¨åŒ…æ‹¬ï¼š
                1. æœºå™¨ç¿»è¯‘ï¼šå°†ä¸€ç§è¯­è¨€çš„æ–‡æœ¬ç¿»è¯‘æˆå¦ä¸€ç§è¯­è¨€
                2. æƒ…æ„Ÿåˆ†æï¼šåˆ†ææ–‡æœ¬ä¸­è¡¨è¾¾çš„æƒ…æ„Ÿå€¾å‘
                3. æ–‡æœ¬æ‘˜è¦ï¼šè‡ªåŠ¨ç”Ÿæˆæ–‡æœ¬çš„ç®€æ´æ‘˜è¦
                4. é—®ç­”ç³»ç»Ÿï¼šå›ç­”ç”¨æˆ·ç”¨è‡ªç„¶è¯­è¨€æå‡ºçš„é—®é¢˜
                5. è¯­éŸ³è¯†åˆ«ï¼šå°†è¯­éŸ³è½¬æ¢ä¸ºæ–‡æœ¬

                ç°ä»£NLPç³»ç»Ÿå¤§å¤šåŸºäºæ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯Transformeræ¶æ„ï¼Œ
                å¦‚GPTã€BERTç­‰é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹åœ¨å„ç§NLPä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºè‰²ã€‚
                """,
                metadata={
                    "source": "nlp_guide.md",
                    "title": "è‡ªç„¶è¯­è¨€å¤„ç†æŒ‡å—",
                    "category": "technology",
                    "author": "NLPç ”ç©¶å‘˜"
                }
            ),
            Document(
                page_content="""
                æ•°æ®ç§‘å­¦æ˜¯ä¸€ä¸ªè·¨å­¦ç§‘é¢†åŸŸï¼Œç»“åˆäº†ç»Ÿè®¡å­¦ã€è®¡ç®—æœºç§‘å­¦å’Œé¢†åŸŸä¸“ä¸šçŸ¥è¯†ï¼Œ
                ä»æ•°æ®ä¸­æå–æœ‰ä»·å€¼çš„æ´å¯Ÿå’ŒçŸ¥è¯†ã€‚

                æ•°æ®ç§‘å­¦çš„å…¸å‹å·¥ä½œæµç¨‹åŒ…æ‹¬ï¼š
                1. æ•°æ®æ”¶é›†ï¼šä»å„ç§æ¥æºæ”¶é›†ç›¸å…³æ•°æ®
                2. æ•°æ®æ¸…æ´—ï¼šå¤„ç†ç¼ºå¤±å€¼ã€å¼‚å¸¸å€¼å’Œæ•°æ®è´¨é‡é—®é¢˜
                3. æ¢ç´¢æ€§æ•°æ®åˆ†æï¼šé€šè¿‡å¯è§†åŒ–å’Œç»Ÿè®¡åˆ†æç†è§£æ•°æ®
                4. ç‰¹å¾å·¥ç¨‹ï¼šåˆ›å»ºå’Œé€‰æ‹©å¯¹æ¨¡å‹æœ‰ç”¨çš„ç‰¹å¾
                5. æ¨¡å‹æ„å»ºï¼šé€‰æ‹©å’Œè®­ç»ƒé€‚å½“çš„æœºå™¨å­¦ä¹ æ¨¡å‹
                6. æ¨¡å‹è¯„ä¼°ï¼šè¯„ä¼°æ¨¡å‹æ€§èƒ½å¹¶è¿›è¡Œè°ƒä¼˜
                7. éƒ¨ç½²å’Œç›‘æ§ï¼šå°†æ¨¡å‹éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒå¹¶æŒç»­ç›‘æ§

                Pythonå’ŒRæ˜¯æ•°æ®ç§‘å­¦ä¸­æœ€å¸¸ç”¨çš„ç¼–ç¨‹è¯­è¨€ï¼Œ
                é…åˆpandasã€scikit-learnã€matplotlibç­‰åº“è¿›è¡Œæ•°æ®åˆ†æå’Œå»ºæ¨¡ã€‚
                """,
                metadata={
                    "source": "data_science.md",
                    "title": "æ•°æ®ç§‘å­¦å…¥é—¨",
                    "category": "technology",
                    "author": "æ•°æ®ç§‘å­¦å®¶"
                }
            )
        ]

        return sample_docs


class VectorStoreManager:
    """å‘é‡å­˜å‚¨ç®¡ç†å™¨"""

    def __init__(self, persist_directory: str = "./data/vector_db"):
        """åˆå§‹åŒ–å‘é‡å­˜å‚¨"""
        self.persist_directory = persist_directory
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-ada-002",
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
        self.vector_store = None
        self._ensure_directory()

    def _ensure_directory(self):
        """ç¡®ä¿å‘é‡æ•°æ®åº“ç›®å½•å­˜åœ¨"""
        Path(self.persist_directory).mkdir(parents=True, exist_ok=True)

    def initialize_vector_store(self, documents: List[Document]):
        """åˆå§‹åŒ–å‘é‡å­˜å‚¨"""
        try:
            print("ğŸ”„ æ­£åœ¨åˆ›å»ºå‘é‡å­˜å‚¨...")

            # åˆ›å»ºChromaå‘é‡æ•°æ®åº“
            self.vector_store = Chroma.from_documents(
                documents=documents,
                embedding=self.embeddings,
                persist_directory=self.persist_directory,
                collection_name="rag_collection"
            )

            # æŒä¹…åŒ–
            self.vector_store.persist()
            print(f"âœ… å‘é‡å­˜å‚¨åˆ›å»ºå®Œæˆï¼ŒåŒ…å« {len(documents)} ä¸ªæ–‡æ¡£")

        except Exception as e:
            print(f"âŒ å‘é‡å­˜å‚¨åˆ›å»ºå¤±è´¥: {e}")
            raise

    def load_existing_vector_store(self):
        """åŠ è½½å·²å­˜åœ¨çš„å‘é‡å­˜å‚¨"""
        try:
            self.vector_store = Chroma(
                persist_directory=self.persist_directory,
                embedding_function=self.embeddings,
                collection_name="rag_collection"
            )
            print("âœ… æˆåŠŸåŠ è½½ç°æœ‰å‘é‡å­˜å‚¨")
            return True
        except Exception as e:
            print(f"âš ï¸ æ— æ³•åŠ è½½ç°æœ‰å‘é‡å­˜å‚¨: {e}")
            return False

    def search_similar_documents(self, query: str, k: int = 5) -> List[RetrievalResult]:
        """æœç´¢ç›¸ä¼¼æ–‡æ¡£"""
        if not self.vector_store:
            raise ValueError("å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–")

        try:
            # æ‰§è¡Œç›¸ä¼¼åº¦æœç´¢
            results = self.vector_store.similarity_search_with_score(query, k=k)

            # è½¬æ¢ä¸ºRetrievalResultæ ¼å¼
            retrieval_results = []
            for doc, score in results:
                result = RetrievalResult(
                    content=doc.page_content,
                    source=doc.metadata.get("source", "unknown"),
                    similarity_score=float(score),
                    metadata=doc.metadata,
                    chunk_id=doc.metadata.get("chunk_id", "")
                )
                retrieval_results.append(result)

            return retrieval_results

        except Exception as e:
            print(f"âŒ ç›¸ä¼¼åº¦æœç´¢å¤±è´¥: {e}")
            return []

    def add_documents(self, documents: List[Document]):
        """æ·»åŠ æ–°æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨"""
        if not self.vector_store:
            raise ValueError("å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–")

        self.vector_store.add_documents(documents)
        self.vector_store.persist()
        print(f"âœ… æˆåŠŸæ·»åŠ  {len(documents)} ä¸ªæ–‡æ¡£")

    def get_collection_stats(self) -> Dict[str, Any]:
        """è·å–é›†åˆç»Ÿè®¡ä¿¡æ¯"""
        if not self.vector_store:
            return {"error": "å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–"}

        try:
            collection = self.vector_store._collection
            return {
                "document_count": collection.count(),
                "collection_name": collection.name,
                "persist_directory": self.persist_directory
            }
        except Exception as e:
            return {"error": str(e)}


# =============================================================================
# RAGç³»ç»ŸèŠ‚ç‚¹
# =============================================================================

def query_analysis_node(state: RAGState) -> RAGState:
    """æŸ¥è¯¢åˆ†æèŠ‚ç‚¹"""
    user_query = state["user_query"]

    # ç®€å•çš„æŸ¥è¯¢åˆ†æï¼ˆå®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„NLPæŠ€æœ¯ï¼‰
    query_length = len(user_query)
    word_count = len(user_query.split())

    # æŸ¥è¯¢ç±»å‹åˆ†æ
    query_type = "general"
    if "ä»€ä¹ˆæ˜¯" in user_query or "å®šä¹‰" in user_query:
        query_type = "definition"
    elif "å¦‚ä½•" in user_query or "æ€ä¹ˆ" in user_query:
        query_type = "how_to"
    elif "ä¸ºä»€ä¹ˆ" in user_query:
        query_type = "explanation"
    elif "æ¯”è¾ƒ" in user_query or "åŒºåˆ«" in user_query:
        query_type = "comparison"

    # æŸ¥è¯¢å¤æ‚åº¦è¯„ä¼°
    complexity = min(1.0, (word_count * 0.1 + query_length * 0.001))

    # å…³é”®è¯æå–ï¼ˆç®€åŒ–ç‰ˆï¼‰
    stop_words = {"çš„", "æ˜¯", "åœ¨", "æœ‰", "å’Œ", "ä¸", "æˆ–", "ä½†", "å¦‚æœ", "é‚£ä¹ˆ"}
    keywords = [word for word in user_query.split() if word not in stop_words and len(word) > 1]

    return {
        **state,
        "processed_query": user_query.strip(),
        "query_type": query_type,
        "query_intent": "information_seeking",
        "query_complexity": complexity,
        "keywords": keywords[:10],  # é™åˆ¶å…³é”®è¯æ•°é‡
        "step_count": state["step_count"] + 1
    }


def document_retrieval_node(state: RAGState) -> RAGState:
    """æ–‡æ¡£æ£€ç´¢èŠ‚ç‚¹"""
    query = state["processed_query"]

    # è·å–å…¨å±€å‘é‡å­˜å‚¨ç®¡ç†å™¨
    vector_manager = getattr(document_retrieval_node, 'vector_manager', None)
    if not vector_manager:
        return {
            **state,
            "error_message": "å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–",
            "step_count": state["step_count"] + 1
        }

    try:
        # æ‰§è¡Œæ£€ç´¢
        k = min(10, max(3, int(state["query_complexity"] * 15)))  # åŠ¨æ€è°ƒæ•´æ£€ç´¢æ•°é‡
        retrieved_docs = vector_manager.search_similar_documents(query, k=k)

        return {
            **state,
            "retrieved_documents": retrieved_docs,
            "step_count": state["step_count"] + 1
        }

    except Exception as e:
        return {
            **state,
            "error_message": f"æ–‡æ¡£æ£€ç´¢å¤±è´¥: {str(e)}",
            "step_count": state["step_count"] + 1
        }


def document_reranking_node(state: RAGState) -> RAGState:
    """æ–‡æ¡£é‡æ’åºèŠ‚ç‚¹"""
    retrieved_docs = state["retrieved_documents"]
    query = state["processed_query"]

    if not retrieved_docs:
        return {
            **state,
            "reranked_documents": [],
            "step_count": state["step_count"] + 1
        }

    # ç®€å•çš„é‡æ’åºç­–ç•¥ï¼šç»“åˆç›¸ä¼¼åº¦åˆ†æ•°å’Œå…ƒæ•°æ®
    def calculate_relevance_score(doc: RetrievalResult) -> float:
        base_score = 1.0 - doc.similarity_score  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°

        # æ ¹æ®æŸ¥è¯¢ç±»å‹è°ƒæ•´åˆ†æ•°
        query_type = state["query_type"]
        if query_type == "definition" and "å®šä¹‰" in doc.content:
            base_score *= 1.2
        elif query_type == "how_to" and any(word in doc.content for word in ["æ–¹æ³•", "æ­¥éª¤", "å¦‚ä½•"]):
            base_score *= 1.15

        # æ ¹æ®æ–‡æ¡£æ–°é²œåº¦è°ƒæ•´ï¼ˆå¦‚æœæœ‰æ—¶é—´æˆ³ï¼‰
        if "last_modified" in doc.metadata:
            # ç®€åŒ–çš„æ–°é²œåº¦åŠ æƒ
            base_score *= 1.05

        # æ ¹æ®æ–‡æ¡£é•¿åº¦è°ƒæ•´
        content_length = len(doc.content)
        if 200 <= content_length <= 1500:  # é€‚ä¸­é•¿åº¦çš„æ–‡æ¡£
            base_score *= 1.1

        return base_score

    # é‡æ–°æ’åº
    reranked_docs = sorted(retrieved_docs, key=calculate_relevance_score, reverse=True)

    # ä¿ç•™å‰5ä¸ªæœ€ç›¸å…³çš„æ–‡æ¡£
    top_docs = reranked_docs[:5]

    return {
        **state,
        "reranked_documents": top_docs,
        "step_count": state["step_count"] + 1
    }


def context_construction_node(state: RAGState) -> RAGState:
    """ä¸Šä¸‹æ–‡æ„å»ºèŠ‚ç‚¹"""
    reranked_docs = state["reranked_documents"]

    if not reranked_docs:
        return {
            **state,
            "selected_context": "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚",
            "step_count": state["step_count"] + 1
        }

    # æ„å»ºä¸Šä¸‹æ–‡
    context_parts = []
    total_length = 0
    max_context_length = 3000  # æ§åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦

    for i, doc in enumerate(reranked_docs, 1):
        doc_text = doc.content.strip()
        if total_length + len(doc_text) > max_context_length:
            break

        context_part = f"æ–‡æ¡£ {i} (æ¥æº: {doc.source}):\n{doc_text}\n"
        context_parts.append(context_part)
        total_length += len(context_part)

    selected_context = "\n".join(context_parts)

    return {
        **state,
        "selected_context": selected_context,
        "step_count": state["step_count"] + 1
    }


def answer_generation_node(state: RAGState) -> RAGState:
    """ç­”æ¡ˆç”ŸæˆèŠ‚ç‚¹"""
    query = state["processed_query"]
    context = state["selected_context"]
    reranked_docs = state["reranked_documents"]

    # åˆ›å»ºLLM
    llm = ChatOpenAI(
        model="gpt-3.5-turbo",
        temperature=0.3,  # è¾ƒä½çš„æ¸©åº¦ç¡®ä¿ç­”æ¡ˆæ›´å‡†ç¡®
        api_key=os.getenv("OPENAI_API_KEY")
    )

    # æ„å»ºæç¤ºè¯
    system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚è¯·åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

è¦æ±‚ï¼š
1. ç­”æ¡ˆå¿…é¡»åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜
3. å¼•ç”¨å…·ä½“çš„æ–‡æ¡£æ¥æº
4. ä¿æŒç­”æ¡ˆå‡†ç¡®ã€ç®€æ´ã€æœ‰ç”¨
5. ä½¿ç”¨ä¸­æ–‡å›ç­”

æŸ¥è¯¢ç±»å‹ï¼š{state['query_type']}
æŸ¥è¯¢æ„å›¾ï¼š{state['query_intent']}

å‚è€ƒæ–‡æ¡£ï¼š
{context}"""

    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"é—®é¢˜ï¼š{query}")
    ]

    try:
        # ç”Ÿæˆç­”æ¡ˆ
        response = llm.invoke(messages)
        generated_answer = response.content

        # ç”Ÿæˆå¼•ç”¨
        citations = []
        for doc in reranked_docs:
            citation = f"{doc.source} (ç›¸ä¼¼åº¦: {1-doc.similarity_score:.3f})"
            citations.append(citation)

        # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆç®€åŒ–ç‰ˆï¼‰
        confidence = 0.8  # åŸºç¡€ç½®ä¿¡åº¦
        if len(reranked_docs) >= 3:
            confidence += 0.1
        if state["query_complexity"] <= 0.5:
            confidence += 0.1
        confidence = min(1.0, confidence)

        return {
            **state,
            "generated_answer": generated_answer,
            "answer_confidence": confidence,
            "citations": citations,
            "step_count": state["step_count"] + 1
        }

    except Exception as e:
        return {
            **state,
            "error_message": f"ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {str(e)}",
            "generated_answer": "æŠ±æ­‰ï¼Œæ— æ³•ç”Ÿæˆç­”æ¡ˆï¼Œè¯·ç¨åé‡è¯•ã€‚",
            "answer_confidence": 0.0,
            "citations": [],
            "step_count": state["step_count"] + 1
        }


def quality_assessment_node(state: RAGState) -> RAGState:
    """è´¨é‡è¯„ä¼°èŠ‚ç‚¹"""
    answer = state["generated_answer"]
    retrieved_docs = state["retrieved_documents"]
    query = state["processed_query"]

    # è®¡ç®—å„ç§è´¨é‡æŒ‡æ ‡
    quality_scores = {}

    # 1. ç­”æ¡ˆé•¿åº¦é€‚ä¸­æ€§
    answer_length = len(answer)
    if 50 <= answer_length <= 500:
        quality_scores["length_appropriateness"] = 1.0
    elif answer_length < 50:
        quality_scores["length_appropriateness"] = 0.6
    else:
        quality_scores["length_appropriateness"] = 0.8

    # 2. æ£€ç´¢è¦†ç›–åº¦
    if retrieved_docs:
        avg_similarity = sum(1 - doc.similarity_score for doc in retrieved_docs) / len(retrieved_docs)
        quality_scores["retrieval_quality"] = avg_similarity
    else:
        quality_scores["retrieval_quality"] = 0.0

    # 3. ç­”æ¡ˆå®Œæ•´æ€§ï¼ˆç®€åŒ–è¯„ä¼°ï¼‰
    if "æŠ±æ­‰" in answer or "æ— æ³•" in answer or "ä¸çŸ¥é“" in answer:
        quality_scores["completeness"] = 0.3
    elif len(answer.split("ã€‚")) >= 2:  # è‡³å°‘ä¸¤ä¸ªå¥å­
        quality_scores["completeness"] = 0.9
    else:
        quality_scores["completeness"] = 0.7

    # 4. å¼•ç”¨è´¨é‡
    citations = state.get("citations", [])
    if citations and len(citations) > 0:
        quality_scores["citation_quality"] = min(1.0, len(citations) * 0.3)
    else:
        quality_scores["citation_quality"] = 0.0

    # ç»¼åˆè´¨é‡åˆ†æ•°
    overall_quality = sum(quality_scores.values()) / len(quality_scores)

    return {
        **state,
        "quality_scores": quality_scores,
        "step_count": state["step_count"] + 1
    }


# =============================================================================
# RAGç³»ç»Ÿå›¾æ„å»º
# =============================================================================

def create_rag_graph() -> StateGraph:
    """åˆ›å»ºRAGç³»ç»Ÿå›¾"""
    graph = StateGraph(RAGState)

    # æ·»åŠ èŠ‚ç‚¹
    graph.add_node("analyze_query", query_analysis_node)
    graph.add_node("retrieve_documents", document_retrieval_node)
    graph.add_node("rerank_documents", document_reranking_node)
    graph.add_node("construct_context", context_construction_node)
    graph.add_node("generate_answer", answer_generation_node)
    graph.add_node("assess_quality", quality_assessment_node)

    # è®¾ç½®å…¥å£ç‚¹
    graph.set_entry_point("analyze_query")

    # æ·»åŠ è¾¹
    graph.add_edge("analyze_query", "retrieve_documents")
    graph.add_edge("retrieve_documents", "rerank_documents")
    graph.add_edge("rerank_documents", "construct_context")
    graph.add_edge("construct_context", "generate_answer")
    graph.add_edge("generate_answer", "assess_quality")
    graph.add_edge("assess_quality", END)

    return graph


# =============================================================================
# RAGç³»ç»Ÿä¸»ç±»
# =============================================================================

class RAGSystem:
    """RAGç³»ç»Ÿä¸»ç±»"""

    def __init__(self, vector_db_path: str = "./data/vector_db"):
        """åˆå§‹åŒ–RAGç³»ç»Ÿ"""
        self.vector_manager = VectorStoreManager(vector_db_path)
        self.document_processor = DocumentProcessor()
        self.graph = create_rag_graph()
        self.memory_saver = MemorySaver()
        self.app = self.graph.compile(checkpointer=self.memory_saver)

        # è®¾ç½®å…¨å±€å‘é‡ç®¡ç†å™¨
        document_retrieval_node.vector_manager = self.vector_manager

        print("ğŸ¤– RAGç³»ç»Ÿå·²å¯åŠ¨")

    def initialize_knowledge_base(self, documents_path: str = None):
        """åˆå§‹åŒ–çŸ¥è¯†åº“"""
        print("ğŸ“š æ­£åœ¨åˆå§‹åŒ–çŸ¥è¯†åº“...")

        # åŠ è½½æ–‡æ¡£
        if documents_path and Path(documents_path).exists():
            documents = self.document_processor.load_documents_from_directory(documents_path)
        else:
            print("ğŸ“ ä½¿ç”¨ç¤ºä¾‹æ–‡æ¡£åˆå§‹åŒ–çŸ¥è¯†åº“")
            documents = self.document_processor.create_sample_documents()

        if not documents:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æ–‡æ¡£")
            return False

        # æ–‡æ¡£åˆ†å—
        chunks = self.document_processor.chunk_documents(documents)

        # åˆ›å»ºå‘é‡å­˜å‚¨
        self.vector_manager.initialize_vector_store(chunks)

        print("âœ… çŸ¥è¯†åº“åˆå§‹åŒ–å®Œæˆ")
        return True

    def load_existing_knowledge_base(self) -> bool:
        """åŠ è½½ç°æœ‰çŸ¥è¯†åº“"""
        return self.vector_manager.load_existing_vector_store()

    def create_initial_state(self, user_query: str) -> RAGState:
        """åˆ›å»ºåˆå§‹çŠ¶æ€"""
        return {
            "user_query": user_query,
            "processed_query": "",
            "query_type": "",
            "query_intent": "",
            "query_complexity": 0.0,
            "keywords": [],
            "retrieved_documents": [],
            "reranked_documents": [],
            "selected_context": "",
            "generated_answer": "",
            "answer_confidence": 0.0,
            "citations": [],
            "step_count": 0,
            "processing_time": 0.0,
            "error_message": "",
            "quality_scores": {}
        }

    async def ask_async(self, question: str) -> RAGAnswer:
        """å¼‚æ­¥é—®ç­”"""
        if not question.strip():
            return RAGAnswer(
                content="è¯·è¾“å…¥æœ‰æ•ˆçš„é—®é¢˜ã€‚",
                sources=[],
                confidence=0.0,
                processing_time=0.0,
                query_analysis={},
                generation_metadata={}
            )

        # åˆ›å»ºåˆå§‹çŠ¶æ€
        initial_state = self.create_initial_state(question)

        # é…ç½®
        config = {"configurable": {"thread_id": f"rag_session_{hash(question)}"}}

        try:
            start_time = datetime.datetime.now()

            # æ‰§è¡ŒRAGæµç¨‹
            result = await self.app.ainvoke(initial_state, config)

            processing_time = (datetime.datetime.now() - start_time).total_seconds()

            # æ„å»ºå›ç­”å¯¹è±¡
            answer = RAGAnswer(
                content=result["generated_answer"],
                sources=result["reranked_documents"],
                confidence=result["answer_confidence"],
                processing_time=processing_time,
                query_analysis={
                    "query_type": result["query_type"],
                    "query_intent": result["query_intent"],
                    "complexity": result["query_complexity"],
                    "keywords": result["keywords"]
                },
                generation_metadata={
                    "quality_scores": result["quality_scores"],
                    "citations": result["citations"],
                    "step_count": result["step_count"]
                }
            )

            return answer

        except Exception as e:
            return RAGAnswer(
                content=f"å¤„ç†é—®é¢˜æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                sources=[],
                confidence=0.0,
                processing_time=0.0,
                query_analysis={},
                generation_metadata={"error": str(e)}
            )

    def ask(self, question: str) -> RAGAnswer:
        """åŒæ­¥é—®ç­”"""
        return asyncio.run(self.ask_async(question))

    def add_documents_to_knowledge_base(self, documents_path: str):
        """å‘çŸ¥è¯†åº“æ·»åŠ æ–°æ–‡æ¡£"""
        documents = self.document_processor.load_documents_from_directory(documents_path)
        chunks = self.document_processor.chunk_documents(documents)
        self.vector_manager.add_documents(chunks)

    def get_knowledge_base_stats(self) -> Dict[str, Any]:
        """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
        return self.vector_manager.get_collection_stats()


# =============================================================================
# æ¼”ç¤ºå’Œæµ‹è¯•
# =============================================================================

async def demo_basic_qa():
    """æ¼”ç¤ºåŸºç¡€é—®ç­”åŠŸèƒ½"""
    print("=== åŸºç¡€é—®ç­”æ¼”ç¤º ===\n")

    # åˆå§‹åŒ–RAGç³»ç»Ÿ
    rag = RAGSystem()

    # å°è¯•åŠ è½½ç°æœ‰çŸ¥è¯†åº“ï¼Œå¦åˆ™åˆå§‹åŒ–æ–°çš„
    if not rag.load_existing_knowledge_base():
        rag.initialize_knowledge_base()

    # æµ‹è¯•é—®é¢˜
    questions = [
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
        "è‡ªç„¶è¯­è¨€å¤„ç†çš„ä¸»è¦åº”ç”¨æœ‰å“ªäº›ï¼Ÿ",
        "æ•°æ®ç§‘å­¦çš„å·¥ä½œæµç¨‹æ˜¯ä»€ä¹ˆï¼Ÿ",
        "Pythonåœ¨æ•°æ®ç§‘å­¦ä¸­çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ"
    ]

    for i, question in enumerate(questions, 1):
        print(f"é—®é¢˜ {i}: {question}")
        answer = await rag.ask_async(question)

        print(f"å›ç­”: {answer.content}")
        print(f"ç½®ä¿¡åº¦: {answer.confidence:.2f}")
        print(f"å¤„ç†æ—¶é—´: {answer.processing_time:.2f}ç§’")

        if answer.sources:
            print("å‚è€ƒæ¥æº:")
            for j, source in enumerate(answer.sources[:3], 1):
                print(f"  {j}. {source.source} (ç›¸ä¼¼åº¦: {1-source.similarity_score:.3f})")

        print("-" * 60)


def demo_knowledge_base_management():
    """æ¼”ç¤ºçŸ¥è¯†åº“ç®¡ç†åŠŸèƒ½"""
    print("=== çŸ¥è¯†åº“ç®¡ç†æ¼”ç¤º ===\n")

    rag = RAGSystem()

    # åˆå§‹åŒ–çŸ¥è¯†åº“
    print("1. åˆå§‹åŒ–çŸ¥è¯†åº“:")
    rag.initialize_knowledge_base()

    # è·å–ç»Ÿè®¡ä¿¡æ¯
    print("\n2. çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯:")
    stats = rag.get_knowledge_base_stats()
    for key, value in stats.items():
        print(f"   {key}: {value}")

    # æµ‹è¯•æœç´¢
    print("\n3. æµ‹è¯•æ£€ç´¢åŠŸèƒ½:")
    test_query = "äººå·¥æ™ºèƒ½"
    results = rag.vector_manager.search_similar_documents(test_query, k=3)

    print(f"æŸ¥è¯¢: {test_query}")
    for i, result in enumerate(results, 1):
        print(f"   ç»“æœ {i}: {result.source} (ç›¸ä¼¼åº¦: {1-result.similarity_score:.3f})")
        print(f"   å†…å®¹: {result.content[:100]}...")


def interactive_demo():
    """äº¤äº’å¼æ¼”ç¤º"""
    print("ğŸ¤– RAGæ™ºèƒ½é—®ç­”ç³»ç»Ÿ")
    print("=" * 50)
    print("è¾“å…¥ 'quit' é€€å‡ºï¼Œ'stats' æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯\n")

    # åˆå§‹åŒ–ç³»ç»Ÿ
    rag = RAGSystem()

    # åŠ è½½æˆ–åˆ›å»ºçŸ¥è¯†åº“
    if not rag.load_existing_knowledge_base():
        print("ğŸ“š é¦–æ¬¡è¿è¡Œï¼Œæ­£åœ¨åˆå§‹åŒ–çŸ¥è¯†åº“...")
        rag.initialize_knowledge_base()

    while True:
        try:
            user_input = input("ğŸ” è¯·è¾“å…¥é—®é¢˜: ").strip()

            if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
                print("ğŸ‘‹ å†è§ï¼")
                break

            if user_input.lower() == 'stats':
                stats = rag.get_knowledge_base_stats()
                print("\nğŸ“Š çŸ¥è¯†åº“ç»Ÿè®¡:")
                for key, value in stats.items():
                    print(f"   {key}: {value}")
                print()
                continue

            if not user_input:
                print("è¯·è¾“å…¥æœ‰æ•ˆé—®é¢˜ã€‚")
                continue

            print("ğŸ¤– æ­£åœ¨æ€è€ƒ...")

            # è·å–ç­”æ¡ˆ
            answer = rag.ask(user_input)

            # æ˜¾ç¤ºç»“æœ
            print(f"\nğŸ“ å›ç­”: {answer.content}")
            print(f"ğŸ¯ ç½®ä¿¡åº¦: {answer.confidence:.2f}")
            print(f"â±ï¸ å¤„ç†æ—¶é—´: {answer.processing_time:.2f}ç§’")

            if answer.sources:
                print("\nğŸ“š å‚è€ƒæ¥æº:")
                for i, source in enumerate(answer.sources[:3], 1):
                    print(f"   {i}. {source.source} (ç›¸ä¼¼åº¦: {1-source.similarity_score:.3f})")

            # æ˜¾ç¤ºè´¨é‡åˆ†æ•°
            if answer.generation_metadata.get("quality_scores"):
                print(f"\nğŸ“ˆ è´¨é‡è¯„åˆ†:")
                for metric, score in answer.generation_metadata["quality_scores"].items():
                    print(f"   {metric}: {score:.2f}")

            print("\n" + "=" * 60 + "\n")

        except KeyboardInterrupt:
            print("\nğŸ‘‹ å¯¹è¯ç»“æŸï¼")
            break
        except Exception as e:
            print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")


# =============================================================================
# ä¸»ç¨‹åº
# =============================================================================

async def main():
    """ä¸»ç¨‹åº"""
    print("ğŸš€ RAGåº”ç”¨ç³»ç»Ÿ - LangGraphå®Œæ•´ç¤ºä¾‹")
    print("=" * 50)

    while True:
        print("\né€‰æ‹©æ¼”ç¤ºæ¨¡å¼:")
        print("1. åŸºç¡€é—®ç­”æ¼”ç¤º")
        print("2. çŸ¥è¯†åº“ç®¡ç†æ¼”ç¤º")
        print("3. äº¤äº’å¼é—®ç­”")
        print("4. é€€å‡º")

        choice = input("\nè¯·é€‰æ‹© (1-4): ").strip()

        if choice == "1":
            await demo_basic_qa()

        elif choice == "2":
            demo_knowledge_base_management()

        elif choice == "3":
            interactive_demo()

        elif choice == "4":
            print("ğŸ‘‹ å†è§ï¼")
            break

        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡è¯•ã€‚")


if __name__ == "__main__":
    asyncio.run(main())


# =============================================================================
# å­¦ä¹ æŒ‡å—å’Œæ‰©å±•å»ºè®®
# =============================================================================

"""
ğŸ¯ RAGç³»ç»Ÿå­¦ä¹ è¦ç‚¹:

1. **æ–‡æ¡£å¤„ç†æµç¨‹**:
   - å¤šæ ¼å¼æ–‡æ¡£åŠ è½½å’Œè§£æ
   - æ™ºèƒ½æ–‡æ¡£åˆ†å—ç­–ç•¥
   - å…ƒæ•°æ®æå–å’Œç®¡ç†

2. **å‘é‡åŒ–æŠ€æœ¯**:
   - æ–‡æœ¬åµŒå…¥æ¨¡å‹çš„é€‰æ‹©å’Œä½¿ç”¨
   - å‘é‡æ•°æ®åº“çš„ç®¡ç†å’Œä¼˜åŒ–
   - ç›¸ä¼¼åº¦è®¡ç®—å’Œæ£€ç´¢ç­–ç•¥

3. **æ£€ç´¢ä¼˜åŒ–**:
   - æŸ¥è¯¢ç†è§£å’Œé‡å†™
   - å¤šé˜¶æ®µæ£€ç´¢å’Œé‡æ’åº
   - æ£€ç´¢ç»“æœçš„è´¨é‡è¯„ä¼°

4. **ç”Ÿæˆå¢å¼º**:
   - ä¸Šä¸‹æ–‡æ„å»ºå’Œç®¡ç†
   - æç¤ºè¯å·¥ç¨‹æŠ€å·§
   - ç”Ÿæˆè´¨é‡æ§åˆ¶

5. **ç³»ç»Ÿé›†æˆ**:
   - ç«¯åˆ°ç«¯çš„RAGæµç¨‹è®¾è®¡
   - é”™è¯¯å¤„ç†å’Œå®¹é”™æœºåˆ¶
   - æ€§èƒ½ç›‘æ§å’Œä¼˜åŒ–

ğŸ”§ æ‰©å±•å»ºè®®:

1. **é«˜çº§æ£€ç´¢æŠ€æœ¯**:
   - æ··åˆæ£€ç´¢ï¼ˆDense + Sparseï¼‰
   - å¤šè·³æ£€ç´¢å’Œæ¨ç†
   - åŠ¨æ€æ£€ç´¢ç­–ç•¥

2. **å¤šæ¨¡æ€RAG**:
   - å›¾ç‰‡å’Œè¡¨æ ¼çš„å¤„ç†
   - å¤šæ¨¡æ€åµŒå…¥æŠ€æœ¯
   - è·¨æ¨¡æ€æ£€ç´¢

3. **å®æ—¶æ›´æ–°**:
   - å¢é‡æ–‡æ¡£æ›´æ–°
   - åœ¨çº¿å­¦ä¹ å’Œé€‚åº”
   - ç¼“å­˜ç­–ç•¥ä¼˜åŒ–

4. **è¯„ä¼°å’Œç›‘æ§**:
   - è‡ªåŠ¨åŒ–è¯„ä¼°æŒ‡æ ‡
   - A/Bæµ‹è¯•æ¡†æ¶
   - ç”¨æˆ·åé¦ˆé›†æˆ

è¿™ä¸ªRAGç³»ç»Ÿä¸ºæ„å»ºæ™ºèƒ½é—®ç­”åº”ç”¨æä¾›äº†å®Œæ•´çš„æŠ€æœ¯åŸºç¡€ï¼
"""