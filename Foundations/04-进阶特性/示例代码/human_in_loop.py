#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Human-in-the-Loopç¤ºä¾‹ä»£ç 
æ¼”ç¤º LangGraph ä¸­äººæœºåä½œçš„å®ç°
"""

from langgraph.graph import StateGraph, END
from typing import TypedDict, List
import time
import json


class HILState(TypedDict):
    """Human-in-the-LoopçŠ¶æ€å®šä¹‰"""
    task_description: str
    ai_analysis: dict
    human_review_required: bool
    human_feedback: dict
    final_decision: str
    approval_chain: List[dict]
    risk_level: str


def create_approval_workflow():
    """åˆ›å»ºå®¡æ‰¹å·¥ä½œæµ"""

    def risk_assessor(state: HILState) -> HILState:
        """é£é™©è¯„ä¼°å™¨"""
        task_description = state.get("task_description", "")

        print(f"ğŸ” AIé£é™©è¯„ä¼°: {task_description}")

        # æ¨¡æ‹ŸAIé£é™©è¯„ä¼°
        risk_factors = []
        risk_score = 0.0

        # æ£€æŸ¥é«˜é£é™©å…³é”®è¯
        high_risk_keywords = ["åˆ é™¤", "è½¬è´¦", "é‡è¦", "æœºå¯†", "æ°¸ä¹…", "ä¸å¯é€†"]
        medium_risk_keywords = ["ä¿®æ”¹", "æ›´æ–°", "å‘é€", "å…¬å¼€"]

        for keyword in high_risk_keywords:
            if keyword in task_description:
                risk_factors.append(f"åŒ…å«é«˜é£é™©å…³é”®è¯: {keyword}")
                risk_score += 0.3

        for keyword in medium_risk_keywords:
            if keyword in task_description:
                risk_factors.append(f"åŒ…å«ä¸­é£é™©å…³é”®è¯: {keyword}")
                risk_score += 0.1

        # æ£€æŸ¥ä»»åŠ¡å¤æ‚åº¦
        if len(task_description) > 100:
            risk_factors.append("ä»»åŠ¡æè¿°å¤æ‚")
            risk_score += 0.1

        # ç¡®å®šé£é™©ç­‰çº§
        if risk_score >= 0.7:
            risk_level = "é«˜é£é™©"
        elif risk_score >= 0.4:
            risk_level = "ä¸­é£é™©"
        else:
            risk_level = "ä½é£é™©"

        ai_analysis = {
            "risk_score": risk_score,
            "risk_level": risk_level,
            "risk_factors": risk_factors,
            "ai_recommendation": "éœ€è¦äººå·¥å®¡æ‰¹" if risk_score >= 0.4 else "å¯ä»¥è‡ªåŠ¨æ‰§è¡Œ",
            "analysis_timestamp": time.time()
        }

        state["ai_analysis"] = ai_analysis
        state["risk_level"] = risk_level
        state["human_review_required"] = risk_score >= 0.4

        print(f"   ğŸ“Š é£é™©ç­‰çº§: {risk_level} (è¯„åˆ†: {risk_score:.2f})")
        print(f"   âš ï¸  é£é™©å› ç´ : {', '.join(risk_factors) if risk_factors else 'æ— '}")

        return state

    def human_approval_node(state: HILState) -> HILState:
        """äººå·¥å®¡æ‰¹èŠ‚ç‚¹"""
        task_description = state.get("task_description", "")
        ai_analysis = state.get("ai_analysis", {})

        print(f"\nğŸ™‹ äººå·¥å®¡æ‰¹è¯·æ±‚")
        print(f"ğŸ“‹ ä»»åŠ¡: {task_description}")
        print(f"ğŸ¤– AIå»ºè®®: {ai_analysis.get('ai_recommendation', 'æ— ')}")
        print(f"âš ï¸  é£é™©å› ç´ : {', '.join(ai_analysis.get('risk_factors', []))}")

        # æ¨¡æ‹Ÿäººå·¥å®¡æ‰¹ç•Œé¢
        approval_request = {
            "task": task_description,
            "ai_analysis": ai_analysis,
            "review_options": ["æ‰¹å‡†", "æ‹’ç»", "éœ€è¦æ›´å¤šä¿¡æ¯", "ä¿®æ”¹åæ‰¹å‡†"],
            "reviewer_guidelines": "è¯·æ ¹æ®é£é™©è¯„ä¼°ç»“æœåšå‡ºå†³ç­–"
        }

        # è¿™é‡Œåœ¨å®é™…åº”ç”¨ä¸­ä¼šä½¿ç”¨ interrupt() å‡½æ•°æš‚åœæ‰§è¡Œ
        # human_feedback = interrupt(approval_request)

        # ä¸ºæ¼”ç¤ºç›®çš„ï¼Œæˆ‘ä»¬æ¨¡æ‹Ÿäººå·¥å†³ç­–
        human_feedback = simulate_human_decision(ai_analysis)

        state["human_feedback"] = human_feedback

        # è®°å½•å®¡æ‰¹é“¾
        approval_chain = state.get("approval_chain", [])
        approval_entry = {
            "reviewer": human_feedback.get("reviewer", "æœªçŸ¥å®¡æ‰¹è€…"),
            "decision": human_feedback.get("decision", "æœªå†³å®š"),
            "reason": human_feedback.get("reason", ""),
            "timestamp": time.time()
        }
        approval_chain.append(approval_entry)
        state["approval_chain"] = approval_chain

        print(f"âœ… äººå·¥å†³ç­–: {human_feedback.get('decision', 'æœªå†³å®š')}")
        print(f"ğŸ’­ ç†ç”±: {human_feedback.get('reason', 'æ— ')}")

        return state

    def decision_processor(state: HILState) -> HILState:
        """å†³ç­–å¤„ç†å™¨"""
        human_feedback = state.get("human_feedback", {})
        ai_analysis = state.get("ai_analysis", {})
        human_review_required = state.get("human_review_required", False)

        if human_review_required:
            # åŸºäºäººå·¥åé¦ˆåšæœ€ç»ˆå†³ç­–
            decision = human_feedback.get("decision", "æ‹’ç»")
            reason = human_feedback.get("reason", "äººå·¥å®¡æ‰¹")
        else:
            # åŸºäºAIåˆ†æè‡ªåŠ¨å†³ç­–
            decision = "è‡ªåŠ¨æ‰¹å‡†"
            reason = "ä½é£é™©ä»»åŠ¡ï¼ŒAIè‡ªåŠ¨æ‰¹å‡†"

        final_decision = f"{decision}: {reason}"
        state["final_decision"] = final_decision

        print(f"\nğŸ¯ æœ€ç»ˆå†³ç­–: {final_decision}")

        return state

    def approval_router(state: HILState) -> str:
        """å®¡æ‰¹è·¯ç”±å™¨"""
        human_review_required = state.get("human_review_required", False)

        if human_review_required:
            return "human_approval"
        else:
            return "auto_decision"

    # æ„å»ºå®¡æ‰¹å·¥ä½œæµå›¾
    graph = StateGraph(HILState)

    graph.add_node("risk_assessor", risk_assessor)
    graph.add_node("human_approval", human_approval_node)
    graph.add_node("auto_decision", decision_processor)
    graph.add_node("final_processor", decision_processor)

    graph.set_entry_point("risk_assessor")

    # é£é™©è¯„ä¼°åçš„è·¯ç”±
    graph.add_conditional_edges(
        "risk_assessor",
        approval_router,
        {
            "human_approval": "human_approval",
            "auto_decision": "auto_decision"
        }
    )

    # äººå·¥å®¡æ‰¹åçš„å¤„ç†
    graph.add_edge("human_approval", "final_processor")
    graph.add_edge("auto_decision", END)
    graph.add_edge("final_processor", END)

    return graph.compile()


def simulate_human_decision(ai_analysis: dict) -> dict:
    """æ¨¡æ‹Ÿäººå·¥å†³ç­–ï¼ˆå®é™…åº”ç”¨ä¸­ç”±çœŸäººå†³ç­–ï¼‰"""
    risk_score = ai_analysis.get("risk_score", 0.0)
    risk_factors = ai_analysis.get("risk_factors", [])

    # æ¨¡æ‹Ÿä¸åŒçš„å®¡æ‰¹è€…å†³ç­–å€¾å‘
    import random

    if risk_score >= 0.8:
        # é«˜é£é™©ï¼Œé€šå¸¸æ‹’ç»
        decisions = ["æ‹’ç»", "éœ€è¦æ›´å¤šä¿¡æ¯"]
        decision = random.choice(decisions)
        reason = "é£é™©è¿‡é«˜ï¼Œä¸å»ºè®®æ‰§è¡Œ"
    elif risk_score >= 0.6:
        # ä¸­é«˜é£é™©ï¼Œå¯èƒ½æ‰¹å‡†
        decisions = ["ä¿®æ”¹åæ‰¹å‡†", "éœ€è¦æ›´å¤šä¿¡æ¯", "æ‹’ç»"]
        decision = random.choice(decisions)
        reason = "å­˜åœ¨ä¸€å®šé£é™©ï¼Œéœ€è¦è°¨æ…å¤„ç†"
    else:
        # ä¸­ä½é£é™©ï¼Œé€šå¸¸æ‰¹å‡†
        decisions = ["æ‰¹å‡†", "ä¿®æ”¹åæ‰¹å‡†"]
        decision = random.choice(decisions)
        reason = "é£é™©å¯æ§ï¼Œå¯ä»¥æ‰§è¡Œ"

    return {
        "reviewer": "é«˜çº§å®¡æ‰¹å‘˜",
        "decision": decision,
        "reason": reason,
        "review_time": time.time(),
        "considered_factors": risk_factors
    }


def create_content_moderation_system():
    """åˆ›å»ºå†…å®¹å®¡æ ¸ç³»ç»Ÿ"""

    class ModerationState(TypedDict):
        content: str
        content_type: str
        ai_moderation: dict
        human_review: dict
        moderation_result: str

    def ai_content_analyzer(state: ModerationState) -> ModerationState:
        """AIå†…å®¹åˆ†æå™¨"""
        content = state.get("content", "")
        content_type = state.get("content_type", "text")

        print(f"ğŸ” AIå†…å®¹åˆ†æ: {content_type}")
        print(f"ğŸ“ å†…å®¹é¢„è§ˆ: {content[:50]}{'...' if len(content) > 50 else ''}")

        # æ¨¡æ‹ŸAIå†…å®¹åˆ†æ
        analysis_results = {
            "toxicity_score": 0.0,
            "spam_probability": 0.0,
            "adult_content": False,
            "hate_speech": False,
            "misinformation": False,
            "detected_issues": []
        }

        # æ£€æŸ¥æœ‰å®³å†…å®¹
        harmful_keywords = ["æš´åŠ›", "ä»‡æ¨", "æ­§è§†", "æ¬ºè¯ˆ"]
        for keyword in harmful_keywords:
            if keyword in content:
                analysis_results["toxicity_score"] += 0.3
                analysis_results["detected_issues"].append(f"åŒ…å«æœ‰å®³å…³é”®è¯: {keyword}")

        # æ£€æŸ¥åƒåœ¾å†…å®¹
        spam_indicators = ["å…è´¹", "ç«‹å³", "ç‚¹å‡»", "é™æ—¶"]
        spam_count = sum(1 for indicator in spam_indicators if indicator in content)
        if spam_count >= 2:
            analysis_results["spam_probability"] = min(spam_count * 0.3, 1.0)
            analysis_results["detected_issues"].append("ç–‘ä¼¼åƒåœ¾å†…å®¹")

        # æ£€æŸ¥æˆäººå†…å®¹
        adult_keywords = ["æˆäºº", "é™åˆ¶çº§"]
        if any(keyword in content for keyword in adult_keywords):
            analysis_results["adult_content"] = True
            analysis_results["detected_issues"].append("åŒ…å«æˆäººå†…å®¹")

        # ç¡®å®šæ˜¯å¦éœ€è¦äººå·¥å®¡æ ¸
        needs_human_review = (
            analysis_results["toxicity_score"] > 0.5 or
            analysis_results["spam_probability"] > 0.6 or
            analysis_results["adult_content"] or
            len(analysis_results["detected_issues"]) > 0
        )

        ai_moderation = {
            **analysis_results,
            "confidence": 0.85,
            "needs_human_review": needs_human_review,
            "ai_recommendation": "éœ€è¦äººå·¥å®¡æ ¸" if needs_human_review else "å¯ä»¥è‡ªåŠ¨é€šè¿‡"
        }

        state["ai_moderation"] = ai_moderation

        print(f"   ğŸ¤– AIå»ºè®®: {ai_moderation['ai_recommendation']}")
        if analysis_results["detected_issues"]:
            print(f"   âš ï¸  æ£€æµ‹åˆ°çš„é—®é¢˜: {', '.join(analysis_results['detected_issues'])}")

        return state

    def human_moderation_node(state: ModerationState) -> ModerationState:
        """äººå·¥å®¡æ ¸èŠ‚ç‚¹"""
        content = state.get("content", "")
        ai_moderation = state.get("ai_moderation", {})

        print(f"\nğŸ‘¥ äººå·¥å†…å®¹å®¡æ ¸")
        print(f"ğŸ“„ å®¡æ ¸å†…å®¹: {content}")
        print(f"ğŸ¤– AIæ£€æµ‹ç»“æœ: {ai_moderation.get('detected_issues', [])}")

        # æ¨¡æ‹Ÿäººå·¥å®¡æ ¸å†³ç­–
        human_review = simulate_human_moderation(ai_moderation, content)

        state["human_review"] = human_review

        print(f"âœ… äººå·¥å®¡æ ¸ç»“æœ: {human_review.get('decision', 'æœªå†³å®š')}")
        print(f"ğŸ’­ å®¡æ ¸ç†ç”±: {human_review.get('reason', 'æ— ')}")

        return state

    def moderation_finalizer(state: ModerationState) -> ModerationState:
        """å®¡æ ¸ç»“æœç»ˆç»“å™¨"""
        ai_moderation = state.get("ai_moderation", {})
        human_review = state.get("human_review", {})

        if ai_moderation.get("needs_human_review", False):
            # åŸºäºäººå·¥å®¡æ ¸ç»“æœ
            decision = human_review.get("decision", "æ‹’ç»")
            reason = human_review.get("reason", "äººå·¥å®¡æ ¸")
        else:
            # åŸºäºAIåˆ†æè‡ªåŠ¨é€šè¿‡
            decision = "é€šè¿‡"
            reason = "AIè‡ªåŠ¨å®¡æ ¸é€šè¿‡"

        moderation_result = f"{decision}: {reason}"
        state["moderation_result"] = moderation_result

        print(f"\nğŸ¯ æœ€ç»ˆå®¡æ ¸ç»“æœ: {moderation_result}")

        return state

    def moderation_router(state: ModerationState) -> str:
        """å®¡æ ¸è·¯ç”±å™¨"""
        ai_moderation = state.get("ai_moderation", {})
        needs_human_review = ai_moderation.get("needs_human_review", False)

        return "human_moderation" if needs_human_review else "auto_approve"

    # æ„å»ºå†…å®¹å®¡æ ¸å›¾
    graph = StateGraph(ModerationState)

    graph.add_node("ai_analyzer", ai_content_analyzer)
    graph.add_node("human_moderation", human_moderation_node)
    graph.add_node("auto_approve", moderation_finalizer)
    graph.add_node("finalize", moderation_finalizer)

    graph.set_entry_point("ai_analyzer")

    graph.add_conditional_edges(
        "ai_analyzer",
        moderation_router,
        {
            "human_moderation": "human_moderation",
            "auto_approve": "auto_approve"
        }
    )

    graph.add_edge("human_moderation", "finalize")
    graph.add_edge("auto_approve", END)
    graph.add_edge("finalize", END)

    return graph.compile()


def simulate_human_moderation(ai_moderation: dict, content: str) -> dict:
    """æ¨¡æ‹Ÿäººå·¥å®¡æ ¸å†³ç­–"""
    detected_issues = ai_moderation.get("detected_issues", [])
    toxicity_score = ai_moderation.get("toxicity_score", 0.0)

    import random

    if toxicity_score > 0.7 or len(detected_issues) > 2:
        # é«˜é£é™©å†…å®¹ï¼Œé€šå¸¸æ‹’ç»
        decision = "æ‹’ç»"
        reason = "å†…å®¹è¿åç¤¾åŒºå‡†åˆ™"
    elif toxicity_score > 0.3 or len(detected_issues) > 0:
        # ä¸­é£é™©å†…å®¹ï¼Œå¯èƒ½éœ€è¦ä¿®æ”¹
        decisions = ["éœ€è¦ä¿®æ”¹", "è­¦å‘Šé€šè¿‡", "æ‹’ç»"]
        decision = random.choice(decisions)
        reason = "å†…å®¹å­˜åœ¨é—®é¢˜ï¼Œéœ€è¦å¤„ç†"
    else:
        # ä½é£é™©å†…å®¹ï¼Œé€šå¸¸é€šè¿‡
        decision = "é€šè¿‡"
        reason = "å†…å®¹ç¬¦åˆç¤¾åŒºå‡†åˆ™"

    return {
        "moderator": "å†…å®¹å®¡æ ¸å‘˜",
        "decision": decision,
        "reason": reason,
        "review_timestamp": time.time(),
        "flagged_sections": detected_issues
    }


def demo_human_in_loop():
    """æ¼”ç¤ºHuman-in-the-Loopç³»ç»Ÿ"""
    print("ğŸš€ Human-in-the-Loopæ¼”ç¤ºå¼€å§‹")
    print("=" * 60)

    # æ¼”ç¤º1: å®¡æ‰¹å·¥ä½œæµ
    print("\nğŸ¯ æ¼”ç¤º1: æ™ºèƒ½å®¡æ‰¹å·¥ä½œæµ")
    print("-" * 40)

    approval_app = create_approval_workflow()

    # æµ‹è¯•ä¸åŒé£é™©çº§åˆ«çš„ä»»åŠ¡
    test_tasks = [
        "æ›´æ–°ç”¨æˆ·ä¿¡æ¯",
        "åˆ é™¤é‡è¦æ•°æ®åº“è®°å½•",
        "å‘é€è¥é”€é‚®ä»¶ç»™æ‰€æœ‰ç”¨æˆ·",
        "å¤‡ä»½ç³»ç»Ÿæ•°æ®",
        "æ°¸ä¹…åˆ é™¤æ‰€æœ‰æœºå¯†æ–‡ä»¶"
    ]

    for i, task in enumerate(test_tasks, 1):
        print(f"\nğŸ“‹ ä»»åŠ¡ {i}: {task}")
        print("-" * 30)

        state = {
            "task_description": task,
            "ai_analysis": {},
            "human_review_required": False,
            "human_feedback": {},
            "final_decision": "",
            "approval_chain": [],
            "risk_level": ""
        }

        result = approval_app.invoke(state)

    # æ¼”ç¤º2: å†…å®¹å®¡æ ¸ç³»ç»Ÿ
    print("\n\nğŸ¯ æ¼”ç¤º2: å†…å®¹å®¡æ ¸ç³»ç»Ÿ")
    print("-" * 40)

    moderation_app = create_content_moderation_system()

    # æµ‹è¯•ä¸åŒç±»å‹çš„å†…å®¹
    test_contents = [
        {"content": "è¿™æ˜¯ä¸€æ¡æ­£å¸¸çš„æ¶ˆæ¯", "type": "text"},
        {"content": "å…è´¹è·å–ï¼ç«‹å³ç‚¹å‡»ï¼é™æ—¶ä¼˜æƒ ï¼", "type": "text"},
        {"content": "æˆ‘å¯¹æŸä¸ªç¾¤ä½“è¡¨ç¤ºä»‡æ¨å’Œæ­§è§†", "type": "text"},
        {"content": "åˆ†äº«ä¸€äº›æŠ€æœ¯çŸ¥è¯†å’Œç»éªŒ", "type": "text"},
        {"content": "è¿™é‡ŒåŒ…å«ä¸€äº›æˆäººå’Œæš´åŠ›å†…å®¹", "type": "text"}
    ]

    for i, content_item in enumerate(test_contents, 1):
        print(f"\nğŸ“„ å†…å®¹ {i}: {content_item['content']}")
        print("-" * 30)

        state = {
            "content": content_item["content"],
            "content_type": content_item["type"],
            "ai_moderation": {},
            "human_review": {},
            "moderation_result": ""
        }

        result = moderation_app.invoke(state)

    print("\nğŸ‰ Human-in-the-Loopæ¼”ç¤ºå®Œæˆï¼")


if __name__ == "__main__":
    demo_human_in_loop()