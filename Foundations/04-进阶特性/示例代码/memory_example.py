#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Memoryæœºåˆ¶ç¤ºä¾‹ä»£ç 
æ¼”ç¤º LangGraph ä¸­è®°å¿†ç®¡ç†çš„å®ç°
"""

from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from typing import TypedDict, List, Annotated
import operator
import time
import json


class MemoryState(TypedDict):
    """è®°å¿†çŠ¶æ€å®šä¹‰"""
    messages: Annotated[List[dict], operator.add]
    user_profile: dict
    conversation_summary: str
    interaction_count: int
    memory_metadata: dict


def create_smart_memory_system():
    """åˆ›å»ºæ™ºèƒ½è®°å¿†ç³»ç»Ÿ"""

    def memory_manager(state: MemoryState) -> MemoryState:
        """è®°å¿†ç®¡ç†å™¨"""
        messages = state.get("messages", [])
        interaction_count = state.get("interaction_count", 0)

        print(f"ğŸ§  è®°å¿†ç®¡ç†å™¨ï¼šå¤„ç†ç¬¬ {interaction_count + 1} æ¬¡äº¤äº’")

        # æ›´æ–°äº¤äº’è®¡æ•°
        state["interaction_count"] = interaction_count + 1

        # å¦‚æœæ¶ˆæ¯è¿‡å¤šï¼Œè¿›è¡Œæ™ºèƒ½æ‘˜è¦
        if len(messages) > 10:
            print("ğŸ“ æ¶ˆæ¯è¿‡å¤šï¼Œè¿›è¡Œæ™ºèƒ½æ‘˜è¦...")
            summary = generate_conversation_summary(messages[:-5])  # ä¿ç•™æœ€è¿‘5æ¡
            state["conversation_summary"] = summary
            state["messages"] = messages[-5:]  # åªä¿ç•™æœ€è¿‘5æ¡æ¶ˆæ¯

            # æ›´æ–°è®°å¿†å…ƒæ•°æ®
            metadata = state.get("memory_metadata", {})
            metadata.update({
                "last_summary_time": time.time(),
                "summarized_messages_count": len(messages) - 5,
                "total_interactions": interaction_count + 1
            })
            state["memory_metadata"] = metadata

        return state

    def profile_extractor(state: MemoryState) -> MemoryState:
        """ç”¨æˆ·ç”»åƒæå–å™¨"""
        messages = state.get("messages", [])
        user_profile = state.get("user_profile", {})

        print("ğŸ‘¤ æå–ç”¨æˆ·ç”»åƒä¿¡æ¯...")

        # ä»æœ€æ–°æ¶ˆæ¯ä¸­æå–ç”¨æˆ·ä¿¡æ¯
        latest_messages = messages[-3:] if len(messages) >= 3 else messages

        for message in latest_messages:
            if message.get("role") == "user":
                content = message.get("content", "").lower()

                # æå–ä¸ªäººä¿¡æ¯
                if "æˆ‘å«" in content or "æˆ‘æ˜¯" in content:
                    # ç®€åŒ–çš„å§“åæå–
                    words = content.split()
                    for i, word in enumerate(words):
                        if word in ["æˆ‘å«", "æˆ‘æ˜¯"] and i + 1 < len(words):
                            user_profile["name"] = words[i + 1]

                # æå–åå¥½ä¿¡æ¯
                if "å–œæ¬¢" in content:
                    preferences = user_profile.get("preferences", [])
                    preference_text = content.split("å–œæ¬¢")[1].split()[0]
                    if preference_text not in preferences:
                        preferences.append(preference_text)
                    user_profile["preferences"] = preferences

                # æå–åœ°ç†ä¿¡æ¯
                if any(city in content for city in ["åŒ—äº¬", "ä¸Šæµ·", "å¹¿å·", "æ·±åœ³"]):
                    for city in ["åŒ—äº¬", "ä¸Šæµ·", "å¹¿å·", "æ·±åœ³"]:
                        if city in content:
                            user_profile["location"] = city
                            break

        state["user_profile"] = user_profile

        if user_profile:
            print(f"   ğŸ“Š æ›´æ–°ç”¨æˆ·ç”»åƒ: {json.dumps(user_profile, ensure_ascii=False)}")

        return state

    def contextual_responder(state: MemoryState) -> MemoryState:
        """ä¸Šä¸‹æ–‡å“åº”å™¨"""
        messages = state.get("messages", [])
        user_profile = state.get("user_profile", {})
        conversation_summary = state.get("conversation_summary", "")

        if not messages or messages[-1].get("role") != "user":
            return state

        user_message = messages[-1].get("content", "")
        print(f"ğŸ’¬ å“åº”ç”¨æˆ·æ¶ˆæ¯: {user_message}")

        # æ„å»ºä¸ªæ€§åŒ–å“åº”
        response = generate_personalized_response(
            user_message, user_profile, conversation_summary
        )

        # æ·»åŠ AIå“åº”åˆ°æ¶ˆæ¯å†å²
        ai_message = {
            "role": "assistant",
            "content": response,
            "timestamp": time.time(),
            "personalization_used": bool(user_profile)
        }

        return {"messages": [ai_message]}

    # æ„å»ºè®°å¿†å›¾
    graph = StateGraph(MemoryState)
    graph.add_node("memory_manager", memory_manager)
    graph.add_node("profile_extractor", profile_extractor)
    graph.add_node("contextual_responder", contextual_responder)

    graph.set_entry_point("memory_manager")
    graph.add_edge("memory_manager", "profile_extractor")
    graph.add_edge("profile_extractor", "contextual_responder")
    graph.add_edge("contextual_responder", END)

    # é…ç½®è®°å¿†ä¿å­˜å™¨
    memory_saver = MemorySaver()
    app = graph.compile(checkpointer=memory_saver)

    return app


def generate_conversation_summary(messages: List[dict]) -> str:
    """ç”Ÿæˆå¯¹è¯æ‘˜è¦"""
    if not messages:
        return ""

    topics = []
    user_interests = []

    for message in messages:
        content = message.get("content", "").lower()

        # æå–ä¸»é¢˜
        if any(word in content for word in ["å…³äº", "è®¨è®º", "è¯´è¯´"]):
            topics.append("è¯é¢˜è®¨è®º")

        if any(word in content for word in ["é—®é¢˜", "æ€ä¹ˆ", "ä¸ºä»€ä¹ˆ"]):
            topics.append("é—®é¢˜å’¨è¯¢")

        if any(word in content for word in ["å–œæ¬¢", "çˆ±å¥½", "å…´è¶£"]):
            user_interests.append("ä¸ªäººå…´è¶£")

    # ç”Ÿæˆæ‘˜è¦
    summary_parts = []
    if topics:
        summary_parts.append(f"è®¨è®ºäº†{len(set(topics))}ä¸ªä¸»é¢˜")
    if user_interests:
        summary_parts.append("åˆ†äº«äº†ä¸ªäººå…´è¶£")

    return "ï¼Œ".join(summary_parts) if summary_parts else "è¿›è¡Œäº†ä¸€èˆ¬æ€§å¯¹è¯"


def generate_personalized_response(user_message: str, user_profile: dict, summary: str) -> str:
    """ç”Ÿæˆä¸ªæ€§åŒ–å“åº”"""
    response_parts = []

    # ä½¿ç”¨ç”¨æˆ·å§“å
    name = user_profile.get("name")
    if name:
        response_parts.append(f"{name}")

    # åŸºäºä½ç½®ä¿¡æ¯
    location = user_profile.get("location")
    if location and "å¤©æ°”" in user_message:
        response_parts.append(f"åœ¨{location}çš„å¤©æ°”ç¡®å®éœ€è¦å…³æ³¨")

    # åŸºäºåå¥½ä¿¡æ¯
    preferences = user_profile.get("preferences", [])
    if preferences and any(pref in user_message for pref in preferences):
        response_parts.append("çœ‹æ¥è¿™å’Œæ‚¨çš„å…´è¶£çˆ±å¥½ç›¸å…³")

    # åŸºäºå¯¹è¯å†å²
    if summary and "ä¹‹å‰" in user_message:
        response_parts.append(f"å›é¡¾æˆ‘ä»¬çš„å¯¹è¯ï¼Œ{summary}")

    # æ„å»ºæœ€ç»ˆå“åº”
    if response_parts:
        personalized_prefix = "ï¼Œ".join(response_parts) + "ã€‚"
    else:
        personalized_prefix = ""

    # åŸºæœ¬å“åº”
    basic_response = f"æˆ‘ç†è§£æ‚¨è¯´çš„'{user_message}'ã€‚"

    return personalized_prefix + basic_response


def create_long_term_memory_demo():
    """åˆ›å»ºé•¿æœŸè®°å¿†æ¼”ç¤º"""

    class LongTermState(TypedDict):
        user_id: str
        session_memories: List[dict]
        learned_patterns: dict
        user_expertise: dict

    def pattern_learner(state: LongTermState) -> LongTermState:
        """æ¨¡å¼å­¦ä¹ å™¨"""
        user_id = state.get("user_id", "")
        session_memories = state.get("session_memories", [])

        print(f"ğŸ” ä¸ºç”¨æˆ· {user_id} å­¦ä¹ äº¤äº’æ¨¡å¼...")

        # åˆ†æç”¨æˆ·è¡Œä¸ºæ¨¡å¼
        patterns = {}

        # åˆ†æé—®é¢˜ç±»å‹åå¥½
        question_types = []
        for memory in session_memories:
            if "type" in memory:
                question_types.append(memory["type"])

        if question_types:
            from collections import Counter
            type_counts = Counter(question_types)
            patterns["preferred_question_types"] = dict(type_counts.most_common(3))

        # åˆ†ææ´»è·ƒæ—¶é—´
        timestamps = [m.get("timestamp", 0) for m in session_memories if "timestamp" in m]
        if timestamps:
            hours = [time.localtime(ts).tm_hour for ts in timestamps]
            from collections import Counter
            hour_counts = Counter(hours)
            patterns["active_hours"] = dict(hour_counts.most_common(3))

        learned_patterns = state.get("learned_patterns", {})
        learned_patterns.update(patterns)
        state["learned_patterns"] = learned_patterns

        print(f"   ğŸ“ˆ å­¦ä¹ åˆ°çš„æ¨¡å¼: {json.dumps(patterns, ensure_ascii=False)}")

        return state

    def expertise_tracker(state: LongTermState) -> LongTermState:
        """ä¸“ä¸šåº¦è·Ÿè¸ªå™¨"""
        session_memories = state.get("session_memories", [])
        user_expertise = state.get("user_expertise", {})

        # åŸºäºç”¨æˆ·é—®é¢˜å’Œå›å¤è´¨é‡è¯„ä¼°ä¸“ä¸šåº¦
        for memory in session_memories:
            topic = memory.get("topic", "general")
            quality = memory.get("quality_score", 0.5)

            if topic not in user_expertise:
                user_expertise[topic] = {"score": 0.5, "interactions": 0}

            # æ›´æ–°ä¸“ä¸šåº¦åˆ†æ•°ï¼ˆç®€åŒ–ç®—æ³•ï¼‰
            current_score = user_expertise[topic]["score"]
            interactions = user_expertise[topic]["interactions"]

            new_score = (current_score * interactions + quality) / (interactions + 1)
            user_expertise[topic] = {
                "score": new_score,
                "interactions": interactions + 1
            }

        state["user_expertise"] = user_expertise

        print(f"ğŸ“ ç”¨æˆ·ä¸“ä¸šåº¦æ›´æ–°: {json.dumps(user_expertise, ensure_ascii=False)}")

        return state

    # æ„å»ºé•¿æœŸè®°å¿†å›¾
    graph = StateGraph(LongTermState)
    graph.add_node("pattern_learner", pattern_learner)
    graph.add_node("expertise_tracker", expertise_tracker)

    graph.set_entry_point("pattern_learner")
    graph.add_edge("pattern_learner", "expertise_tracker")
    graph.add_edge("expertise_tracker", END)

    return graph.compile()


def demo_memory_system():
    """æ¼”ç¤ºè®°å¿†ç³»ç»Ÿ"""
    print("ğŸš€ Memoryæœºåˆ¶æ¼”ç¤ºå¼€å§‹")
    print("=" * 60)

    # æ¼”ç¤º1: æ™ºèƒ½è®°å¿†ç³»ç»Ÿ
    print("\nğŸ¯ æ¼”ç¤º1: æ™ºèƒ½å¯¹è¯è®°å¿†")
    print("-" * 40)

    memory_app = create_smart_memory_system()

    # æ¨¡æ‹Ÿå¤šè½®å¯¹è¯
    conversation_scenarios = [
        "ä½ å¥½ï¼Œæˆ‘å«å¼ ä¸‰",
        "æˆ‘æ¥è‡ªåŒ—äº¬ï¼Œå–œæ¬¢ç¼–ç¨‹",
        "ä½ èƒ½å¸®æˆ‘è§£å†³ä¸€ä¸ªPythoné—®é¢˜å—ï¼Ÿ",
        "æˆ‘ä¹‹å‰è¯´è¿‡æˆ‘æ¥è‡ªå“ªé‡Œï¼Ÿ",
        "æˆ‘çš„å…´è¶£çˆ±å¥½æ˜¯ä»€ä¹ˆï¼Ÿ",
        "ä»Šå¤©åŒ—äº¬å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
        "è°¢è°¢ä½ è®°ä½äº†æˆ‘çš„ä¿¡æ¯",
        "è¿˜æœ‰å…¶ä»–é—®é¢˜å—ï¼Ÿ",
        "æˆ‘æƒ³äº†è§£æ›´å¤šå…³äºAIçš„çŸ¥è¯†",
        "å†è§"
    ]

    config = {"configurable": {"thread_id": "user_conversation_001"}}

    for i, user_input in enumerate(conversation_scenarios, 1):
        print(f"\nğŸ‘¤ ç”¨æˆ· ({i}/10): {user_input}")

        state = {
            "messages": [{"role": "user", "content": user_input, "timestamp": time.time()}],
            "user_profile": {},
            "conversation_summary": "",
            "interaction_count": 0,
            "memory_metadata": {}
        }

        result = memory_app.invoke(state, config=config)

        # æ˜¾ç¤ºAIå›å¤
        if result["messages"]:
            ai_response = result["messages"][-1]["content"]
            print(f"ğŸ¤– AI: {ai_response}")

        # æ˜¾ç¤ºè®°å¿†ä¿¡æ¯
        if result.get("user_profile"):
            print(f"ğŸ’¾ ç”¨æˆ·ç”»åƒ: {json.dumps(result['user_profile'], ensure_ascii=False)}")

        time.sleep(1)  # æ¨¡æ‹Ÿå¯¹è¯é—´éš”

    # æ¼”ç¤º2: é•¿æœŸè®°å¿†å­¦ä¹ 
    print("\n\nğŸ¯ æ¼”ç¤º2: é•¿æœŸè®°å¿†å­¦ä¹ ")
    print("-" * 40)

    long_term_app = create_long_term_memory_demo()

    # æ¨¡æ‹Ÿå†å²ä¼šè¯è®°å½•
    historical_sessions = [
        {"type": "technical", "topic": "python", "quality_score": 0.8, "timestamp": time.time() - 86400},
        {"type": "casual", "topic": "general", "quality_score": 0.6, "timestamp": time.time() - 43200},
        {"type": "technical", "topic": "javascript", "quality_score": 0.9, "timestamp": time.time() - 3600},
        {"type": "help", "topic": "python", "quality_score": 0.7, "timestamp": time.time() - 1800},
    ]

    long_term_state = {
        "user_id": "zhang_san",
        "session_memories": historical_sessions,
        "learned_patterns": {},
        "user_expertise": {}
    }

    result = long_term_app.invoke(long_term_state)

    print("\nğŸ‰ Memoryæœºåˆ¶æ¼”ç¤ºå®Œæˆï¼")
    print("\nğŸ“Š æœ€ç»ˆå­¦ä¹ ç»“æœ:")
    print(f"   ğŸ§  å­¦ä¹ æ¨¡å¼: {json.dumps(result.get('learned_patterns', {}), ensure_ascii=False)}")
    print(f"   ğŸ“ ä¸“ä¸šåº¦è¯„ä¼°: {json.dumps(result.get('user_expertise', {}), ensure_ascii=False)}")


if __name__ == "__main__":
    demo_memory_system()