# LangGraph æœ€ä½³å®è·µ

> ğŸ¯ **å­¦ä¹ ç›®æ ‡**ï¼šæŒæ¡ LangGraph å¼€å‘çš„è®¾è®¡æ¨¡å¼ã€ä»£ç è§„èŒƒå’Œæ¶æ„åŸåˆ™ï¼Œæ„å»ºå¯ç»´æŠ¤ã€å¯æ‰©å±•çš„åº”ç”¨

## ğŸ—ï¸ æ¶æ„è®¾è®¡åŸåˆ™

### 1. å•ä¸€èŒè´£åŸåˆ™ (SRP)

æ¯ä¸ªèŠ‚ç‚¹åº”è¯¥åªè´Ÿè´£ä¸€ä¸ªæ˜ç¡®çš„åŠŸèƒ½ã€‚

```python
from typing import TypedDict, List, Dict, Any
from langgraph import StateGraph, START, END

# âŒ é”™è¯¯ï¼šèŠ‚ç‚¹èŒè´£æ··ä¹±
def bad_multi_purpose_node(state: dict) -> dict:
    """åä¾‹ï¼šä¸€ä¸ªèŠ‚ç‚¹åšå¤ªå¤šäº‹æƒ…"""
    # éªŒè¯è¾“å…¥
    if not state.get("input"):
        raise ValueError("è¾“å…¥ä¸ºç©º")

    # è°ƒç”¨API
    import requests
    response = requests.get("https://api.example.com/data")

    # å¤„ç†æ•°æ®
    processed_data = response.json()["data"].upper()

    # å‘é€é‚®ä»¶
    send_email(processed_data)

    # ä¿å­˜åˆ°æ•°æ®åº“
    save_to_database(processed_data)

    return {"result": processed_data}

# âœ… æ­£ç¡®ï¼šæ‹†åˆ†ä¸ºå¤šä¸ªä¸“èŒèŠ‚ç‚¹
class ProcessingState(TypedDict):
    input_data: str
    validated_data: str
    api_response: Dict[str, Any]
    processed_data: str
    email_sent: bool
    db_saved: bool
    errors: List[str]

def validation_node(state: ProcessingState) -> ProcessingState:
    """ä¸“èŒï¼šè¾“å…¥éªŒè¯"""
    input_data = state.get("input_data", "")
    errors = state.get("errors", [])

    if not input_data.strip():
        errors.append("è¾“å…¥æ•°æ®ä¸ºç©º")
        return {**state, "errors": errors}

    return {
        **state,
        "validated_data": input_data.strip(),
        "errors": errors
    }

def api_call_node(state: ProcessingState) -> ProcessingState:
    """ä¸“èŒï¼šAPIè°ƒç”¨"""
    try:
        import requests
        response = requests.get(
            "https://api.example.com/data",
            params={"input": state["validated_data"]},
            timeout=5
        )
        response.raise_for_status()

        return {
            **state,
            "api_response": response.json()
        }
    except Exception as e:
        errors = state.get("errors", [])
        errors.append(f"APIè°ƒç”¨å¤±è´¥: {str(e)}")
        return {**state, "errors": errors}

def data_processing_node(state: ProcessingState) -> ProcessingState:
    """ä¸“èŒï¼šæ•°æ®å¤„ç†"""
    if state.get("errors"):
        return state  # æœ‰é”™è¯¯æ—¶è·³è¿‡å¤„ç†

    api_data = state.get("api_response", {}).get("data", "")
    processed_data = api_data.upper() if api_data else ""

    return {
        **state,
        "processed_data": processed_data
    }

def notification_node(state: ProcessingState) -> ProcessingState:
    """ä¸“èŒï¼šé€šçŸ¥å‘é€"""
    if state.get("errors") or not state.get("processed_data"):
        return state

    try:
        # æ¨¡æ‹Ÿå‘é€é‚®ä»¶
        send_email_notification(state["processed_data"])
        return {**state, "email_sent": True}
    except Exception as e:
        errors = state.get("errors", [])
        errors.append(f"é‚®ä»¶å‘é€å¤±è´¥: {str(e)}")
        return {**state, "errors": errors, "email_sent": False}

def persistence_node(state: ProcessingState) -> ProcessingState:
    """ä¸“èŒï¼šæ•°æ®æŒä¹…åŒ–"""
    if state.get("errors") or not state.get("processed_data"):
        return state

    try:
        # æ¨¡æ‹Ÿä¿å­˜åˆ°æ•°æ®åº“
        save_to_database(state["processed_data"])
        return {**state, "db_saved": True}
    except Exception as e:
        errors = state.get("errors", [])
        errors.append(f"æ•°æ®åº“ä¿å­˜å¤±è´¥: {str(e)}")
        return {**state, "errors": errors, "db_saved": False}

def create_well_architected_graph():
    """åˆ›å»ºæ¶æ„è‰¯å¥½çš„å›¾"""
    graph = StateGraph(ProcessingState)

    # æ·»åŠ ä¸“èŒèŠ‚ç‚¹
    graph.add_node("validate", validation_node)
    graph.add_node("api_call", api_call_node)
    graph.add_node("process", data_processing_node)
    graph.add_node("notify", notification_node)
    graph.add_node("persist", persistence_node)

    # å®šä¹‰æ‰§è¡Œæµç¨‹
    graph.add_edge(START, "validate")
    graph.add_edge("validate", "api_call")
    graph.add_edge("api_call", "process")

    # å¹¶è¡Œæ‰§è¡Œé€šçŸ¥å’ŒæŒä¹…åŒ–
    graph.add_edge("process", "notify")
    graph.add_edge("process", "persist")

    graph.add_edge("notify", END)
    graph.add_edge("persist", END)

    return graph.compile()

def send_email_notification(data: str):
    """æ¨¡æ‹Ÿé‚®ä»¶å‘é€"""
    print(f"ğŸ“§ å‘é€é‚®ä»¶: {data}")

def save_to_database(data: str):
    """æ¨¡æ‹Ÿæ•°æ®åº“ä¿å­˜"""
    print(f"ğŸ’¾ ä¿å­˜åˆ°æ•°æ®åº“: {data}")
```

### 2. å¼€é—­åŸåˆ™ (OCP)

ä»£ç åº”è¯¥å¯¹æ‰©å±•å¼€æ”¾ï¼Œå¯¹ä¿®æ”¹å…³é—­ã€‚

```python
from abc import ABC, abstractmethod
from typing import Protocol

# âœ… ä½¿ç”¨ç­–ç•¥æ¨¡å¼å®ç°å¯æ‰©å±•çš„å¤„ç†å™¨
class DataProcessor(Protocol):
    """æ•°æ®å¤„ç†å™¨åè®®"""

    def process(self, data: str) -> str:
        """å¤„ç†æ•°æ®"""
        ...

class UpperCaseProcessor:
    """å¤§å†™å¤„ç†å™¨"""

    def process(self, data: str) -> str:
        return data.upper()

class LowerCaseProcessor:
    """å°å†™å¤„ç†å™¨"""

    def process(self, data: str) -> str:
        return data.lower()

class TitleCaseProcessor:
    """æ ‡é¢˜æ ¼å¼å¤„ç†å™¨"""

    def process(self, data: str) -> str:
        return data.title()

class ReverseProcessor:
    """åè½¬å¤„ç†å™¨"""

    def process(self, data: str) -> str:
        return data[::-1]

# å¤„ç†å™¨æ³¨å†Œè¡¨
PROCESSORS = {
    "upper": UpperCaseProcessor(),
    "lower": LowerCaseProcessor(),
    "title": TitleCaseProcessor(),
    "reverse": ReverseProcessor()
}

def extensible_processing_node(state: dict) -> dict:
    """å¯æ‰©å±•çš„å¤„ç†èŠ‚ç‚¹"""
    processor_type = state.get("processor_type", "upper")
    input_data = state.get("input_data", "")

    # è·å–å¤„ç†å™¨
    processor = PROCESSORS.get(processor_type)
    if not processor:
        return {
            **state,
            "error": f"æœªçŸ¥çš„å¤„ç†å™¨ç±»å‹: {processor_type}",
            "processed_data": ""
        }

    # æ‰§è¡Œå¤„ç†
    try:
        processed_data = processor.process(input_data)
        return {
            **state,
            "processed_data": processed_data,
            "processor_used": processor_type
        }
    except Exception as e:
        return {
            **state,
            "error": f"å¤„ç†å¤±è´¥: {str(e)}",
            "processed_data": ""
        }

# æ³¨å†Œæ–°çš„å¤„ç†å™¨ï¼ˆæ‰©å±•ï¼‰
class JsonFormatter:
    """JSONæ ¼å¼åŒ–å¤„ç†å™¨"""

    def process(self, data: str) -> str:
        import json
        try:
            # å°è¯•è§£æå¹¶ç¾åŒ–JSON
            parsed = json.loads(data)
            return json.dumps(parsed, indent=2, ensure_ascii=False)
        except json.JSONDecodeError:
            # å¦‚æœä¸æ˜¯JSONï¼ŒåŒ…è£…æˆJSON
            return json.dumps({"text": data}, indent=2, ensure_ascii=False)

# åŠ¨æ€æ³¨å†Œæ–°å¤„ç†å™¨
PROCESSORS["json"] = JsonFormatter()

# æµ‹è¯•å¯æ‰©å±•æ€§
def test_extensible_processing():
    """æµ‹è¯•å¯æ‰©å±•å¤„ç†"""
    test_cases = [
        {"processor_type": "upper", "input_data": "hello world"},
        {"processor_type": "lower", "input_data": "HELLO WORLD"},
        {"processor_type": "title", "input_data": "hello world"},
        {"processor_type": "reverse", "input_data": "hello"},
        {"processor_type": "json", "input_data": '{"name": "test"}'},
        {"processor_type": "unknown", "input_data": "test"}
    ]

    for case in test_cases:
        result = extensible_processing_node(case)
        print(f"è¾“å…¥: {case}, è¾“å‡º: {result['processed_data'][:50]}")
```

### 3. ä¾èµ–å€’ç½®åŸåˆ™ (DIP)

é«˜å±‚æ¨¡å—ä¸åº”è¯¥ä¾èµ–ä½å±‚æ¨¡å—ï¼Œä¸¤è€…éƒ½åº”è¯¥ä¾èµ–æŠ½è±¡ã€‚

```python
from abc import ABC, abstractmethod
from typing import Optional

# âœ… å®šä¹‰æŠ½è±¡æ¥å£
class DatabaseInterface(ABC):
    """æ•°æ®åº“æ¥å£æŠ½è±¡"""

    @abstractmethod
    def save(self, key: str, data: str) -> bool:
        pass

    @abstractmethod
    def load(self, key: str) -> Optional[str]:
        pass

class CacheInterface(ABC):
    """ç¼“å­˜æ¥å£æŠ½è±¡"""

    @abstractmethod
    def get(self, key: str) -> Optional[str]:
        pass

    @abstractmethod
    def set(self, key: str, value: str, ttl: int = 3600) -> bool:
        pass

class NotificationInterface(ABC):
    """é€šçŸ¥æ¥å£æŠ½è±¡"""

    @abstractmethod
    def send(self, message: str, recipient: str) -> bool:
        pass

# å…·ä½“å®ç°
class PostgreSQLDatabase(DatabaseInterface):
    """PostgreSQLæ•°æ®åº“å®ç°"""

    def save(self, key: str, data: str) -> bool:
        print(f"ä¿å­˜åˆ°PostgreSQL: {key} = {data}")
        return True

    def load(self, key: str) -> Optional[str]:
        print(f"ä»PostgreSQLåŠ è½½: {key}")
        return f"data_from_postgres_{key}"

class RedisCache(CacheInterface):
    """Redisç¼“å­˜å®ç°"""

    def get(self, key: str) -> Optional[str]:
        print(f"ä»Redisè·å–: {key}")
        return f"cached_{key}"

    def set(self, key: str, value: str, ttl: int = 3600) -> bool:
        print(f"å­˜å‚¨åˆ°Redis: {key} = {value} (TTL: {ttl}s)")
        return True

class EmailNotification(NotificationInterface):
    """é‚®ä»¶é€šçŸ¥å®ç°"""

    def send(self, message: str, recipient: str) -> bool:
        print(f"å‘é€é‚®ä»¶åˆ° {recipient}: {message}")
        return True

# é«˜å±‚ä¸šåŠ¡é€»è¾‘ä¾èµ–æŠ½è±¡
class DataService:
    """æ•°æ®æœåŠ¡ - ä¾èµ–æŠ½è±¡æ¥å£"""

    def __init__(self,
                 database: DatabaseInterface,
                 cache: CacheInterface,
                 notification: NotificationInterface):
        self.database = database
        self.cache = cache
        self.notification = notification

    def process_data(self, key: str, data: str, notify_recipient: str = None) -> dict:
        """å¤„ç†æ•°æ®çš„é«˜å±‚é€»è¾‘"""
        try:
            # 1. æ£€æŸ¥ç¼“å­˜
            cached_data = self.cache.get(key)
            if cached_data:
                return {"result": cached_data, "source": "cache"}

            # 2. å¤„ç†æ•°æ®
            processed_data = f"processed_{data}"

            # 3. ä¿å­˜åˆ°æ•°æ®åº“
            if self.database.save(key, processed_data):
                # 4. æ›´æ–°ç¼“å­˜
                self.cache.set(key, processed_data)

                # 5. å‘é€é€šçŸ¥
                if notify_recipient:
                    self.notification.send(
                        f"æ•°æ®å¤„ç†å®Œæˆ: {key}",
                        notify_recipient
                    )

                return {"result": processed_data, "source": "processed"}
            else:
                return {"error": "æ•°æ®ä¿å­˜å¤±è´¥"}

        except Exception as e:
            return {"error": f"å¤„ç†å¤±è´¥: {str(e)}"}

# ä¾èµ–æ³¨å…¥èŠ‚ç‚¹
def dependency_injection_node(state: dict) -> dict:
    """ä½¿ç”¨ä¾èµ–æ³¨å…¥çš„èŠ‚ç‚¹"""
    # ä»çŠ¶æ€ä¸­è·å–ä¾èµ–ï¼ˆåœ¨å›¾åˆ›å»ºæ—¶æ³¨å…¥ï¼‰
    data_service = state.get("data_service")
    if not data_service:
        return {**state, "error": "æ•°æ®æœåŠ¡æœªæ³¨å…¥"}

    key = state.get("key", "default")
    data = state.get("input_data", "")
    recipient = state.get("notification_recipient")

    result = data_service.process_data(key, data, recipient)

    return {
        **state,
        "service_result": result
    }

def create_dependency_injection_graph():
    """åˆ›å»ºä½¿ç”¨ä¾èµ–æ³¨å…¥çš„å›¾"""
    # åˆ›å»ºå…·ä½“å®ç°
    database = PostgreSQLDatabase()
    cache = RedisCache()
    notification = EmailNotification()

    # æ³¨å…¥ä¾èµ–
    data_service = DataService(database, cache, notification)

    graph = StateGraph(dict)
    graph.add_node("process", dependency_injection_node)
    graph.add_edge(START, "process")
    graph.add_edge("process", END)

    app = graph.compile()

    # è¿”å›é…ç½®äº†ä¾èµ–çš„åº”ç”¨
    def configured_invoke(state: dict):
        # åœ¨è°ƒç”¨æ—¶æ³¨å…¥æœåŠ¡
        state_with_deps = {
            **state,
            "data_service": data_service
        }
        return app.invoke(state_with_deps)

    return configured_invoke
```

## ğŸ¨ è®¾è®¡æ¨¡å¼

### 1. ç­–ç•¥æ¨¡å¼

æ ¹æ®ä¸åŒæ¡ä»¶é€‰æ‹©ä¸åŒçš„å¤„ç†ç­–ç•¥ã€‚

```python
from enum import Enum
from typing import Dict, Callable

class ProcessingStrategy(Enum):
    """å¤„ç†ç­–ç•¥æšä¸¾"""
    SIMPLE = "simple"
    ADVANCED = "advanced"
    BATCH = "batch"
    STREAMING = "streaming"

class StrategyManager:
    """ç­–ç•¥ç®¡ç†å™¨"""

    def __init__(self):
        self.strategies: Dict[ProcessingStrategy, Callable] = {}

    def register_strategy(self, strategy: ProcessingStrategy, handler: Callable):
        """æ³¨å†Œç­–ç•¥"""
        self.strategies[strategy] = handler

    def execute_strategy(self, strategy: ProcessingStrategy, state: dict) -> dict:
        """æ‰§è¡Œç­–ç•¥"""
        handler = self.strategies.get(strategy)
        if not handler:
            return {**state, "error": f"æœªæ‰¾åˆ°ç­–ç•¥: {strategy.value}"}

        return handler(state)

# ç­–ç•¥å®ç°
def simple_processing_strategy(state: dict) -> dict:
    """ç®€å•å¤„ç†ç­–ç•¥"""
    data = state.get("input_data", "")
    return {
        **state,
        "processed_data": f"ç®€å•å¤„ç†: {data}",
        "strategy_used": "simple"
    }

def advanced_processing_strategy(state: dict) -> dict:
    """é«˜çº§å¤„ç†ç­–ç•¥"""
    data = state.get("input_data", "")
    # æ¨¡æ‹Ÿå¤æ‚å¤„ç†
    processed = data.upper().replace(" ", "_")
    return {
        **state,
        "processed_data": f"é«˜çº§å¤„ç†: {processed}",
        "strategy_used": "advanced",
        "metadata": {"complexity": "high", "transformations": ["upper", "underscore"]}
    }

def batch_processing_strategy(state: dict) -> dict:
    """æ‰¹å¤„ç†ç­–ç•¥"""
    data = state.get("input_data", "")
    items = data.split(",") if "," in data else [data]

    processed_items = [f"æ‰¹å¤„ç†_{item.strip()}" for item in items]

    return {
        **state,
        "processed_data": ",".join(processed_items),
        "strategy_used": "batch",
        "batch_size": len(items)
    }

def streaming_processing_strategy(state: dict) -> dict:
    """æµå¤„ç†ç­–ç•¥"""
    data = state.get("input_data", "")
    # æ¨¡æ‹Ÿæµå¤„ç†
    chunks = [data[i:i+10] for i in range(0, len(data), 10)]

    return {
        **state,
        "processed_data": f"æµå¤„ç†: {len(chunks)}ä¸ªå—",
        "strategy_used": "streaming",
        "chunks": chunks
    }

# åˆ›å»ºç­–ç•¥ç®¡ç†å™¨
strategy_manager = StrategyManager()
strategy_manager.register_strategy(ProcessingStrategy.SIMPLE, simple_processing_strategy)
strategy_manager.register_strategy(ProcessingStrategy.ADVANCED, advanced_processing_strategy)
strategy_manager.register_strategy(ProcessingStrategy.BATCH, batch_processing_strategy)
strategy_manager.register_strategy(ProcessingStrategy.STREAMING, streaming_processing_strategy)

def strategy_selection_node(state: dict) -> dict:
    """ç­–ç•¥é€‰æ‹©èŠ‚ç‚¹"""
    # æ ¹æ®è¾“å…¥ç‰¹å¾é€‰æ‹©ç­–ç•¥
    data = state.get("input_data", "")
    data_length = len(data)

    if "," in data:
        strategy = ProcessingStrategy.BATCH
    elif data_length > 100:
        strategy = ProcessingStrategy.STREAMING
    elif data_length > 20:
        strategy = ProcessingStrategy.ADVANCED
    else:
        strategy = ProcessingStrategy.SIMPLE

    print(f"é€‰æ‹©ç­–ç•¥: {strategy.value} (æ•°æ®é•¿åº¦: {data_length})")

    return strategy_manager.execute_strategy(strategy, state)
```

### 2. è§‚å¯Ÿè€…æ¨¡å¼

å®ç°äº‹ä»¶é©±åŠ¨çš„æ¶æ„ã€‚

```python
from typing import List, Callable, Any
from dataclasses import dataclass
from datetime import datetime

@dataclass
class GraphEvent:
    """å›¾äº‹ä»¶"""
    event_type: str
    node_name: str
    timestamp: datetime
    data: Any

class EventObserver(ABC):
    """äº‹ä»¶è§‚å¯Ÿè€…æŠ½è±¡åŸºç±»"""

    @abstractmethod
    def on_event(self, event: GraphEvent):
        """å¤„ç†äº‹ä»¶"""
        pass

class LoggingObserver(EventObserver):
    """æ—¥å¿—è§‚å¯Ÿè€…"""

    def on_event(self, event: GraphEvent):
        print(f"ğŸ“ [{event.timestamp.strftime('%H:%M:%S')}] "
              f"{event.event_type}: {event.node_name} - {event.data}")

class MetricsObserver(EventObserver):
    """æŒ‡æ ‡è§‚å¯Ÿè€…"""

    def __init__(self):
        self.metrics = {
            "node_executions": {},
            "total_events": 0,
            "errors": 0
        }

    def on_event(self, event: GraphEvent):
        self.metrics["total_events"] += 1

        if event.event_type == "node_start":
            node_name = event.node_name
            if node_name not in self.metrics["node_executions"]:
                self.metrics["node_executions"][node_name] = 0
            self.metrics["node_executions"][node_name] += 1

        elif event.event_type == "node_error":
            self.metrics["errors"] += 1

    def get_metrics(self) -> dict:
        return self.metrics.copy()

class AlertObserver(EventObserver):
    """å‘Šè­¦è§‚å¯Ÿè€…"""

    def __init__(self, error_threshold: int = 3):
        self.error_threshold = error_threshold
        self.error_count = 0

    def on_event(self, event: GraphEvent):
        if event.event_type == "node_error":
            self.error_count += 1
            if self.error_count >= self.error_threshold:
                self.send_alert(event)

    def send_alert(self, event: GraphEvent):
        print(f"ğŸš¨ å‘Šè­¦: é”™è¯¯æ¬¡æ•°è¾¾åˆ°é˜ˆå€¼ ({self.error_count})! "
              f"æœ€æ–°é”™è¯¯: {event.node_name} - {event.data}")

class EventPublisher:
    """äº‹ä»¶å‘å¸ƒå™¨"""

    def __init__(self):
        self.observers: List[EventObserver] = []

    def add_observer(self, observer: EventObserver):
        """æ·»åŠ è§‚å¯Ÿè€…"""
        self.observers.append(observer)

    def remove_observer(self, observer: EventObserver):
        """ç§»é™¤è§‚å¯Ÿè€…"""
        if observer in self.observers:
            self.observers.remove(observer)

    def notify_observers(self, event: GraphEvent):
        """é€šçŸ¥æ‰€æœ‰è§‚å¯Ÿè€…"""
        for observer in self.observers:
            try:
                observer.on_event(event)
            except Exception as e:
                print(f"âš ï¸ è§‚å¯Ÿè€…å¤„ç†äº‹ä»¶æ—¶å‡ºé”™: {e}")

# å…¨å±€äº‹ä»¶å‘å¸ƒå™¨
event_publisher = EventPublisher()

def observable_node(node_name: str):
    """å¯è§‚å¯ŸèŠ‚ç‚¹è£…é¥°å™¨"""
    def decorator(func: Callable):
        def wrapper(state: dict) -> dict:
            # å‘å¸ƒèŠ‚ç‚¹å¼€å§‹äº‹ä»¶
            event_publisher.notify_observers(GraphEvent(
                event_type="node_start",
                node_name=node_name,
                timestamp=datetime.now(),
                data={"input_keys": list(state.keys())}
            ))

            try:
                result = func(state)

                # å‘å¸ƒèŠ‚ç‚¹å®Œæˆäº‹ä»¶
                event_publisher.notify_observers(GraphEvent(
                    event_type="node_complete",
                    node_name=node_name,
                    timestamp=datetime.now(),
                    data={"output_keys": list(result.keys())}
                ))

                return result

            except Exception as e:
                # å‘å¸ƒèŠ‚ç‚¹é”™è¯¯äº‹ä»¶
                event_publisher.notify_observers(GraphEvent(
                    event_type="node_error",
                    node_name=node_name,
                    timestamp=datetime.now(),
                    data={"error": str(e)}
                ))
                raise

        return wrapper
    return decorator

# ä½¿ç”¨è§‚å¯Ÿè€…æ¨¡å¼
@observable_node("processing")
def observed_processing_node(state: dict) -> dict:
    """è¢«è§‚å¯Ÿçš„å¤„ç†èŠ‚ç‚¹"""
    data = state.get("input_data", "")

    # æ¨¡æ‹Ÿå¯èƒ½çš„é”™è¯¯
    if "error" in data.lower():
        raise ValueError("æ¨¡æ‹Ÿå¤„ç†é”™è¯¯")

    return {
        **state,
        "processed_data": f"è§‚å¯Ÿè€…æ¨¡å¼å¤„ç†: {data}"
    }

def setup_observers():
    """è®¾ç½®è§‚å¯Ÿè€…"""
    # æ·»åŠ å„ç§è§‚å¯Ÿè€…
    event_publisher.add_observer(LoggingObserver())

    metrics_observer = MetricsObserver()
    event_publisher.add_observer(metrics_observer)

    alert_observer = AlertObserver(error_threshold=2)
    event_publisher.add_observer(alert_observer)

    return metrics_observer, alert_observer
```

### 3. å·¥å‚æ¨¡å¼

åŠ¨æ€åˆ›å»ºä¸åŒç±»å‹çš„å›¾å’ŒèŠ‚ç‚¹ã€‚

```python
from typing import Type, Dict, Any
from enum import Enum

class GraphType(Enum):
    """å›¾ç±»å‹æšä¸¾"""
    SIMPLE_PIPELINE = "simple_pipeline"
    CONDITIONAL_FLOW = "conditional_flow"
    PARALLEL_PROCESSING = "parallel_processing"
    LOOP_PROCESSING = "loop_processing"

class NodeFactory:
    """èŠ‚ç‚¹å·¥å‚"""

    @staticmethod
    def create_processing_node(processing_type: str) -> Callable:
        """åˆ›å»ºå¤„ç†èŠ‚ç‚¹"""
        processing_functions = {
            "text_upper": lambda state: {
                **state, "result": state.get("input", "").upper()
            },
            "text_lower": lambda state: {
                **state, "result": state.get("input", "").lower()
            },
            "text_reverse": lambda state: {
                **state, "result": state.get("input", "")[::-1]
            },
            "math_double": lambda state: {
                **state, "result": str(int(state.get("input", "0")) * 2)
            }
        }

        processor = processing_functions.get(processing_type)
        if not processor:
            raise ValueError(f"æœªçŸ¥çš„å¤„ç†ç±»å‹: {processing_type}")

        return processor

    @staticmethod
    def create_condition_node(condition_type: str) -> Callable:
        """åˆ›å»ºæ¡ä»¶èŠ‚ç‚¹"""
        condition_functions = {
            "length_check": lambda state: (
                "long" if len(state.get("input", "")) > 10 else "short"
            ),
            "type_check": lambda state: (
                "numeric" if state.get("input", "").isdigit() else "text"
            ),
            "empty_check": lambda state: (
                "empty" if not state.get("input", "").strip() else "not_empty"
            )
        }

        condition = condition_functions.get(condition_type)
        if not condition:
            raise ValueError(f"æœªçŸ¥çš„æ¡ä»¶ç±»å‹: {condition_type}")

        return condition

class GraphFactory:
    """å›¾å·¥å‚"""

    @classmethod
    def create_graph(cls, graph_type: GraphType, config: Dict[str, Any] = None) -> StateGraph:
        """åˆ›å»ºå›¾"""
        config = config or {}

        if graph_type == GraphType.SIMPLE_PIPELINE:
            return cls._create_simple_pipeline(config)
        elif graph_type == GraphType.CONDITIONAL_FLOW:
            return cls._create_conditional_flow(config)
        elif graph_type == GraphType.PARALLEL_PROCESSING:
            return cls._create_parallel_processing(config)
        elif graph_type == GraphType.LOOP_PROCESSING:
            return cls._create_loop_processing(config)
        else:
            raise ValueError(f"æœªçŸ¥çš„å›¾ç±»å‹: {graph_type}")

    @classmethod
    def _create_simple_pipeline(cls, config: Dict[str, Any]) -> StateGraph:
        """åˆ›å»ºç®€å•ç®¡é“å›¾"""
        processing_steps = config.get("processing_steps", ["text_upper"])

        graph = StateGraph(dict)

        # åˆ›å»ºå¤„ç†èŠ‚ç‚¹
        for i, step in enumerate(processing_steps):
            node_name = f"step_{i+1}"
            processor = NodeFactory.create_processing_node(step)
            graph.add_node(node_name, processor)

        # è¿æ¥èŠ‚ç‚¹
        if processing_steps:
            graph.add_edge(START, "step_1")
            for i in range(len(processing_steps) - 1):
                graph.add_edge(f"step_{i+1}", f"step_{i+2}")
            graph.add_edge(f"step_{len(processing_steps)}", END)

        return graph

    @classmethod
    def _create_conditional_flow(cls, config: Dict[str, Any]) -> StateGraph:
        """åˆ›å»ºæ¡ä»¶æµå›¾"""
        condition_type = config.get("condition_type", "length_check")
        branches = config.get("branches", {
            "long": "text_upper",
            "short": "text_lower"
        })

        graph = StateGraph(dict)

        # åˆ›å»ºæ¡ä»¶å‡½æ•°
        condition_func = NodeFactory.create_condition_node(condition_type)

        # åˆ›å»ºåˆ†æ”¯èŠ‚ç‚¹
        for branch_name, processing_type in branches.items():
            processor = NodeFactory.create_processing_node(processing_type)
            graph.add_node(branch_name, processor)

        # åˆ›å»ºæ¡ä»¶è¾¹
        graph.add_conditional_edges(
            START,
            condition_func,
            {branch: branch for branch in branches.keys()}
        )

        # è¿æ¥åˆ°ç»“æŸ
        for branch in branches.keys():
            graph.add_edge(branch, END)

        return graph

    @classmethod
    def _create_parallel_processing(cls, config: Dict[str, Any]) -> StateGraph:
        """åˆ›å»ºå¹¶è¡Œå¤„ç†å›¾"""
        parallel_processes = config.get("parallel_processes", ["text_upper", "text_lower"])

        graph = StateGraph(dict)

        # åˆ›å»ºå¹¶è¡ŒèŠ‚ç‚¹
        for i, process_type in enumerate(parallel_processes):
            node_name = f"parallel_{i+1}"
            processor = NodeFactory.create_processing_node(process_type)
            graph.add_node(node_name, processor)

        # å¹¶è¡Œå¯åŠ¨
        for i in range(len(parallel_processes)):
            graph.add_edge(START, f"parallel_{i+1}")
            graph.add_edge(f"parallel_{i+1}", END)

        return graph

    @classmethod
    def _create_loop_processing(cls, config: Dict[str, Any]) -> StateGraph:
        """åˆ›å»ºå¾ªç¯å¤„ç†å›¾"""
        max_iterations = config.get("max_iterations", 3)
        processing_type = config.get("processing_type", "text_upper")

        # åˆ›å»ºå¸¦è®¡æ•°çš„å¤„ç†èŠ‚ç‚¹
        def loop_processor(state: dict) -> dict:
            iteration = state.get("iteration", 0) + 1
            processor = NodeFactory.create_processing_node(processing_type)

            result = processor(state)
            result["iteration"] = iteration

            return result

        def should_continue(state: dict) -> str:
            return "continue" if state.get("iteration", 0) < max_iterations else "end"

        graph = StateGraph(dict)
        graph.add_node("loop_processor", loop_processor)

        graph.add_edge(START, "loop_processor")
        graph.add_conditional_edges(
            "loop_processor",
            should_continue,
            {
                "continue": "loop_processor",
                "end": END
            }
        )

        return graph

# ä½¿ç”¨å·¥å‚æ¨¡å¼
def demonstrate_graph_factory():
    """æ¼”ç¤ºå›¾å·¥å‚"""
    # åˆ›å»ºä¸åŒç±»å‹çš„å›¾
    configs = [
        {
            "type": GraphType.SIMPLE_PIPELINE,
            "config": {"processing_steps": ["text_upper", "text_reverse"]}
        },
        {
            "type": GraphType.CONDITIONAL_FLOW,
            "config": {
                "condition_type": "length_check",
                "branches": {"long": "text_upper", "short": "text_lower"}
            }
        },
        {
            "type": GraphType.PARALLEL_PROCESSING,
            "config": {"parallel_processes": ["text_upper", "text_lower", "text_reverse"]}
        },
        {
            "type": GraphType.LOOP_PROCESSING,
            "config": {"max_iterations": 2, "processing_type": "text_upper"}
        }
    ]

    for graph_config in configs:
        print(f"\nğŸ“Š åˆ›å»ºå›¾ç±»å‹: {graph_config['type'].value}")

        graph = GraphFactory.create_graph(
            graph_config['type'],
            graph_config['config']
        )

        app = graph.compile()

        result = app.invoke({"input": "hello world"})
        print(f"ç»“æœ: {result}")
```

## ğŸ“ ä»£ç è§„èŒƒ

### 1. å‘½åè§„èŒƒ

```python
# âœ… å¥½çš„å‘½åè§„èŒƒ
class UserDataProcessingState(TypedDict):
    """ç”¨æˆ·æ•°æ®å¤„ç†çŠ¶æ€

    ä½¿ç”¨æè¿°æ€§çš„ç±»åï¼Œéµå¾ª PascalCase
    """
    user_input: str              # ä½¿ç”¨ snake_case
    processed_output: str
    validation_errors: List[str]
    processing_metadata: Dict[str, Any]

def validate_user_input_node(state: UserDataProcessingState) -> UserDataProcessingState:
    """éªŒè¯ç”¨æˆ·è¾“å…¥èŠ‚ç‚¹

    å‡½æ•°åä½¿ç”¨ snake_caseï¼Œæ¸…æ™°æè¿°åŠŸèƒ½
    """
    # å¸¸é‡ä½¿ç”¨ UPPER_CASE
    MAX_INPUT_LENGTH = 1000
    MIN_INPUT_LENGTH = 1

    user_input = state["user_input"]
    validation_errors = []

    # ä½¿ç”¨æè¿°æ€§çš„å˜é‡å
    input_length = len(user_input)
    is_input_valid = True

    if input_length < MIN_INPUT_LENGTH:
        validation_errors.append("è¾“å…¥é•¿åº¦ä¸èƒ½å°‘äº1ä¸ªå­—ç¬¦")
        is_input_valid = False

    if input_length > MAX_INPUT_LENGTH:
        validation_errors.append(f"è¾“å…¥é•¿åº¦ä¸èƒ½è¶…è¿‡{MAX_INPUT_LENGTH}ä¸ªå­—ç¬¦")
        is_input_valid = False

    return {
        **state,
        "validation_errors": validation_errors,
        "processing_metadata": {
            "input_length": input_length,
            "is_valid": is_input_valid,
            "validation_timestamp": __import__('datetime').datetime.now().isoformat()
        }
    }

# âŒ ä¸å¥½çš„å‘½å
def proc_data(st):  # å‡½æ•°åä¸æ¸…æ™°
    x = st["data"]  # å˜é‡åæ²¡æœ‰æ„ä¹‰
    if len(x) > 100:  # é­”æ•°
        return {"result": x[:100]}
    return {"result": x}
```

### 2. æ–‡æ¡£è§„èŒƒ

```python
from typing import TypedDict, List, Dict, Any, Optional

class DocumentedState(TypedDict):
    """æ–‡æ¡£åŒ–çš„çŠ¶æ€å®šä¹‰

    è¿™ä¸ªçŠ¶æ€ç”¨äºæ¼”ç¤ºå®Œæ•´çš„æ–‡æ¡£åŒ–è§„èŒƒï¼ŒåŒ…å«äº†æ‰€æœ‰å¿…è¦çš„å­—æ®µç±»å‹å’Œè¯´æ˜ã€‚

    Attributes:
        input_text: åŸå§‹è¾“å…¥æ–‡æœ¬ï¼Œä¸èƒ½ä¸ºç©º
        processed_text: å¤„ç†åçš„æ–‡æœ¬ï¼Œåˆå§‹ä¸ºç©ºå­—ç¬¦ä¸²
        processing_steps: è®°å½•å¤„ç†æ­¥éª¤çš„åˆ—è¡¨
        metadata: å­˜å‚¨é¢å¤–å…ƒæ•°æ®çš„å­—å…¸
        error_message: é”™è¯¯ä¿¡æ¯ï¼Œæ— é”™è¯¯æ—¶ä¸º None
    """
    input_text: str
    processed_text: str
    processing_steps: List[str]
    metadata: Dict[str, Any]
    error_message: Optional[str]

def text_preprocessing_node(state: DocumentedState) -> DocumentedState:
    """æ–‡æœ¬é¢„å¤„ç†èŠ‚ç‚¹

    å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œæ ‡å‡†åŒ–é¢„å¤„ç†ï¼ŒåŒ…æ‹¬å»é™¤å¤šä½™ç©ºæ ¼ã€è½¬æ¢ç¼–ç ç­‰ã€‚

    Args:
        state: åŒ…å«è¾“å…¥æ–‡æœ¬çš„çŠ¶æ€å¯¹è±¡

    Returns:
        å¤„ç†åçš„çŠ¶æ€å¯¹è±¡ï¼ŒåŒ…å«é¢„å¤„ç†åçš„æ–‡æœ¬

    Raises:
        ValueError: å½“è¾“å…¥æ–‡æœ¬ä¸ºç©ºæˆ–Noneæ—¶

    Examples:
        >>> initial_state = {
        ...     "input_text": "  Hello World  ",
        ...     "processed_text": "",
        ...     "processing_steps": [],
        ...     "metadata": {},
        ...     "error_message": None
        ... }
        >>> result = text_preprocessing_node(initial_state)
        >>> result["processed_text"]
        'Hello World'
    """
    try:
        # éªŒè¯è¾“å…¥
        if not state["input_text"] or not state["input_text"].strip():
            raise ValueError("è¾“å…¥æ–‡æœ¬ä¸èƒ½ä¸ºç©º")

        # æ‰§è¡Œé¢„å¤„ç†
        cleaned_text = state["input_text"].strip()
        # ç§»é™¤å¤šä½™çš„ç©ºæ ¼
        cleaned_text = " ".join(cleaned_text.split())

        # è®°å½•å¤„ç†æ­¥éª¤
        processing_steps = state["processing_steps"] + ["text_preprocessing"]

        # æ›´æ–°å…ƒæ•°æ®
        metadata = state["metadata"].copy()
        metadata.update({
            "preprocessing_applied": True,
            "original_length": len(state["input_text"]),
            "cleaned_length": len(cleaned_text),
            "characters_removed": len(state["input_text"]) - len(cleaned_text)
        })

        return {
            **state,
            "processed_text": cleaned_text,
            "processing_steps": processing_steps,
            "metadata": metadata,
            "error_message": None
        }

    except Exception as e:
        # é”™è¯¯å¤„ç†
        return {
            **state,
            "error_message": f"é¢„å¤„ç†å¤±è´¥: {str(e)}"
        }

def advanced_text_analysis_node(
    state: DocumentedState,
    enable_sentiment_analysis: bool = True,
    enable_entity_extraction: bool = False,
    confidence_threshold: float = 0.8
) -> DocumentedState:
    """é«˜çº§æ–‡æœ¬åˆ†æèŠ‚ç‚¹

    å¯¹é¢„å¤„ç†åçš„æ–‡æœ¬è¿›è¡Œæ·±åº¦åˆ†æï¼ŒåŒ…æ‹¬æƒ…æ„Ÿåˆ†æã€å®ä½“æå–ç­‰åŠŸèƒ½ã€‚

    Args:
        state: åŒ…å«é¢„å¤„ç†æ–‡æœ¬çš„çŠ¶æ€å¯¹è±¡
        enable_sentiment_analysis: æ˜¯å¦å¯ç”¨æƒ…æ„Ÿåˆ†æï¼Œé»˜è®¤ä¸ºTrue
        enable_entity_extraction: æ˜¯å¦å¯ç”¨å®ä½“æå–ï¼Œé»˜è®¤ä¸ºFalse
        confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œé»˜è®¤ä¸º0.8

    Returns:
        åŒ…å«åˆ†æç»“æœçš„çŠ¶æ€å¯¹è±¡

    Note:
        æ­¤èŠ‚ç‚¹éœ€è¦é¢„å¤„ç†æ–‡æœ¬ä¸ä¸ºç©ºã€‚å¦‚æœå¯ç”¨å®ä½“æå–ï¼Œ
        å°†ä¼šå¢åŠ å¤„ç†æ—¶é—´ä½†æä¾›æ›´ä¸°å¯Œçš„åˆ†æç»“æœã€‚

    Warning:
        å®ä½“æå–åŠŸèƒ½ç›®å‰ä¸ºå®éªŒæ€§åŠŸèƒ½ï¼Œå¯èƒ½å½±å“æ€§èƒ½ã€‚
    """
    if state["error_message"]:
        return state  # å¦‚æœå·²æœ‰é”™è¯¯ï¼Œç›´æ¥è¿”å›

    try:
        text = state["processed_text"]
        if not text:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°é¢„å¤„ç†åçš„æ–‡æœ¬")

        analysis_results = {}
        processing_steps = state["processing_steps"].copy()

        # æƒ…æ„Ÿåˆ†æ
        if enable_sentiment_analysis:
            # æ¨¡æ‹Ÿæƒ…æ„Ÿåˆ†æ
            sentiment_score = 0.7  # ç¤ºä¾‹åˆ†æ•°
            if sentiment_score >= confidence_threshold:
                analysis_results["sentiment"] = {
                    "score": sentiment_score,
                    "label": "positive" if sentiment_score > 0.5 else "negative",
                    "confidence": sentiment_score
                }
                processing_steps.append("sentiment_analysis")

        # å®ä½“æå–
        if enable_entity_extraction:
            # æ¨¡æ‹Ÿå®ä½“æå–
            entities = [{"text": "World", "type": "LOCATION"}]  # ç¤ºä¾‹å®ä½“
            analysis_results["entities"] = entities
            processing_steps.append("entity_extraction")

        # æ›´æ–°å…ƒæ•°æ®
        metadata = state["metadata"].copy()
        metadata.update({
            "analysis_performed": True,
            "analysis_features": {
                "sentiment_analysis": enable_sentiment_analysis,
                "entity_extraction": enable_entity_extraction
            },
            "confidence_threshold": confidence_threshold
        })

        return {
            **state,
            "processed_text": text,  # ä¿æŒæ–‡æœ¬ä¸å˜
            "processing_steps": processing_steps,
            "metadata": {**metadata, "analysis_results": analysis_results},
            "error_message": None
        }

    except Exception as e:
        return {
            **state,
            "error_message": f"æ–‡æœ¬åˆ†æå¤±è´¥: {str(e)}"
        }

# å›¾çš„æ–‡æ¡£åŒ–
def create_documented_text_processing_graph() -> StateGraph:
    """åˆ›å»ºæ–‡æ¡£åŒ–çš„æ–‡æœ¬å¤„ç†å›¾

    æ„å»ºä¸€ä¸ªå®Œæ•´çš„æ–‡æœ¬å¤„ç†ç®¡é“ï¼ŒåŒ…æ‹¬é¢„å¤„ç†å’Œé«˜çº§åˆ†æåŠŸèƒ½ã€‚

    Returns:
        ç¼–è¯‘åçš„çŠ¶æ€å›¾ï¼Œå¯ç›´æ¥è°ƒç”¨invokeæ–¹æ³•

    Graph Structure:
        START -> preprocess -> analyze -> END

    State Flow:
        1. preprocess: æ¸…ç†å’Œæ ‡å‡†åŒ–è¾“å…¥æ–‡æœ¬
        2. analyze: æ‰§è¡Œæƒ…æ„Ÿåˆ†æå’Œå®ä½“æå–

    Example:
        >>> graph = create_documented_text_processing_graph()
        >>> app = graph.compile()
        >>> result = app.invoke({
        ...     "input_text": "I love this product!",
        ...     "processed_text": "",
        ...     "processing_steps": [],
        ...     "metadata": {},
        ...     "error_message": None
        ... })
    """
    graph = StateGraph(DocumentedState)

    # æ·»åŠ èŠ‚ç‚¹
    graph.add_node("preprocess", text_preprocessing_node)
    graph.add_node("analyze", advanced_text_analysis_node)

    # å®šä¹‰è¾¹
    graph.add_edge(START, "preprocess")
    graph.add_edge("preprocess", "analyze")
    graph.add_edge("analyze", END)

    return graph
```

### 3. ç±»å‹æ³¨è§£è§„èŒƒ

```python
from typing import (
    TypedDict, List, Dict, Any, Optional, Union,
    Callable, Tuple, Set, Protocol, TypeVar, Generic
)
from dataclasses import dataclass
from abc import ABC, abstractmethod

# ç±»å‹åˆ«å
NodeFunction = Callable[[Dict[str, Any]], Dict[str, Any]]
ConditionFunction = Callable[[Dict[str, Any]], str]
StateType = TypeVar('StateType', bound=Dict[str, Any])

class ProcessingResult(TypedDict):
    """å¤„ç†ç»“æœç±»å‹å®šä¹‰"""
    success: bool
    data: Any
    error_message: Optional[str]
    metadata: Dict[str, Any]

class ConfigurableNode(Protocol):
    """å¯é…ç½®èŠ‚ç‚¹åè®®"""

    def configure(self, config: Dict[str, Any]) -> None:
        """é…ç½®èŠ‚ç‚¹"""
        ...

    def process(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†çŠ¶æ€"""
        ...

@dataclass
class NodeMetrics:
    """èŠ‚ç‚¹æŒ‡æ ‡æ•°æ®ç±»"""
    execution_count: int = 0
    total_execution_time: float = 0.0
    error_count: int = 0
    last_execution_time: Optional[float] = None

class BaseProcessor(ABC, Generic[StateType]):
    """åŸºç¡€å¤„ç†å™¨æ³›å‹æŠ½è±¡ç±»"""

    @abstractmethod
    def validate_state(self, state: StateType) -> bool:
        """éªŒè¯çŠ¶æ€"""
        pass

    @abstractmethod
    def process_state(self, state: StateType) -> StateType:
        """å¤„ç†çŠ¶æ€"""
        pass

class TextProcessor(BaseProcessor[DocumentedState]):
    """æ–‡æœ¬å¤„ç†å™¨å®ç°"""

    def __init__(self,
                 min_length: int = 1,
                 max_length: int = 10000,
                 allowed_languages: Optional[Set[str]] = None) -> None:
        self.min_length = min_length
        self.max_length = max_length
        self.allowed_languages = allowed_languages or {"zh", "en"}
        self.metrics = NodeMetrics()

    def validate_state(self, state: DocumentedState) -> bool:
        """éªŒè¯æ–‡æœ¬çŠ¶æ€

        Args:
            state: è¦éªŒè¯çš„çŠ¶æ€

        Returns:
            éªŒè¯æ˜¯å¦é€šè¿‡
        """
        text = state.get("input_text", "")
        if not isinstance(text, str):
            return False

        text_length = len(text.strip())
        return self.min_length <= text_length <= self.max_length

    def process_state(self, state: DocumentedState) -> DocumentedState:
        """å¤„ç†æ–‡æœ¬çŠ¶æ€

        Args:
            state: è¾“å…¥çŠ¶æ€

        Returns:
            å¤„ç†åçš„çŠ¶æ€

        Raises:
            ValueError: å½“çŠ¶æ€éªŒè¯å¤±è´¥æ—¶
        """
        if not self.validate_state(state):
            raise ValueError("çŠ¶æ€éªŒè¯å¤±è´¥")

        # å¤„ç†é€»è¾‘
        processed_text = state["input_text"].strip().title()

        return {
            **state,
            "processed_text": processed_text,
            "processing_steps": state["processing_steps"] + ["text_processing"],
            "metadata": {
                **state["metadata"],
                "processor_type": "TextProcessor",
                "processing_time": 0.1  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            }
        }

def create_typed_processing_node(
    processor: BaseProcessor[StateType],
    metrics_enabled: bool = True
) -> Callable[[StateType], StateType]:
    """åˆ›å»ºç±»å‹åŒ–å¤„ç†èŠ‚ç‚¹

    Args:
        processor: å¤„ç†å™¨å®ä¾‹
        metrics_enabled: æ˜¯å¦å¯ç”¨æŒ‡æ ‡æ”¶é›†

    Returns:
        é…ç½®å¥½çš„èŠ‚ç‚¹å‡½æ•°
    """
    import time

    def typed_node(state: StateType) -> StateType:
        start_time = time.time() if metrics_enabled else None

        try:
            result = processor.process_state(state)

            if metrics_enabled and hasattr(processor, 'metrics'):
                execution_time = time.time() - start_time
                processor.metrics.execution_count += 1
                processor.metrics.total_execution_time += execution_time
                processor.metrics.last_execution_time = execution_time

            return result

        except Exception as e:
            if metrics_enabled and hasattr(processor, 'metrics'):
                processor.metrics.error_count += 1
            raise

    return typed_node

# ä½¿ç”¨ç±»å‹åŒ–ç»„ä»¶
def create_fully_typed_graph() -> Tuple[StateGraph, TextProcessor]:
    """åˆ›å»ºå®Œå…¨ç±»å‹åŒ–çš„å›¾

    Returns:
        ç¼–è¯‘å¥½çš„å›¾å’Œå¤„ç†å™¨å®ä¾‹çš„å…ƒç»„
    """
    processor = TextProcessor(min_length=5, max_length=500)

    graph = StateGraph(DocumentedState)

    # ä½¿ç”¨ç±»å‹åŒ–èŠ‚ç‚¹
    typed_node = create_typed_processing_node(processor, metrics_enabled=True)
    graph.add_node("typed_processor", typed_node)

    graph.add_edge(START, "typed_processor")
    graph.add_edge("typed_processor", END)

    return graph, processor
```

## ğŸš€ éƒ¨ç½²å’Œè¿ç»´æœ€ä½³å®è·µ

### 1. é…ç½®ç®¡ç†

```python
import os
import json
from pathlib import Path
from typing import Any, Dict, Optional
from dataclasses import dataclass, asdict
from enum import Enum

class Environment(Enum):
    """ç¯å¢ƒæšä¸¾"""
    DEVELOPMENT = "development"
    TESTING = "testing"
    STAGING = "staging"
    PRODUCTION = "production"

@dataclass
class DatabaseConfig:
    """æ•°æ®åº“é…ç½®"""
    host: str
    port: int
    database: str
    username: str
    password: str
    pool_size: int = 10
    ssl_enabled: bool = True

@dataclass
class LLMConfig:
    """LLMé…ç½®"""
    provider: str
    model_name: str
    api_key: str
    max_tokens: int = 1000
    temperature: float = 0.7
    timeout: int = 30

@dataclass
class AppConfig:
    """åº”ç”¨é…ç½®"""
    environment: Environment
    debug: bool
    log_level: str
    database: DatabaseConfig
    llm: LLMConfig
    redis_url: str
    max_workers: int = 4
    enable_metrics: bool = True

class ConfigManager:
    """é…ç½®ç®¡ç†å™¨"""

    def __init__(self, config_dir: str = "config"):
        self.config_dir = Path(config_dir)
        self.config_dir.mkdir(exist_ok=True)
        self._config: Optional[AppConfig] = None

    def load_config(self, environment: Environment = None) -> AppConfig:
        """åŠ è½½é…ç½®"""
        if environment is None:
            environment = Environment(os.getenv("APP_ENV", "development"))

        # åŠ è½½åŸºç¡€é…ç½®
        base_config = self._load_config_file("base.json")

        # åŠ è½½ç¯å¢ƒç‰¹å®šé…ç½®
        env_config = self._load_config_file(f"{environment.value}.json")

        # åˆå¹¶é…ç½®
        merged_config = {**base_config, **env_config}

        # åº”ç”¨ç¯å¢ƒå˜é‡è¦†ç›–
        self._apply_env_overrides(merged_config)

        # åˆ›å»ºé…ç½®å¯¹è±¡
        self._config = self._create_config_object(merged_config, environment)

        return self._config

    def _load_config_file(self, filename: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        filepath = self.config_dir / filename
        if not filepath.exists():
            return {}

        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)

    def _apply_env_overrides(self, config: Dict[str, Any]):
        """åº”ç”¨ç¯å¢ƒå˜é‡è¦†ç›–"""
        env_mapping = {
            "DB_HOST": ("database", "host"),
            "DB_PORT": ("database", "port"),
            "DB_PASSWORD": ("database", "password"),
            "LLM_API_KEY": ("llm", "api_key"),
            "LLM_MODEL": ("llm", "model_name"),
            "REDIS_URL": ("redis_url",),
            "DEBUG": ("debug",),
            "LOG_LEVEL": ("log_level",),
        }

        for env_var, config_path in env_mapping.items():
            value = os.getenv(env_var)
            if value is not None:
                # ç±»å‹è½¬æ¢
                if env_var in ["DB_PORT", "DEBUG"]:
                    value = int(value) if env_var == "DB_PORT" else value.lower() == "true"

                # è®¾ç½®åµŒå¥—é…ç½®å€¼
                target = config
                for key in config_path[:-1]:
                    if key not in target:
                        target[key] = {}
                    target = target[key]
                target[config_path[-1]] = value

    def _create_config_object(self, config: Dict[str, Any], environment: Environment) -> AppConfig:
        """åˆ›å»ºé…ç½®å¯¹è±¡"""
        return AppConfig(
            environment=environment,
            debug=config.get("debug", False),
            log_level=config.get("log_level", "INFO"),
            database=DatabaseConfig(**config.get("database", {})),
            llm=LLMConfig(**config.get("llm", {})),
            redis_url=config.get("redis_url", "redis://localhost:6379"),
            max_workers=config.get("max_workers", 4),
            enable_metrics=config.get("enable_metrics", True)
        )

    def save_config_template(self):
        """ä¿å­˜é…ç½®æ¨¡æ¿"""
        template_config = {
            "debug": False,
            "log_level": "INFO",
            "max_workers": 4,
            "enable_metrics": True,
            "database": {
                "host": "localhost",
                "port": 5432,
                "database": "langgraph_app",
                "username": "user",
                "password": "password",
                "pool_size": 10,
                "ssl_enabled": True
            },
            "llm": {
                "provider": "openai",
                "model_name": "gpt-3.5-turbo",
                "api_key": "your-api-key",
                "max_tokens": 1000,
                "temperature": 0.7,
                "timeout": 30
            },
            "redis_url": "redis://localhost:6379"
        }

        # ä¿å­˜åŸºç¡€é…ç½®
        base_file = self.config_dir / "base.json"
        with open(base_file, 'w', encoding='utf-8') as f:
            json.dump(template_config, f, indent=2)

        # ä¿å­˜ç¯å¢ƒç‰¹å®šé…ç½®æ¨¡æ¿
        env_configs = {
            "development": {"debug": True, "log_level": "DEBUG"},
            "testing": {"database": {"database": "test_db"}},
            "production": {"debug": False, "enable_metrics": True}
        }

        for env, config in env_configs.items():
            env_file = self.config_dir / f"{env}.json"
            with open(env_file, 'w', encoding='utf-8') as f:
                json.dump(config, f, indent=2)

# å…¨å±€é…ç½®ç®¡ç†å™¨
config_manager = ConfigManager()

def get_config() -> AppConfig:
    """è·å–åº”ç”¨é…ç½®"""
    return config_manager.load_config()

# ä½¿ç”¨é…ç½®çš„å›¾åˆ›å»º
def create_configured_graph(config: AppConfig = None) -> StateGraph:
    """åˆ›å»ºé…ç½®åŒ–çš„å›¾"""
    if config is None:
        config = get_config()

    def configured_llm_node(state: dict) -> dict:
        """é…ç½®åŒ–çš„LLMèŠ‚ç‚¹"""
        # ä½¿ç”¨é…ç½®ä¸­çš„LLMè®¾ç½®
        llm_config = config.llm

        # æ¨¡æ‹ŸLLMè°ƒç”¨
        response = f"ä½¿ç”¨ {llm_config.model_name} å¤„ç†: {state.get('input', '')}"

        return {
            **state,
            "llm_response": response,
            "model_used": llm_config.model_name,
            "provider": llm_config.provider
        }

    graph = StateGraph(dict)
    graph.add_node("llm_processor", configured_llm_node)
    graph.add_edge(START, "llm_processor")
    graph.add_edge("llm_processor", END)

    return graph
```

### 2. æ—¥å¿—å’Œç›‘æ§

```python
import logging
import json
import time
from datetime import datetime
from typing import Dict, Any, Optional
from functools import wraps
from dataclasses import dataclass, asdict

@dataclass
class LogEntry:
    """æ—¥å¿—æ¡ç›®"""
    timestamp: str
    level: str
    message: str
    node_name: Optional[str] = None
    execution_time: Optional[float] = None
    metadata: Optional[Dict[str, Any]] = None

class StructuredLogger:
    """ç»“æ„åŒ–æ—¥å¿—å™¨"""

    def __init__(self, name: str, config: AppConfig):
        self.logger = logging.getLogger(name)
        self.config = config
        self._setup_logger()

    def _setup_logger(self):
        """è®¾ç½®æ—¥å¿—å™¨"""
        # è®¾ç½®æ—¥å¿—çº§åˆ«
        level = getattr(logging, self.config.log_level.upper())
        self.logger.setLevel(level)

        # æ¸…é™¤ç°æœ‰å¤„ç†å™¨
        self.logger.handlers.clear()

        # åˆ›å»ºæ ¼å¼åŒ–å™¨
        if self.config.environment == Environment.PRODUCTION:
            # ç”Ÿäº§ç¯å¢ƒä½¿ç”¨JSONæ ¼å¼
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
        else:
            # å¼€å‘ç¯å¢ƒä½¿ç”¨å¯è¯»æ ¼å¼
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )

        # æ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        self.logger.addHandler(console_handler)

        # æ–‡ä»¶å¤„ç†å™¨ï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰
        if self.config.environment == Environment.PRODUCTION:
            file_handler = logging.FileHandler('app.log')
            file_handler.setFormatter(formatter)
            self.logger.addHandler(file_handler)

    def log_node_execution(self,
                          node_name: str,
                          level: str,
                          message: str,
                          execution_time: Optional[float] = None,
                          metadata: Optional[Dict[str, Any]] = None):
        """è®°å½•èŠ‚ç‚¹æ‰§è¡Œæ—¥å¿—"""
        log_entry = LogEntry(
            timestamp=datetime.now().isoformat(),
            level=level,
            message=message,
            node_name=node_name,
            execution_time=execution_time,
            metadata=metadata
        )

        # æ ¹æ®ç¯å¢ƒé€‰æ‹©æ—¥å¿—æ ¼å¼
        if self.config.environment == Environment.PRODUCTION:
            log_message = json.dumps(asdict(log_entry), ensure_ascii=False)
        else:
            log_message = f"[{node_name}] {message}"
            if execution_time:
                log_message += f" (è€—æ—¶: {execution_time:.3f}s)"

        # è®°å½•æ—¥å¿—
        log_level = getattr(self.logger, level.lower())
        log_level(log_message)

class MetricsCollector:
    """æŒ‡æ ‡æ”¶é›†å™¨"""

    def __init__(self, enabled: bool = True):
        self.enabled = enabled
        self.metrics = {
            "node_executions": {},
            "execution_times": {},
            "error_counts": {},
            "graph_runs": 0,
            "total_execution_time": 0.0
        }

    def record_node_execution(self, node_name: str, execution_time: float, success: bool):
        """è®°å½•èŠ‚ç‚¹æ‰§è¡ŒæŒ‡æ ‡"""
        if not self.enabled:
            return

        # æ‰§è¡Œæ¬¡æ•°
        if node_name not in self.metrics["node_executions"]:
            self.metrics["node_executions"][node_name] = 0
        self.metrics["node_executions"][node_name] += 1

        # æ‰§è¡Œæ—¶é—´
        if node_name not in self.metrics["execution_times"]:
            self.metrics["execution_times"][node_name] = []
        self.metrics["execution_times"][node_name].append(execution_time)

        # é”™è¯¯è®¡æ•°
        if not success:
            if node_name not in self.metrics["error_counts"]:
                self.metrics["error_counts"][node_name] = 0
            self.metrics["error_counts"][node_name] += 1

    def record_graph_run(self, execution_time: float):
        """è®°å½•å›¾è¿è¡ŒæŒ‡æ ‡"""
        if not self.enabled:
            return

        self.metrics["graph_runs"] += 1
        self.metrics["total_execution_time"] += execution_time

    def get_metrics_summary(self) -> Dict[str, Any]:
        """è·å–æŒ‡æ ‡æ‘˜è¦"""
        if not self.enabled:
            return {"metrics_disabled": True}

        summary = {}

        # èŠ‚ç‚¹æŒ‡æ ‡æ‘˜è¦
        for node_name, times in self.metrics["execution_times"].items():
            if times:
                summary[node_name] = {
                    "executions": self.metrics["node_executions"].get(node_name, 0),
                    "avg_time": sum(times) / len(times),
                    "max_time": max(times),
                    "min_time": min(times),
                    "errors": self.metrics["error_counts"].get(node_name, 0)
                }

        # æ•´ä½“æŒ‡æ ‡
        summary["overall"] = {
            "total_graph_runs": self.metrics["graph_runs"],
            "total_execution_time": self.metrics["total_execution_time"],
            "avg_graph_time": (
                self.metrics["total_execution_time"] / self.metrics["graph_runs"]
                if self.metrics["graph_runs"] > 0 else 0
            )
        }

        return summary

# å…¨å±€ç»„ä»¶
app_config = get_config()
structured_logger = StructuredLogger("langgraph_app", app_config)
metrics_collector = MetricsCollector(enabled=app_config.enable_metrics)

def monitored_node(node_name: str):
    """ç›‘æ§èŠ‚ç‚¹è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        def wrapper(state: dict) -> dict:
            start_time = time.time()

            # è®°å½•å¼€å§‹
            structured_logger.log_node_execution(
                node_name, "INFO", f"å¼€å§‹æ‰§è¡ŒèŠ‚ç‚¹: {node_name}"
            )

            try:
                result = func(state)
                execution_time = time.time() - start_time

                # è®°å½•æˆåŠŸ
                structured_logger.log_node_execution(
                    node_name, "INFO", f"èŠ‚ç‚¹æ‰§è¡ŒæˆåŠŸ: {node_name}",
                    execution_time=execution_time
                )

                metrics_collector.record_node_execution(
                    node_name, execution_time, success=True
                )

                return result

            except Exception as e:
                execution_time = time.time() - start_time

                # è®°å½•é”™è¯¯
                structured_logger.log_node_execution(
                    node_name, "ERROR", f"èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥: {node_name} - {str(e)}",
                    execution_time=execution_time,
                    metadata={"error_type": type(e).__name__}
                )

                metrics_collector.record_node_execution(
                    node_name, execution_time, success=False
                )

                raise

        return wrapper
    return decorator

def create_monitored_graph() -> StateGraph:
    """åˆ›å»ºç›‘æ§åŒ–çš„å›¾"""

    @monitored_node("data_processor")
    def data_processor(state: dict) -> dict:
        # æ¨¡æ‹Ÿå¤„ç†
        time.sleep(0.1)
        return {**state, "processed": True}

    @monitored_node("validator")
    def validator(state: dict) -> dict:
        # æ¨¡æ‹ŸéªŒè¯
        if not state.get("input"):
            raise ValueError("è¾“å…¥ä¸ºç©º")
        return {**state, "validated": True}

    graph = StateGraph(dict)
    graph.add_node("validator", validator)
    graph.add_node("processor", data_processor)

    graph.add_edge(START, "validator")
    graph.add_edge("validator", "processor")
    graph.add_edge("processor", END)

    return graph

def run_with_monitoring(graph_app, input_state: dict) -> dict:
    """è¿è¡Œå¸¦ç›‘æ§çš„å›¾"""
    start_time = time.time()

    try:
        result = graph_app.invoke(input_state)
        execution_time = time.time() - start_time

        metrics_collector.record_graph_run(execution_time)

        structured_logger.log_node_execution(
            "graph", "INFO", "å›¾æ‰§è¡Œå®Œæˆ",
            execution_time=execution_time,
            metadata={"success": True}
        )

        return result

    except Exception as e:
        execution_time = time.time() - start_time

        structured_logger.log_node_execution(
            "graph", "ERROR", f"å›¾æ‰§è¡Œå¤±è´¥: {str(e)}",
            execution_time=execution_time,
            metadata={"success": False, "error_type": type(e).__name__}
        )

        raise
```

## ğŸ“š å»¶ä¼¸é˜…è¯»

- [Clean Code åŸåˆ™](https://clean-code-developer.com/)
- [Python ä»£ç é£æ ¼æŒ‡å— PEP 8](https://pep8.org/)
- [è®¾è®¡æ¨¡å¼è¯¦è§£](https://refactoring.guru/design-patterns)
- [åäºŒè¦ç´ åº”ç”¨](https://12factor.net/zh_cn/)

---

ğŸ’¡ **å°è´´å£«**ï¼šå¥½çš„å®è·µéœ€è¦æŒç»­çš„åšæŒå’Œæ”¹è¿›ã€‚ä»å°å¤„ç€æ‰‹ï¼Œé€æ­¥å»ºç«‹èµ·å®Œæ•´çš„å¼€å‘è§„èŒƒä½“ç³»ï¼