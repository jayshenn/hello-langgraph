# LangGraph æ€§èƒ½ä¼˜åŒ–

> ğŸ¯ **å­¦ä¹ ç›®æ ‡**ï¼šæŒæ¡ LangGraph åº”ç”¨çš„æ€§èƒ½ä¼˜åŒ–æŠ€å·§ï¼ŒåŒ…æ‹¬æµå¼å¤„ç†ã€å¹¶è¡Œæ‰§è¡Œã€å†…å­˜ç®¡ç†ç­‰

## ğŸš€ æµå¼å¤„ç†ä¼˜åŒ–

### 1. åŸºç¡€æµå¼å¤„ç†

æµå¼å¤„ç†æ˜¯æå‡ç”¨æˆ·ä½“éªŒçš„å…³é”®æŠ€æœ¯ï¼Œè®©ç”¨æˆ·èƒ½å®æ—¶çœ‹åˆ°å¤„ç†è¿›åº¦ã€‚

```python
from langgraph import StateGraph, START, END
from typing import TypedDict, List
import asyncio
import time

class StreamingState(TypedDict):
    messages: List[str]
    progress: int
    streaming_data: List[str]

def streaming_node(state: StreamingState) -> StreamingState:
    """æ”¯æŒæµå¼è¾“å‡ºçš„èŠ‚ç‚¹"""
    print(f"ğŸ“¡ å¼€å§‹æµå¼å¤„ç†... å½“å‰è¿›åº¦: {state['progress']}%")

    # æ¨¡æ‹Ÿåˆ†å—å¤„ç†
    chunks = ["å¤„ç†ä¸­...", "åˆ†ææ•°æ®...", "ç”Ÿæˆç»“æœ...", "å®Œæˆ!"]
    streaming_data = state.get('streaming_data', [])

    for i, chunk in enumerate(chunks):
        streaming_data.append(chunk)
        progress = int((i + 1) / len(chunks) * 100)

        # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        time.sleep(0.5)

        print(f"ğŸ“¦ æµå¼è¾“å‡º: {chunk} ({progress}%)")

    return {
        "messages": state["messages"] + ["æµå¼å¤„ç†å®Œæˆ"],
        "progress": 100,
        "streaming_data": streaming_data
    }

def create_streaming_graph():
    """åˆ›å»ºæ”¯æŒæµå¼å¤„ç†çš„å›¾"""
    graph = StateGraph(StreamingState)
    graph.add_node("streaming_node", streaming_node)
    graph.add_edge(START, "streaming_node")
    graph.add_edge("streaming_node", END)
    return graph.compile()

# ä½¿ç”¨æµå¼å¤„ç†
def demonstrate_streaming():
    """æ¼”ç¤ºæµå¼å¤„ç†"""
    app = create_streaming_graph()
    initial_state = {
        "messages": ["å¼€å§‹"],
        "progress": 0,
        "streaming_data": []
    }

    print("ğŸ¬ å¼€å§‹æµå¼æ‰§è¡Œ:")
    for event in app.stream(initial_state):
        print(f"ğŸ“¨ æ¥æ”¶äº‹ä»¶: {event}")
```

### 2. å¼‚æ­¥æµå¼å¤„ç†

```python
import asyncio
from typing import AsyncIterator

async def async_streaming_node(state: StreamingState) -> StreamingState:
    """å¼‚æ­¥æµå¼èŠ‚ç‚¹"""
    print("ğŸ”„ å¼‚æ­¥æµå¼å¤„ç†å¼€å§‹...")

    # æ¨¡æ‹Ÿå¼‚æ­¥æ“ä½œ
    tasks = [
        "è¿æ¥æ•°æ®åº“",
        "æŸ¥è¯¢æ•°æ®",
        "å¤„ç†æ•°æ®",
        "ç”Ÿæˆå“åº”"
    ]

    streaming_data = state.get('streaming_data', [])

    for i, task in enumerate(tasks):
        # å¼‚æ­¥ç­‰å¾…
        await asyncio.sleep(0.3)

        streaming_data.append(f"âœ… {task}")
        progress = int((i + 1) / len(tasks) * 100)

        print(f"âš¡ å¼‚æ­¥å¤„ç†: {task} ({progress}%)")

    return {
        "messages": state["messages"] + ["å¼‚æ­¥å¤„ç†å®Œæˆ"],
        "progress": 100,
        "streaming_data": streaming_data
    }

async def async_streaming_demo():
    """å¼‚æ­¥æµå¼å¤„ç†æ¼”ç¤º"""
    graph = StateGraph(StreamingState)
    graph.add_node("async_node", async_streaming_node)
    graph.add_edge(START, "async_node")
    graph.add_edge("async_node", END)

    app = graph.compile()

    initial_state = {
        "messages": ["å¼‚æ­¥å¼€å§‹"],
        "progress": 0,
        "streaming_data": []
    }

    print("âš¡ å¼‚æ­¥æµå¼æ‰§è¡Œ:")
    async for event in app.astream(initial_state):
        print(f"ğŸ“¬ å¼‚æ­¥äº‹ä»¶: {event}")

# è¿è¡Œå¼‚æ­¥æ¼”ç¤º
# asyncio.run(async_streaming_demo())
```

### 3. å¤šæ¨¡å¼æµå¼å¤„ç†

```python
from langgraph import StateGraph

def create_multi_mode_streaming():
    """åˆ›å»ºå¤šæ¨¡å¼æµå¼å¤„ç†"""
    graph = StateGraph(StreamingState)
    graph.add_node("processor", streaming_node)
    graph.add_edge(START, "processor")
    graph.add_edge("processor", END)

    app = graph.compile()

    return app

def demonstrate_streaming_modes():
    """æ¼”ç¤ºä¸åŒçš„æµå¼æ¨¡å¼"""
    app = create_multi_mode_streaming()

    initial_state = {
        "messages": ["å¤šæ¨¡å¼æµ‹è¯•"],
        "progress": 0,
        "streaming_data": []
    }

    print("ğŸ“Š æ¨¡å¼ 1: é»˜è®¤æµå¼ (updates)")
    for event in app.stream(initial_state, stream_mode="updates"):
        print(f"ğŸ“ˆ Update: {event}")

    print("\nğŸ“Š æ¨¡å¼ 2: å€¼æµå¼ (values)")
    for event in app.stream(initial_state, stream_mode="values"):
        print(f"ğŸ“Š Value: {event}")

    print("\nğŸ“Š æ¨¡å¼ 3: è°ƒè¯•æµå¼ (debug)")
    for event in app.stream(initial_state, stream_mode="debug"):
        print(f"ğŸ› Debug: {event}")
```

## âš¡ å¹¶è¡Œæ‰§è¡Œä¼˜åŒ–

### 1. èŠ‚ç‚¹å¹¶è¡Œå¤„ç†

```python
from typing import TypedDict, Annotated
from operator import add
import threading
import time

class ParallelState(TypedDict):
    input_data: str
    results_a: List[str]
    results_b: List[str]
    combined_results: Annotated[List[str], add]  # ä½¿ç”¨ reducer åˆå¹¶ç»“æœ
    execution_times: dict

def parallel_node_a(state: ParallelState) -> ParallelState:
    """å¹¶è¡ŒèŠ‚ç‚¹ A"""
    thread_id = threading.current_thread().ident
    start_time = time.time()

    print(f"ğŸ…°ï¸  èŠ‚ç‚¹ A å¼€å§‹æ‰§è¡Œ (çº¿ç¨‹: {thread_id})")

    # æ¨¡æ‹Ÿè€—æ—¶æ“ä½œ
    time.sleep(2)

    execution_time = time.time() - start_time
    result = f"Aå¤„ç†ç»“æœ: {state['input_data']}"

    print(f"âœ… èŠ‚ç‚¹ A å®Œæˆ (è€—æ—¶: {execution_time:.2f}s)")

    return {
        "results_a": [result],
        "combined_results": [result],
        "execution_times": {"node_a": execution_time}
    }

def parallel_node_b(state: ParallelState) -> ParallelState:
    """å¹¶è¡ŒèŠ‚ç‚¹ B"""
    thread_id = threading.current_thread().ident
    start_time = time.time()

    print(f"ğŸ…±ï¸  èŠ‚ç‚¹ B å¼€å§‹æ‰§è¡Œ (çº¿ç¨‹: {thread_id})")

    # æ¨¡æ‹Ÿä¸åŒçš„è€—æ—¶æ“ä½œ
    time.sleep(1.5)

    execution_time = time.time() - start_time
    result = f"Bå¤„ç†ç»“æœ: {state['input_data']}"

    print(f"âœ… èŠ‚ç‚¹ B å®Œæˆ (è€—æ—¶: {execution_time:.2f}s)")

    return {
        "results_b": [result],
        "combined_results": [result],
        "execution_times": {"node_b": execution_time}
    }

def create_parallel_graph():
    """åˆ›å»ºå¹¶è¡Œæ‰§è¡Œå›¾"""
    graph = StateGraph(ParallelState)

    # æ·»åŠ å¹¶è¡ŒèŠ‚ç‚¹
    graph.add_node("node_a", parallel_node_a)
    graph.add_node("node_b", parallel_node_b)

    # å¹¶è¡Œå¯åŠ¨
    graph.add_edge(START, "node_a")
    graph.add_edge(START, "node_b")

    # éƒ½å®Œæˆåç»“æŸ
    graph.add_edge("node_a", END)
    graph.add_edge("node_b", END)

    return graph.compile()

def test_parallel_execution():
    """æµ‹è¯•å¹¶è¡Œæ‰§è¡Œ"""
    app = create_parallel_graph()

    start_time = time.time()

    result = app.invoke({
        "input_data": "æµ‹è¯•æ•°æ®",
        "results_a": [],
        "results_b": [],
        "combined_results": [],
        "execution_times": {}
    })

    total_time = time.time() - start_time

    print(f"\nğŸ“Š å¹¶è¡Œæ‰§è¡Œç»“æœ:")
    print(f"   æ€»æ‰§è¡Œæ—¶é—´: {total_time:.2f}s")
    print(f"   èŠ‚ç‚¹æ‰§è¡Œæ—¶é—´: {result['execution_times']}")
    print(f"   åˆå¹¶ç»“æœ: {result['combined_results']}")
    print(f"   å¹¶è¡Œæ•ˆç‡: {sum(result['execution_times'].values()) / total_time:.2f}x")
```

### 2. æ¡ä»¶å¹¶è¡Œæ‰§è¡Œ

```python
def parallel_processor_node(state: ParallelState) -> ParallelState:
    """æ ¹æ®æ¡ä»¶å†³å®šå¹¶è¡Œå¤„ç†"""
    data_size = len(state.get("input_data", ""))

    if data_size > 100:
        print("ğŸ“Š æ•°æ®é‡å¤§ï¼Œå¯ç”¨å¹¶è¡Œå¤„ç†")
        # åˆ†å‰²æ•°æ®è¿›è¡Œå¹¶è¡Œå¤„ç†
        chunks = [state["input_data"][i:i+50] for i in range(0, len(state["input_data"]), 50)]

        # æ¨¡æ‹Ÿå¹¶è¡Œå¤„ç†å¤šä¸ªå—
        results = []
        for i, chunk in enumerate(chunks):
            result = f"å—{i+1}å¤„ç†ç»“æœ: {len(chunk)}å­—ç¬¦"
            results.append(result)
            print(f"ğŸ“¦ å¤„ç†å— {i+1}: {len(chunk)} å­—ç¬¦")

        return {
            **state,
            "combined_results": results,
            "processing_strategy": "parallel"
        }
    else:
        print("ğŸ“ æ•°æ®é‡å°ï¼Œä½¿ç”¨é¡ºåºå¤„ç†")
        result = f"é¡ºåºå¤„ç†ç»“æœ: {state['input_data']}"

        return {
            **state,
            "combined_results": [result],
            "processing_strategy": "sequential"
        }

def smart_routing_condition(state: ParallelState) -> str:
    """æ™ºèƒ½è·¯ç”±æ¡ä»¶"""
    data_size = len(state.get("input_data", ""))

    if data_size > 200:
        return "heavy_parallel"
    elif data_size > 50:
        return "light_parallel"
    else:
        return "sequential"

def create_adaptive_parallel_graph():
    """åˆ›å»ºè‡ªé€‚åº”å¹¶è¡Œå›¾"""
    graph = StateGraph(ParallelState)

    # æ·»åŠ ä¸åŒçš„å¤„ç†èŠ‚ç‚¹
    graph.add_node("router", parallel_processor_node)
    graph.add_node("heavy_processor", lambda s: {**s, "method": "heavy"})
    graph.add_node("light_processor", lambda s: {**s, "method": "light"})
    graph.add_node("sequential_processor", lambda s: {**s, "method": "sequential"})

    # è®¾ç½®è·¯ç”±
    graph.add_edge(START, "router")
    graph.add_conditional_edges(
        "router",
        smart_routing_condition,
        {
            "heavy_parallel": "heavy_processor",
            "light_parallel": "light_processor",
            "sequential": "sequential_processor"
        }
    )

    graph.add_edge("heavy_processor", END)
    graph.add_edge("light_processor", END)
    graph.add_edge("sequential_processor", END)

    return graph.compile()
```

## ğŸ§  å†…å­˜ç®¡ç†ä¼˜åŒ–

### 1. çŠ¶æ€å‹ç¼©å’Œæ¸…ç†

```python
from typing import TypedDict, Any
import sys
import gc

class MemoryOptimizedState(TypedDict):
    current_data: Any
    history: List[dict]
    memory_usage: dict
    compressed_history: List[str]

def memory_monitor(func):
    """å†…å­˜ç›‘æ§è£…é¥°å™¨"""
    def wrapper(state):
        # æ‰§è¡Œå‰å†…å­˜
        gc.collect()
        mem_before = sys.getsizeof(state)

        result = func(state)

        # æ‰§è¡Œåå†…å­˜
        gc.collect()
        mem_after = sys.getsizeof(result)

        # æ·»åŠ å†…å­˜ä½¿ç”¨ä¿¡æ¯
        if "memory_usage" not in result:
            result["memory_usage"] = {}

        result["memory_usage"][func.__name__] = {
            "before": mem_before,
            "after": mem_after,
            "change": mem_after - mem_before
        }

        print(f"ğŸ§  {func.__name__} å†…å­˜å˜åŒ–: {mem_after - mem_before:+d} bytes")
        return result

    return wrapper

@memory_monitor
def memory_heavy_node(state: MemoryOptimizedState) -> MemoryOptimizedState:
    """å†…å­˜å¯†é›†å‹èŠ‚ç‚¹"""
    # ç”Ÿæˆå¤§é‡æ•°æ®
    large_data = ["å¤§æ•°æ®é¡¹" + str(i) for i in range(10000)]

    # å‹ç¼©å†å²è®°å½•
    compressed_history = state.get("compressed_history", [])
    if len(state.get("history", [])) > 100:
        # åªä¿ç•™æœ€è¿‘çš„è®°å½•ï¼Œå…¶ä½™å‹ç¼©
        recent_history = state["history"][-50:]
        old_summary = f"å†å²è®°å½•æ‘˜è¦: å…±{len(state['history'])-50}æ¡è®°å½•"
        compressed_history.append(old_summary)
    else:
        recent_history = state.get("history", [])

    return {
        "current_data": large_data[:1000],  # åªä¿ç•™éƒ¨åˆ†æ•°æ®
        "history": recent_history + [{"step": "memory_heavy", "data_size": len(large_data)}],
        "memory_usage": state.get("memory_usage", {}),
        "compressed_history": compressed_history
    }

@memory_monitor
def memory_optimized_node(state: MemoryOptimizedState) -> MemoryOptimizedState:
    """å†…å­˜ä¼˜åŒ–èŠ‚ç‚¹"""
    # ä½¿ç”¨ç”Ÿæˆå™¨å‡å°‘å†…å­˜å ç”¨
    def data_generator():
        for i in range(1000):
            yield f"ä¼˜åŒ–æ•°æ®{i}"

    # åªä¿ç•™å¿…è¦çš„æ•°æ®
    current_data = list(data_generator())[:100]  # åªå–å‰100é¡¹

    # æ¸…ç†ä¸éœ€è¦çš„å†å²
    history = state.get("history", [])
    if len(history) > 10:
        # æ¸…ç†æ—§çš„å†å²è®°å½•
        history = history[-5:]  # åªä¿ç•™æœ€è¿‘5æ¡

    return {
        "current_data": current_data,
        "history": history + [{"step": "memory_optimized", "optimized": True}],
        "memory_usage": state.get("memory_usage", {}),
        "compressed_history": state.get("compressed_history", [])
    }

def create_memory_optimized_graph():
    """åˆ›å»ºå†…å­˜ä¼˜åŒ–å›¾"""
    graph = StateGraph(MemoryOptimizedState)

    graph.add_node("heavy_node", memory_heavy_node)
    graph.add_node("optimized_node", memory_optimized_node)

    graph.add_edge(START, "heavy_node")
    graph.add_edge("heavy_node", "optimized_node")
    graph.add_edge("optimized_node", END)

    return graph.compile()
```

### 2. æ‡’åŠ è½½å’Œç¼“å­˜ç­–ç•¥

```python
from functools import lru_cache
import weakref
import pickle

class LazyLoader:
    """æ‡’åŠ è½½å™¨"""

    def __init__(self):
        self._cache = weakref.WeakValueDictionary()
        self._loading_cache = {}

    @lru_cache(maxsize=128)
    def load_expensive_resource(self, resource_id: str):
        """åŠ è½½æ˜‚è´µèµ„æºï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        print(f"ğŸ”„ åŠ è½½èµ„æº: {resource_id}")

        # æ¨¡æ‹Ÿæ˜‚è´µçš„åŠ è½½æ“ä½œ
        time.sleep(1)

        resource = {
            "id": resource_id,
            "data": f"æ˜‚è´µèµ„æºæ•°æ®_{resource_id}",
            "size": 1024 * 1024  # 1MB
        }

        return resource

    def get_resource(self, resource_id: str):
        """è·å–èµ„æºï¼ˆå¸¦å¼±å¼•ç”¨ç¼“å­˜ï¼‰"""
        # æ£€æŸ¥å¼±å¼•ç”¨ç¼“å­˜
        if resource_id in self._cache:
            print(f"ğŸ’¾ ä»å¼±å¼•ç”¨ç¼“å­˜è·å–: {resource_id}")
            return self._cache[resource_id]

        # åŠ è½½æ–°èµ„æº
        resource = self.load_expensive_resource(resource_id)
        self._cache[resource_id] = resource

        return resource

# å…¨å±€æ‡’åŠ è½½å™¨
lazy_loader = LazyLoader()

def lazy_loading_node(state: dict) -> dict:
    """æ‡’åŠ è½½èŠ‚ç‚¹"""
    resource_ids = state.get("required_resources", [])
    loaded_resources = {}

    for resource_id in resource_ids:
        # åªåœ¨éœ€è¦æ—¶åŠ è½½
        if state.get("load_immediately", False):
            resource = lazy_loader.get_resource(resource_id)
            loaded_resources[resource_id] = resource
        else:
            # å»¶è¿ŸåŠ è½½ - åªå­˜å‚¨å¼•ç”¨
            loaded_resources[resource_id] = {"lazy_ref": resource_id}

    return {
        **state,
        "loaded_resources": loaded_resources,
        "load_strategy": "lazy" if not state.get("load_immediately") else "immediate"
    }

def caching_node(state: dict) -> dict:
    """ç¼“å­˜èŠ‚ç‚¹"""
    cache_key = str(hash(str(state.get("input_data", ""))))

    # æ£€æŸ¥ç¼“å­˜
    if hasattr(caching_node, "_cache"):
        if cache_key in caching_node._cache:
            print(f"ğŸ“¦ ä»ç¼“å­˜è·å–ç»“æœ: {cache_key[:8]}...")
            return {
                **state,
                "result": caching_node._cache[cache_key],
                "from_cache": True
            }

    # è®¡ç®—æ–°ç»“æœ
    print(f"âš™ï¸  è®¡ç®—æ–°ç»“æœ: {cache_key[:8]}...")
    time.sleep(0.5)  # æ¨¡æ‹Ÿè®¡ç®—æ—¶é—´

    result = f"è®¡ç®—ç»“æœ_{cache_key[:8]}"

    # ç¼“å­˜ç»“æœ
    if not hasattr(caching_node, "_cache"):
        caching_node._cache = {}

    caching_node._cache[cache_key] = result

    # é™åˆ¶ç¼“å­˜å¤§å°
    if len(caching_node._cache) > 100:
        # åˆ é™¤æœ€æ—§çš„ç¼“å­˜é¡¹
        oldest_key = next(iter(caching_node._cache))
        del caching_node._cache[oldest_key]

    return {
        **state,
        "result": result,
        "from_cache": False
    }
```

## ğŸ”§ ç³»ç»Ÿçº§ä¼˜åŒ–

### 1. è¿æ¥æ± å’Œèµ„æºæ± 

```python
import threading
from queue import Queue
from contextlib import contextmanager

class ConnectionPool:
    """è¿æ¥æ± å®ç°"""

    def __init__(self, create_connection_func, max_connections=10):
        self.create_connection = create_connection_func
        self.max_connections = max_connections
        self._pool = Queue(maxsize=max_connections)
        self._lock = threading.Lock()
        self._created_connections = 0

    @contextmanager
    def get_connection(self):
        """è·å–è¿æ¥ï¼ˆä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼‰"""
        connection = self._get_connection()
        try:
            yield connection
        finally:
            self._return_connection(connection)

    def _get_connection(self):
        """è·å–è¿æ¥"""
        try:
            # å°è¯•ä»æ± ä¸­è·å–
            return self._pool.get_nowait()
        except:
            # æ± ä¸ºç©ºï¼Œåˆ›å»ºæ–°è¿æ¥
            with self._lock:
                if self._created_connections < self.max_connections:
                    connection = self.create_connection()
                    self._created_connections += 1
                    print(f"ğŸ”Œ åˆ›å»ºæ–°è¿æ¥ ({self._created_connections}/{self.max_connections})")
                    return connection
                else:
                    # ç­‰å¾…å¯ç”¨è¿æ¥
                    print("â³ ç­‰å¾…å¯ç”¨è¿æ¥...")
                    return self._pool.get()

    def _return_connection(self, connection):
        """å½’è¿˜è¿æ¥"""
        try:
            self._pool.put_nowait(connection)
        except:
            print("âš ï¸  è¿æ¥æ± å·²æ»¡ï¼Œä¸¢å¼ƒè¿æ¥")

# æ¨¡æ‹Ÿæ•°æ®åº“è¿æ¥
def create_db_connection():
    """åˆ›å»ºæ•°æ®åº“è¿æ¥"""
    time.sleep(0.1)  # æ¨¡æ‹Ÿè¿æ¥æ—¶é—´
    return {"connection_id": time.time(), "status": "connected"}

# å…¨å±€è¿æ¥æ± 
db_pool = ConnectionPool(create_db_connection, max_connections=5)

def database_node(state: dict) -> dict:
    """ä½¿ç”¨è¿æ¥æ± çš„æ•°æ®åº“èŠ‚ç‚¹"""
    with db_pool.get_connection() as connection:
        print(f"ğŸ’¾ ä½¿ç”¨è¿æ¥: {connection['connection_id']}")

        # æ¨¡æ‹Ÿæ•°æ®åº“æ“ä½œ
        time.sleep(0.2)

        query_result = f"æŸ¥è¯¢ç»“æœ_{connection['connection_id']}"

        return {
            **state,
            "db_result": query_result,
            "connection_id": connection['connection_id']
        }
```

### 2. æ‰¹é‡å¤„ç†ä¼˜åŒ–

```python
from typing import List
import threading
from collections import defaultdict

class BatchProcessor:
    """æ‰¹é‡å¤„ç†å™¨"""

    def __init__(self, batch_size=10, max_wait_time=1.0):
        self.batch_size = batch_size
        self.max_wait_time = max_wait_time
        self._batches = defaultdict(list)
        self._timers = {}
        self._lock = threading.Lock()

    def add_to_batch(self, batch_key: str, item: dict, callback):
        """æ·»åŠ é¡¹åˆ°æ‰¹æ¬¡"""
        with self._lock:
            self._batches[batch_key].append((item, callback))

            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ‰¹æ¬¡å¤§å°
            if len(self._batches[batch_key]) >= self.batch_size:
                self._process_batch(batch_key)
            else:
                # è®¾ç½®å®šæ—¶å™¨
                if batch_key not in self._timers:
                    timer = threading.Timer(self.max_wait_time, self._process_batch, [batch_key])
                    timer.start()
                    self._timers[batch_key] = timer

    def _process_batch(self, batch_key: str):
        """å¤„ç†æ‰¹æ¬¡"""
        with self._lock:
            if batch_key not in self._batches:
                return

            batch = self._batches.pop(batch_key)

            # æ¸…ç†å®šæ—¶å™¨
            if batch_key in self._timers:
                self._timers[batch_key].cancel()
                del self._timers[batch_key]

        if not batch:
            return

        print(f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_key}: {len(batch)} é¡¹")

        # æ‰¹é‡å¤„ç†
        items = [item for item, _ in batch]
        callbacks = [callback for _, callback in batch]

        # æ¨¡æ‹Ÿæ‰¹é‡æ“ä½œ
        results = self._batch_operation(items)

        # è°ƒç”¨å›è°ƒ
        for result, callback in zip(results, callbacks):
            callback(result)

    def _batch_operation(self, items: List[dict]) -> List[dict]:
        """æ‰¹é‡æ“ä½œ"""
        print(f"âš™ï¸  æ‰¹é‡å¤„ç† {len(items)} é¡¹...")
        time.sleep(0.1 * len(items))  # æ¨¡æ‹Ÿæ‰¹é‡å¤„ç†æ—¶é—´

        return [{"result": f"æ‰¹é‡å¤„ç†_{item.get('id', 'unknown')}"} for item in items]

# å…¨å±€æ‰¹é‡å¤„ç†å™¨
batch_processor = BatchProcessor(batch_size=5, max_wait_time=2.0)

def batch_processing_node(state: dict) -> dict:
    """æ”¯æŒæ‰¹é‡å¤„ç†çš„èŠ‚ç‚¹"""
    result = {"pending": True}

    def result_callback(batch_result):
        """ç»“æœå›è°ƒ"""
        result.update(batch_result)
        result["pending"] = False

    # æ·»åŠ åˆ°æ‰¹æ¬¡
    batch_processor.add_to_batch(
        "default_batch",
        {"id": state.get("id", "unknown"), "data": state.get("data")},
        result_callback
    )

    # ç­‰å¾…æ‰¹é‡å¤„ç†å®Œæˆ
    while result.get("pending", True):
        time.sleep(0.1)

    return {
        **state,
        "batch_result": result.get("result"),
        "processing_method": "batch"
    }
```

## ğŸ“Š æ€§èƒ½ç›‘æ§å’Œåˆ†æ

### 1. æ€§èƒ½åˆ†æå™¨

```python
import cProfile
import pstats
import io
from functools import wraps

class PerformanceProfiler:
    """æ€§èƒ½åˆ†æå™¨"""

    def __init__(self):
        self.profiles = {}

    def profile_node(self, node_name: str):
        """èŠ‚ç‚¹æ€§èƒ½åˆ†æè£…é¥°å™¨"""
        def decorator(func):
            @wraps(func)
            def wrapper(state):
                profiler = cProfile.Profile()
                profiler.enable()

                try:
                    result = func(state)
                finally:
                    profiler.disable()

                # ä¿å­˜åˆ†æç»“æœ
                self.profiles[node_name] = profiler

                return result
            return wrapper
        return decorator

    def get_stats(self, node_name: str, sort_by='cumulative'):
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        if node_name not in self.profiles:
            return None

        s = io.StringIO()
        ps = pstats.Stats(self.profiles[node_name], stream=s)
        ps.sort_stats(sort_by)
        ps.print_stats()

        return s.getvalue()

    def print_all_stats(self):
        """æ‰“å°æ‰€æœ‰èŠ‚ç‚¹çš„æ€§èƒ½ç»Ÿè®¡"""
        for node_name in self.profiles:
            print(f"\nğŸ“Š {node_name} æ€§èƒ½åˆ†æ:")
            print(self.get_stats(node_name))

# å…¨å±€æ€§èƒ½åˆ†æå™¨
perf_profiler = PerformanceProfiler()

@perf_profiler.profile_node("cpu_intensive_node")
def cpu_intensive_node(state: dict) -> dict:
    """CPUå¯†é›†å‹èŠ‚ç‚¹"""
    # æ¨¡æ‹ŸCPUå¯†é›†å‹æ“ä½œ
    result = 0
    for i in range(1000000):
        result += i * i

    return {
        **state,
        "cpu_result": result,
        "iterations": 1000000
    }

@perf_profiler.profile_node("optimized_cpu_node")
def optimized_cpu_node(state: dict) -> dict:
    """ä¼˜åŒ–åçš„CPUå¯†é›†å‹èŠ‚ç‚¹"""
    # ä½¿ç”¨æ›´é«˜æ•ˆçš„ç®—æ³•
    n = 1000000
    result = n * (n - 1) * (2 * n - 1) // 6  # å¹³æ–¹å’Œå…¬å¼

    return {
        **state,
        "cpu_result": result,
        "iterations": n,
        "optimized": True
    }
```

### 2. å®æ—¶æ€§èƒ½ç›‘æ§

```python
import psutil
import time
from threading import Thread
from dataclasses import dataclass
from typing import List

@dataclass
class PerformanceMetrics:
    timestamp: float
    cpu_percent: float
    memory_percent: float
    memory_mb: float
    node_name: str = None

class RealTimeMonitor:
    """å®æ—¶æ€§èƒ½ç›‘æ§å™¨"""

    def __init__(self, interval=1.0):
        self.interval = interval
        self.metrics: List[PerformanceMetrics] = []
        self.monitoring = False
        self.monitor_thread = None

    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        if self.monitoring:
            return

        self.monitoring = True
        self.monitor_thread = Thread(target=self._monitor_loop)
        self.monitor_thread.start()
        print("ğŸ“ˆ å¼€å§‹å®æ—¶æ€§èƒ½ç›‘æ§")

    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join()
        print("ğŸ“‰ åœæ­¢æ€§èƒ½ç›‘æ§")

    def _monitor_loop(self):
        """ç›‘æ§å¾ªç¯"""
        while self.monitoring:
            metrics = PerformanceMetrics(
                timestamp=time.time(),
                cpu_percent=psutil.cpu_percent(),
                memory_percent=psutil.virtual_memory().percent,
                memory_mb=psutil.virtual_memory().used / 1024 / 1024
            )

            self.metrics.append(metrics)

            # åªä¿ç•™æœ€è¿‘çš„1000ä¸ªæ•°æ®ç‚¹
            if len(self.metrics) > 1000:
                self.metrics = self.metrics[-1000:]

            time.sleep(self.interval)

    def add_node_marker(self, node_name: str):
        """æ·»åŠ èŠ‚ç‚¹æ ‡è®°"""
        if self.metrics:
            self.metrics[-1].node_name = node_name

    def get_summary(self) -> dict:
        """è·å–æ€§èƒ½æ‘˜è¦"""
        if not self.metrics:
            return {}

        cpu_values = [m.cpu_percent for m in self.metrics]
        memory_values = [m.memory_mb for m in self.metrics]

        return {
            "duration": self.metrics[-1].timestamp - self.metrics[0].timestamp,
            "samples": len(self.metrics),
            "cpu": {
                "avg": sum(cpu_values) / len(cpu_values),
                "max": max(cpu_values),
                "min": min(cpu_values)
            },
            "memory": {
                "avg": sum(memory_values) / len(memory_values),
                "max": max(memory_values),
                "min": min(memory_values),
                "peak_mb": max(memory_values)
            }
        }

# å…¨å±€ç›‘æ§å™¨
rt_monitor = RealTimeMonitor(interval=0.5)

def monitored_node(node_name: str):
    """ç›‘æ§èŠ‚ç‚¹è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        def wrapper(state):
            rt_monitor.add_node_marker(f"{node_name}_start")

            start_time = time.time()
            result = func(state)
            execution_time = time.time() - start_time

            rt_monitor.add_node_marker(f"{node_name}_end")

            # æ·»åŠ æ‰§è¡Œæ—¶é—´åˆ°ç»“æœ
            if "performance" not in result:
                result["performance"] = {}
            result["performance"][node_name] = execution_time

            return result
        return wrapper
    return decorator

@monitored_node("example_heavy_node")
def example_heavy_node(state: dict) -> dict:
    """ç¤ºä¾‹é‡å‹èŠ‚ç‚¹"""
    # æ¨¡æ‹Ÿé‡å‹æ“ä½œ
    time.sleep(2)
    return {
        **state,
        "heavy_result": "å®Œæˆé‡å‹æ“ä½œ"
    }
```

## ğŸ¯ æ€§èƒ½ä¼˜åŒ–æœ€ä½³å®è·µ

### 1. æ€§èƒ½ä¼˜åŒ–æ£€æŸ¥æ¸…å•

```python
def performance_optimization_checklist():
    """æ€§èƒ½ä¼˜åŒ–æ£€æŸ¥æ¸…å•"""
    checklist = [
        "âœ… ä½¿ç”¨æµå¼å¤„ç†æå‡ç”¨æˆ·ä½“éªŒ",
        "âœ… è¯†åˆ«å¹¶è¡Œæ‰§è¡Œæœºä¼š",
        "âœ… å®ç°çŠ¶æ€å‹ç¼©å’Œæ¸…ç†",
        "âœ… ä½¿ç”¨æ‡’åŠ è½½å‡å°‘å†…å­˜å ç”¨",
        "âœ… å®ç°ç¼“å­˜ç­–ç•¥",
        "âœ… ä½¿ç”¨è¿æ¥æ± ç®¡ç†èµ„æº",
        "âœ… è€ƒè™‘æ‰¹é‡å¤„ç†ä¼˜åŒ–",
        "âœ… ç›‘æ§å…³é”®æ€§èƒ½æŒ‡æ ‡",
        "âœ… å®šæœŸè¿›è¡Œæ€§èƒ½åˆ†æ",
        "âœ… è®¾ç½®æ€§èƒ½å‘Šè­¦é˜ˆå€¼"
    ]

    print("ğŸš€ æ€§èƒ½ä¼˜åŒ–æ£€æŸ¥æ¸…å•:")
    for item in checklist:
        print(f"  {item}")

performance_optimization_checklist()
```

### 2. æ€§èƒ½æµ‹è¯•æ¡†æ¶

```python
import unittest
import time
from typing import Callable

class PerformanceTestCase(unittest.TestCase):
    """æ€§èƒ½æµ‹è¯•ç”¨ä¾‹åŸºç±»"""

    def setUp(self):
        self.start_time = time.time()
        rt_monitor.start_monitoring()

    def tearDown(self):
        rt_monitor.stop_monitoring()
        execution_time = time.time() - self.start_time
        print(f"ğŸ“Š æµ‹è¯•æ‰§è¡Œæ—¶é—´: {execution_time:.2f}s")
        print(f"ğŸ“ˆ æ€§èƒ½æ‘˜è¦: {rt_monitor.get_summary()}")

    def assertExecutionTime(self, func: Callable, max_time: float, *args, **kwargs):
        """æ–­è¨€æ‰§è¡Œæ—¶é—´"""
        start = time.time()
        result = func(*args, **kwargs)
        execution_time = time.time() - start

        self.assertLessEqual(
            execution_time,
            max_time,
            f"æ‰§è¡Œæ—¶é—´ {execution_time:.2f}s è¶…è¿‡é¢„æœŸ {max_time}s"
        )

        return result

    def assertMemoryUsage(self, func: Callable, max_memory_mb: float, *args, **kwargs):
        """æ–­è¨€å†…å­˜ä½¿ç”¨"""
        process = psutil.Process()
        mem_before = process.memory_info().rss / 1024 / 1024

        result = func(*args, **kwargs)

        mem_after = process.memory_info().rss / 1024 / 1024
        memory_increase = mem_after - mem_before

        self.assertLessEqual(
            memory_increase,
            max_memory_mb,
            f"å†…å­˜å¢é•¿ {memory_increase:.2f}MB è¶…è¿‡é¢„æœŸ {max_memory_mb}MB"
        )

        return result

class GraphPerformanceTests(PerformanceTestCase):
    """å›¾æ€§èƒ½æµ‹è¯•"""

    def test_streaming_performance(self):
        """æµ‹è¯•æµå¼å¤„ç†æ€§èƒ½"""
        app = create_streaming_graph()

        def run_streaming_test():
            return app.invoke({
                "messages": ["æ€§èƒ½æµ‹è¯•"],
                "progress": 0,
                "streaming_data": []
            })

        # æ–­è¨€æ‰§è¡Œæ—¶é—´ä¸è¶…è¿‡3ç§’
        result = self.assertExecutionTime(run_streaming_test, 3.0)
        self.assertEqual(result["progress"], 100)

    def test_parallel_performance(self):
        """æµ‹è¯•å¹¶è¡Œå¤„ç†æ€§èƒ½"""
        app = create_parallel_graph()

        def run_parallel_test():
            return app.invoke({
                "input_data": "æ€§èƒ½æµ‹è¯•æ•°æ®",
                "results_a": [],
                "results_b": [],
                "combined_results": [],
                "execution_times": {}
            })

        # å¹¶è¡Œæ‰§è¡Œåº”è¯¥æ¯”é¡ºåºæ‰§è¡Œå¿«
        result = self.assertExecutionTime(run_parallel_test, 2.5)
        self.assertIn("node_a", result["execution_times"])
        self.assertIn("node_b", result["execution_times"])

# è¿è¡Œæ€§èƒ½æµ‹è¯•
# if __name__ == "__main__":
#     unittest.main()
```

## ğŸ“š å»¶ä¼¸é˜…è¯»

- [LangGraph æµå¼å¤„ç†æ–‡æ¡£](https://langchain-ai.github.io/langgraph/concepts/streaming/)
- [Python æ€§èƒ½ä¼˜åŒ–æŒ‡å—](https://docs.python.org/3/howto/perf_profiling.html)
- [å¼‚æ­¥ç¼–ç¨‹æœ€ä½³å®è·µ](https://docs.python.org/3/library/asyncio.html)

---

ğŸ’¡ **å°è´´å£«**ï¼šæ€§èƒ½ä¼˜åŒ–æ˜¯ä¸€ä¸ªæŒç»­çš„è¿‡ç¨‹ã€‚ä»ç”¨æˆ·ä½“éªŒå‡ºå‘ï¼Œè¯†åˆ«ç“¶é¢ˆï¼Œæœ‰é’ˆå¯¹æ€§åœ°ä¼˜åŒ–ï¼Œå¹¶æŒç»­ç›‘æ§æ•ˆæœï¼