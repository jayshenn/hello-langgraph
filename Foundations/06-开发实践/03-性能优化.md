# LangGraph 性能优化

> 🎯 **学习目标**：掌握 LangGraph 应用的性能优化技巧，包括流式处理、并行执行、内存管理等

## 🚀 流式处理优化

### 1. 基础流式处理

流式处理是提升用户体验的关键技术，让用户能实时看到处理进度。

```python
from langgraph import StateGraph, START, END
from typing import TypedDict, List
import asyncio
import time

class StreamingState(TypedDict):
    messages: List[str]
    progress: int
    streaming_data: List[str]

def streaming_node(state: StreamingState) -> StreamingState:
    """支持流式输出的节点"""
    print(f"📡 开始流式处理... 当前进度: {state['progress']}%")

    # 模拟分块处理
    chunks = ["处理中...", "分析数据...", "生成结果...", "完成!"]
    streaming_data = state.get('streaming_data', [])

    for i, chunk in enumerate(chunks):
        streaming_data.append(chunk)
        progress = int((i + 1) / len(chunks) * 100)

        # 模拟处理时间
        time.sleep(0.5)

        print(f"📦 流式输出: {chunk} ({progress}%)")

    return {
        "messages": state["messages"] + ["流式处理完成"],
        "progress": 100,
        "streaming_data": streaming_data
    }

def create_streaming_graph():
    """创建支持流式处理的图"""
    graph = StateGraph(StreamingState)
    graph.add_node("streaming_node", streaming_node)
    graph.add_edge(START, "streaming_node")
    graph.add_edge("streaming_node", END)
    return graph.compile()

# 使用流式处理
def demonstrate_streaming():
    """演示流式处理"""
    app = create_streaming_graph()
    initial_state = {
        "messages": ["开始"],
        "progress": 0,
        "streaming_data": []
    }

    print("🎬 开始流式执行:")
    for event in app.stream(initial_state):
        print(f"📨 接收事件: {event}")
```

### 2. 异步流式处理

```python
import asyncio
from typing import AsyncIterator

async def async_streaming_node(state: StreamingState) -> StreamingState:
    """异步流式节点"""
    print("🔄 异步流式处理开始...")

    # 模拟异步操作
    tasks = [
        "连接数据库",
        "查询数据",
        "处理数据",
        "生成响应"
    ]

    streaming_data = state.get('streaming_data', [])

    for i, task in enumerate(tasks):
        # 异步等待
        await asyncio.sleep(0.3)

        streaming_data.append(f"✅ {task}")
        progress = int((i + 1) / len(tasks) * 100)

        print(f"⚡ 异步处理: {task} ({progress}%)")

    return {
        "messages": state["messages"] + ["异步处理完成"],
        "progress": 100,
        "streaming_data": streaming_data
    }

async def async_streaming_demo():
    """异步流式处理演示"""
    graph = StateGraph(StreamingState)
    graph.add_node("async_node", async_streaming_node)
    graph.add_edge(START, "async_node")
    graph.add_edge("async_node", END)

    app = graph.compile()

    initial_state = {
        "messages": ["异步开始"],
        "progress": 0,
        "streaming_data": []
    }

    print("⚡ 异步流式执行:")
    async for event in app.astream(initial_state):
        print(f"📬 异步事件: {event}")

# 运行异步演示
# asyncio.run(async_streaming_demo())
```

### 3. 多模式流式处理

```python
from langgraph import StateGraph

def create_multi_mode_streaming():
    """创建多模式流式处理"""
    graph = StateGraph(StreamingState)
    graph.add_node("processor", streaming_node)
    graph.add_edge(START, "processor")
    graph.add_edge("processor", END)

    app = graph.compile()

    return app

def demonstrate_streaming_modes():
    """演示不同的流式模式"""
    app = create_multi_mode_streaming()

    initial_state = {
        "messages": ["多模式测试"],
        "progress": 0,
        "streaming_data": []
    }

    print("📊 模式 1: 默认流式 (updates)")
    for event in app.stream(initial_state, stream_mode="updates"):
        print(f"📈 Update: {event}")

    print("\n📊 模式 2: 值流式 (values)")
    for event in app.stream(initial_state, stream_mode="values"):
        print(f"📊 Value: {event}")

    print("\n📊 模式 3: 调试流式 (debug)")
    for event in app.stream(initial_state, stream_mode="debug"):
        print(f"🐛 Debug: {event}")
```

## ⚡ 并行执行优化

### 1. 节点并行处理

```python
from typing import TypedDict, Annotated
from operator import add
import threading
import time

class ParallelState(TypedDict):
    input_data: str
    results_a: List[str]
    results_b: List[str]
    combined_results: Annotated[List[str], add]  # 使用 reducer 合并结果
    execution_times: dict

def parallel_node_a(state: ParallelState) -> ParallelState:
    """并行节点 A"""
    thread_id = threading.current_thread().ident
    start_time = time.time()

    print(f"🅰️  节点 A 开始执行 (线程: {thread_id})")

    # 模拟耗时操作
    time.sleep(2)

    execution_time = time.time() - start_time
    result = f"A处理结果: {state['input_data']}"

    print(f"✅ 节点 A 完成 (耗时: {execution_time:.2f}s)")

    return {
        "results_a": [result],
        "combined_results": [result],
        "execution_times": {"node_a": execution_time}
    }

def parallel_node_b(state: ParallelState) -> ParallelState:
    """并行节点 B"""
    thread_id = threading.current_thread().ident
    start_time = time.time()

    print(f"🅱️  节点 B 开始执行 (线程: {thread_id})")

    # 模拟不同的耗时操作
    time.sleep(1.5)

    execution_time = time.time() - start_time
    result = f"B处理结果: {state['input_data']}"

    print(f"✅ 节点 B 完成 (耗时: {execution_time:.2f}s)")

    return {
        "results_b": [result],
        "combined_results": [result],
        "execution_times": {"node_b": execution_time}
    }

def create_parallel_graph():
    """创建并行执行图"""
    graph = StateGraph(ParallelState)

    # 添加并行节点
    graph.add_node("node_a", parallel_node_a)
    graph.add_node("node_b", parallel_node_b)

    # 并行启动
    graph.add_edge(START, "node_a")
    graph.add_edge(START, "node_b")

    # 都完成后结束
    graph.add_edge("node_a", END)
    graph.add_edge("node_b", END)

    return graph.compile()

def test_parallel_execution():
    """测试并行执行"""
    app = create_parallel_graph()

    start_time = time.time()

    result = app.invoke({
        "input_data": "测试数据",
        "results_a": [],
        "results_b": [],
        "combined_results": [],
        "execution_times": {}
    })

    total_time = time.time() - start_time

    print(f"\n📊 并行执行结果:")
    print(f"   总执行时间: {total_time:.2f}s")
    print(f"   节点执行时间: {result['execution_times']}")
    print(f"   合并结果: {result['combined_results']}")
    print(f"   并行效率: {sum(result['execution_times'].values()) / total_time:.2f}x")
```

### 2. 条件并行执行

```python
def parallel_processor_node(state: ParallelState) -> ParallelState:
    """根据条件决定并行处理"""
    data_size = len(state.get("input_data", ""))

    if data_size > 100:
        print("📊 数据量大，启用并行处理")
        # 分割数据进行并行处理
        chunks = [state["input_data"][i:i+50] for i in range(0, len(state["input_data"]), 50)]

        # 模拟并行处理多个块
        results = []
        for i, chunk in enumerate(chunks):
            result = f"块{i+1}处理结果: {len(chunk)}字符"
            results.append(result)
            print(f"📦 处理块 {i+1}: {len(chunk)} 字符")

        return {
            **state,
            "combined_results": results,
            "processing_strategy": "parallel"
        }
    else:
        print("📝 数据量小，使用顺序处理")
        result = f"顺序处理结果: {state['input_data']}"

        return {
            **state,
            "combined_results": [result],
            "processing_strategy": "sequential"
        }

def smart_routing_condition(state: ParallelState) -> str:
    """智能路由条件"""
    data_size = len(state.get("input_data", ""))

    if data_size > 200:
        return "heavy_parallel"
    elif data_size > 50:
        return "light_parallel"
    else:
        return "sequential"

def create_adaptive_parallel_graph():
    """创建自适应并行图"""
    graph = StateGraph(ParallelState)

    # 添加不同的处理节点
    graph.add_node("router", parallel_processor_node)
    graph.add_node("heavy_processor", lambda s: {**s, "method": "heavy"})
    graph.add_node("light_processor", lambda s: {**s, "method": "light"})
    graph.add_node("sequential_processor", lambda s: {**s, "method": "sequential"})

    # 设置路由
    graph.add_edge(START, "router")
    graph.add_conditional_edges(
        "router",
        smart_routing_condition,
        {
            "heavy_parallel": "heavy_processor",
            "light_parallel": "light_processor",
            "sequential": "sequential_processor"
        }
    )

    graph.add_edge("heavy_processor", END)
    graph.add_edge("light_processor", END)
    graph.add_edge("sequential_processor", END)

    return graph.compile()
```

## 🧠 内存管理优化

### 1. 状态压缩和清理

```python
from typing import TypedDict, Any
import sys
import gc

class MemoryOptimizedState(TypedDict):
    current_data: Any
    history: List[dict]
    memory_usage: dict
    compressed_history: List[str]

def memory_monitor(func):
    """内存监控装饰器"""
    def wrapper(state):
        # 执行前内存
        gc.collect()
        mem_before = sys.getsizeof(state)

        result = func(state)

        # 执行后内存
        gc.collect()
        mem_after = sys.getsizeof(result)

        # 添加内存使用信息
        if "memory_usage" not in result:
            result["memory_usage"] = {}

        result["memory_usage"][func.__name__] = {
            "before": mem_before,
            "after": mem_after,
            "change": mem_after - mem_before
        }

        print(f"🧠 {func.__name__} 内存变化: {mem_after - mem_before:+d} bytes")
        return result

    return wrapper

@memory_monitor
def memory_heavy_node(state: MemoryOptimizedState) -> MemoryOptimizedState:
    """内存密集型节点"""
    # 生成大量数据
    large_data = ["大数据项" + str(i) for i in range(10000)]

    # 压缩历史记录
    compressed_history = state.get("compressed_history", [])
    if len(state.get("history", [])) > 100:
        # 只保留最近的记录，其余压缩
        recent_history = state["history"][-50:]
        old_summary = f"历史记录摘要: 共{len(state['history'])-50}条记录"
        compressed_history.append(old_summary)
    else:
        recent_history = state.get("history", [])

    return {
        "current_data": large_data[:1000],  # 只保留部分数据
        "history": recent_history + [{"step": "memory_heavy", "data_size": len(large_data)}],
        "memory_usage": state.get("memory_usage", {}),
        "compressed_history": compressed_history
    }

@memory_monitor
def memory_optimized_node(state: MemoryOptimizedState) -> MemoryOptimizedState:
    """内存优化节点"""
    # 使用生成器减少内存占用
    def data_generator():
        for i in range(1000):
            yield f"优化数据{i}"

    # 只保留必要的数据
    current_data = list(data_generator())[:100]  # 只取前100项

    # 清理不需要的历史
    history = state.get("history", [])
    if len(history) > 10:
        # 清理旧的历史记录
        history = history[-5:]  # 只保留最近5条

    return {
        "current_data": current_data,
        "history": history + [{"step": "memory_optimized", "optimized": True}],
        "memory_usage": state.get("memory_usage", {}),
        "compressed_history": state.get("compressed_history", [])
    }

def create_memory_optimized_graph():
    """创建内存优化图"""
    graph = StateGraph(MemoryOptimizedState)

    graph.add_node("heavy_node", memory_heavy_node)
    graph.add_node("optimized_node", memory_optimized_node)

    graph.add_edge(START, "heavy_node")
    graph.add_edge("heavy_node", "optimized_node")
    graph.add_edge("optimized_node", END)

    return graph.compile()
```

### 2. 懒加载和缓存策略

```python
from functools import lru_cache
import weakref
import pickle

class LazyLoader:
    """懒加载器"""

    def __init__(self):
        self._cache = weakref.WeakValueDictionary()
        self._loading_cache = {}

    @lru_cache(maxsize=128)
    def load_expensive_resource(self, resource_id: str):
        """加载昂贵资源（带缓存）"""
        print(f"🔄 加载资源: {resource_id}")

        # 模拟昂贵的加载操作
        time.sleep(1)

        resource = {
            "id": resource_id,
            "data": f"昂贵资源数据_{resource_id}",
            "size": 1024 * 1024  # 1MB
        }

        return resource

    def get_resource(self, resource_id: str):
        """获取资源（带弱引用缓存）"""
        # 检查弱引用缓存
        if resource_id in self._cache:
            print(f"💾 从弱引用缓存获取: {resource_id}")
            return self._cache[resource_id]

        # 加载新资源
        resource = self.load_expensive_resource(resource_id)
        self._cache[resource_id] = resource

        return resource

# 全局懒加载器
lazy_loader = LazyLoader()

def lazy_loading_node(state: dict) -> dict:
    """懒加载节点"""
    resource_ids = state.get("required_resources", [])
    loaded_resources = {}

    for resource_id in resource_ids:
        # 只在需要时加载
        if state.get("load_immediately", False):
            resource = lazy_loader.get_resource(resource_id)
            loaded_resources[resource_id] = resource
        else:
            # 延迟加载 - 只存储引用
            loaded_resources[resource_id] = {"lazy_ref": resource_id}

    return {
        **state,
        "loaded_resources": loaded_resources,
        "load_strategy": "lazy" if not state.get("load_immediately") else "immediate"
    }

def caching_node(state: dict) -> dict:
    """缓存节点"""
    cache_key = str(hash(str(state.get("input_data", ""))))

    # 检查缓存
    if hasattr(caching_node, "_cache"):
        if cache_key in caching_node._cache:
            print(f"📦 从缓存获取结果: {cache_key[:8]}...")
            return {
                **state,
                "result": caching_node._cache[cache_key],
                "from_cache": True
            }

    # 计算新结果
    print(f"⚙️  计算新结果: {cache_key[:8]}...")
    time.sleep(0.5)  # 模拟计算时间

    result = f"计算结果_{cache_key[:8]}"

    # 缓存结果
    if not hasattr(caching_node, "_cache"):
        caching_node._cache = {}

    caching_node._cache[cache_key] = result

    # 限制缓存大小
    if len(caching_node._cache) > 100:
        # 删除最旧的缓存项
        oldest_key = next(iter(caching_node._cache))
        del caching_node._cache[oldest_key]

    return {
        **state,
        "result": result,
        "from_cache": False
    }
```

## 🔧 系统级优化

### 1. 连接池和资源池

```python
import threading
from queue import Queue
from contextlib import contextmanager

class ConnectionPool:
    """连接池实现"""

    def __init__(self, create_connection_func, max_connections=10):
        self.create_connection = create_connection_func
        self.max_connections = max_connections
        self._pool = Queue(maxsize=max_connections)
        self._lock = threading.Lock()
        self._created_connections = 0

    @contextmanager
    def get_connection(self):
        """获取连接（上下文管理器）"""
        connection = self._get_connection()
        try:
            yield connection
        finally:
            self._return_connection(connection)

    def _get_connection(self):
        """获取连接"""
        try:
            # 尝试从池中获取
            return self._pool.get_nowait()
        except:
            # 池为空，创建新连接
            with self._lock:
                if self._created_connections < self.max_connections:
                    connection = self.create_connection()
                    self._created_connections += 1
                    print(f"🔌 创建新连接 ({self._created_connections}/{self.max_connections})")
                    return connection
                else:
                    # 等待可用连接
                    print("⏳ 等待可用连接...")
                    return self._pool.get()

    def _return_connection(self, connection):
        """归还连接"""
        try:
            self._pool.put_nowait(connection)
        except:
            print("⚠️  连接池已满，丢弃连接")

# 模拟数据库连接
def create_db_connection():
    """创建数据库连接"""
    time.sleep(0.1)  # 模拟连接时间
    return {"connection_id": time.time(), "status": "connected"}

# 全局连接池
db_pool = ConnectionPool(create_db_connection, max_connections=5)

def database_node(state: dict) -> dict:
    """使用连接池的数据库节点"""
    with db_pool.get_connection() as connection:
        print(f"💾 使用连接: {connection['connection_id']}")

        # 模拟数据库操作
        time.sleep(0.2)

        query_result = f"查询结果_{connection['connection_id']}"

        return {
            **state,
            "db_result": query_result,
            "connection_id": connection['connection_id']
        }
```

### 2. 批量处理优化

```python
from typing import List
import threading
from collections import defaultdict

class BatchProcessor:
    """批量处理器"""

    def __init__(self, batch_size=10, max_wait_time=1.0):
        self.batch_size = batch_size
        self.max_wait_time = max_wait_time
        self._batches = defaultdict(list)
        self._timers = {}
        self._lock = threading.Lock()

    def add_to_batch(self, batch_key: str, item: dict, callback):
        """添加项到批次"""
        with self._lock:
            self._batches[batch_key].append((item, callback))

            # 检查是否达到批次大小
            if len(self._batches[batch_key]) >= self.batch_size:
                self._process_batch(batch_key)
            else:
                # 设置定时器
                if batch_key not in self._timers:
                    timer = threading.Timer(self.max_wait_time, self._process_batch, [batch_key])
                    timer.start()
                    self._timers[batch_key] = timer

    def _process_batch(self, batch_key: str):
        """处理批次"""
        with self._lock:
            if batch_key not in self._batches:
                return

            batch = self._batches.pop(batch_key)

            # 清理定时器
            if batch_key in self._timers:
                self._timers[batch_key].cancel()
                del self._timers[batch_key]

        if not batch:
            return

        print(f"📦 处理批次 {batch_key}: {len(batch)} 项")

        # 批量处理
        items = [item for item, _ in batch]
        callbacks = [callback for _, callback in batch]

        # 模拟批量操作
        results = self._batch_operation(items)

        # 调用回调
        for result, callback in zip(results, callbacks):
            callback(result)

    def _batch_operation(self, items: List[dict]) -> List[dict]:
        """批量操作"""
        print(f"⚙️  批量处理 {len(items)} 项...")
        time.sleep(0.1 * len(items))  # 模拟批量处理时间

        return [{"result": f"批量处理_{item.get('id', 'unknown')}"} for item in items]

# 全局批量处理器
batch_processor = BatchProcessor(batch_size=5, max_wait_time=2.0)

def batch_processing_node(state: dict) -> dict:
    """支持批量处理的节点"""
    result = {"pending": True}

    def result_callback(batch_result):
        """结果回调"""
        result.update(batch_result)
        result["pending"] = False

    # 添加到批次
    batch_processor.add_to_batch(
        "default_batch",
        {"id": state.get("id", "unknown"), "data": state.get("data")},
        result_callback
    )

    # 等待批量处理完成
    while result.get("pending", True):
        time.sleep(0.1)

    return {
        **state,
        "batch_result": result.get("result"),
        "processing_method": "batch"
    }
```

## 📊 性能监控和分析

### 1. 性能分析器

```python
import cProfile
import pstats
import io
from functools import wraps

class PerformanceProfiler:
    """性能分析器"""

    def __init__(self):
        self.profiles = {}

    def profile_node(self, node_name: str):
        """节点性能分析装饰器"""
        def decorator(func):
            @wraps(func)
            def wrapper(state):
                profiler = cProfile.Profile()
                profiler.enable()

                try:
                    result = func(state)
                finally:
                    profiler.disable()

                # 保存分析结果
                self.profiles[node_name] = profiler

                return result
            return wrapper
        return decorator

    def get_stats(self, node_name: str, sort_by='cumulative'):
        """获取性能统计"""
        if node_name not in self.profiles:
            return None

        s = io.StringIO()
        ps = pstats.Stats(self.profiles[node_name], stream=s)
        ps.sort_stats(sort_by)
        ps.print_stats()

        return s.getvalue()

    def print_all_stats(self):
        """打印所有节点的性能统计"""
        for node_name in self.profiles:
            print(f"\n📊 {node_name} 性能分析:")
            print(self.get_stats(node_name))

# 全局性能分析器
perf_profiler = PerformanceProfiler()

@perf_profiler.profile_node("cpu_intensive_node")
def cpu_intensive_node(state: dict) -> dict:
    """CPU密集型节点"""
    # 模拟CPU密集型操作
    result = 0
    for i in range(1000000):
        result += i * i

    return {
        **state,
        "cpu_result": result,
        "iterations": 1000000
    }

@perf_profiler.profile_node("optimized_cpu_node")
def optimized_cpu_node(state: dict) -> dict:
    """优化后的CPU密集型节点"""
    # 使用更高效的算法
    n = 1000000
    result = n * (n - 1) * (2 * n - 1) // 6  # 平方和公式

    return {
        **state,
        "cpu_result": result,
        "iterations": n,
        "optimized": True
    }
```

### 2. 实时性能监控

```python
import psutil
import time
from threading import Thread
from dataclasses import dataclass
from typing import List

@dataclass
class PerformanceMetrics:
    timestamp: float
    cpu_percent: float
    memory_percent: float
    memory_mb: float
    node_name: str = None

class RealTimeMonitor:
    """实时性能监控器"""

    def __init__(self, interval=1.0):
        self.interval = interval
        self.metrics: List[PerformanceMetrics] = []
        self.monitoring = False
        self.monitor_thread = None

    def start_monitoring(self):
        """开始监控"""
        if self.monitoring:
            return

        self.monitoring = True
        self.monitor_thread = Thread(target=self._monitor_loop)
        self.monitor_thread.start()
        print("📈 开始实时性能监控")

    def stop_monitoring(self):
        """停止监控"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join()
        print("📉 停止性能监控")

    def _monitor_loop(self):
        """监控循环"""
        while self.monitoring:
            metrics = PerformanceMetrics(
                timestamp=time.time(),
                cpu_percent=psutil.cpu_percent(),
                memory_percent=psutil.virtual_memory().percent,
                memory_mb=psutil.virtual_memory().used / 1024 / 1024
            )

            self.metrics.append(metrics)

            # 只保留最近的1000个数据点
            if len(self.metrics) > 1000:
                self.metrics = self.metrics[-1000:]

            time.sleep(self.interval)

    def add_node_marker(self, node_name: str):
        """添加节点标记"""
        if self.metrics:
            self.metrics[-1].node_name = node_name

    def get_summary(self) -> dict:
        """获取性能摘要"""
        if not self.metrics:
            return {}

        cpu_values = [m.cpu_percent for m in self.metrics]
        memory_values = [m.memory_mb for m in self.metrics]

        return {
            "duration": self.metrics[-1].timestamp - self.metrics[0].timestamp,
            "samples": len(self.metrics),
            "cpu": {
                "avg": sum(cpu_values) / len(cpu_values),
                "max": max(cpu_values),
                "min": min(cpu_values)
            },
            "memory": {
                "avg": sum(memory_values) / len(memory_values),
                "max": max(memory_values),
                "min": min(memory_values),
                "peak_mb": max(memory_values)
            }
        }

# 全局监控器
rt_monitor = RealTimeMonitor(interval=0.5)

def monitored_node(node_name: str):
    """监控节点装饰器"""
    def decorator(func):
        @wraps(func)
        def wrapper(state):
            rt_monitor.add_node_marker(f"{node_name}_start")

            start_time = time.time()
            result = func(state)
            execution_time = time.time() - start_time

            rt_monitor.add_node_marker(f"{node_name}_end")

            # 添加执行时间到结果
            if "performance" not in result:
                result["performance"] = {}
            result["performance"][node_name] = execution_time

            return result
        return wrapper
    return decorator

@monitored_node("example_heavy_node")
def example_heavy_node(state: dict) -> dict:
    """示例重型节点"""
    # 模拟重型操作
    time.sleep(2)
    return {
        **state,
        "heavy_result": "完成重型操作"
    }
```

## 🎯 性能优化最佳实践

### 1. 性能优化检查清单

```python
def performance_optimization_checklist():
    """性能优化检查清单"""
    checklist = [
        "✅ 使用流式处理提升用户体验",
        "✅ 识别并行执行机会",
        "✅ 实现状态压缩和清理",
        "✅ 使用懒加载减少内存占用",
        "✅ 实现缓存策略",
        "✅ 使用连接池管理资源",
        "✅ 考虑批量处理优化",
        "✅ 监控关键性能指标",
        "✅ 定期进行性能分析",
        "✅ 设置性能告警阈值"
    ]

    print("🚀 性能优化检查清单:")
    for item in checklist:
        print(f"  {item}")

performance_optimization_checklist()
```

### 2. 性能测试框架

```python
import unittest
import time
from typing import Callable

class PerformanceTestCase(unittest.TestCase):
    """性能测试用例基类"""

    def setUp(self):
        self.start_time = time.time()
        rt_monitor.start_monitoring()

    def tearDown(self):
        rt_monitor.stop_monitoring()
        execution_time = time.time() - self.start_time
        print(f"📊 测试执行时间: {execution_time:.2f}s")
        print(f"📈 性能摘要: {rt_monitor.get_summary()}")

    def assertExecutionTime(self, func: Callable, max_time: float, *args, **kwargs):
        """断言执行时间"""
        start = time.time()
        result = func(*args, **kwargs)
        execution_time = time.time() - start

        self.assertLessEqual(
            execution_time,
            max_time,
            f"执行时间 {execution_time:.2f}s 超过预期 {max_time}s"
        )

        return result

    def assertMemoryUsage(self, func: Callable, max_memory_mb: float, *args, **kwargs):
        """断言内存使用"""
        process = psutil.Process()
        mem_before = process.memory_info().rss / 1024 / 1024

        result = func(*args, **kwargs)

        mem_after = process.memory_info().rss / 1024 / 1024
        memory_increase = mem_after - mem_before

        self.assertLessEqual(
            memory_increase,
            max_memory_mb,
            f"内存增长 {memory_increase:.2f}MB 超过预期 {max_memory_mb}MB"
        )

        return result

class GraphPerformanceTests(PerformanceTestCase):
    """图性能测试"""

    def test_streaming_performance(self):
        """测试流式处理性能"""
        app = create_streaming_graph()

        def run_streaming_test():
            return app.invoke({
                "messages": ["性能测试"],
                "progress": 0,
                "streaming_data": []
            })

        # 断言执行时间不超过3秒
        result = self.assertExecutionTime(run_streaming_test, 3.0)
        self.assertEqual(result["progress"], 100)

    def test_parallel_performance(self):
        """测试并行处理性能"""
        app = create_parallel_graph()

        def run_parallel_test():
            return app.invoke({
                "input_data": "性能测试数据",
                "results_a": [],
                "results_b": [],
                "combined_results": [],
                "execution_times": {}
            })

        # 并行执行应该比顺序执行快
        result = self.assertExecutionTime(run_parallel_test, 2.5)
        self.assertIn("node_a", result["execution_times"])
        self.assertIn("node_b", result["execution_times"])

# 运行性能测试
# if __name__ == "__main__":
#     unittest.main()
```

## 📚 延伸阅读

- [LangGraph 流式处理文档](https://langchain-ai.github.io/langgraph/concepts/streaming/)
- [Python 性能优化指南](https://docs.python.org/3/howto/perf_profiling.html)
- [异步编程最佳实践](https://docs.python.org/3/library/asyncio.html)

---

💡 **小贴士**：性能优化是一个持续的过程。从用户体验出发，识别瓶颈，有针对性地优化，并持续监控效果！