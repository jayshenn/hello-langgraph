# 04-ç›‘æ§ä¸è¿½è¸ª

## æ¦‚è¿°

ç›‘æ§ä¸è¿½è¸ªæ˜¯ç”Ÿäº§ç¯å¢ƒä¸­ LangGraph åº”ç”¨çš„é‡è¦ä¿éšœã€‚é€šè¿‡ LangSmith é›†æˆã€è‡ªå®šä¹‰æŒ‡æ ‡ã€æ—¥å¿—ç³»ç»Ÿå’Œæ€§èƒ½ç›‘æ§ï¼Œå®ç°åº”ç”¨çš„å¯è§‚æµ‹æ€§ã€æ•…éšœè¯Šæ–­å’Œæ€§èƒ½ä¼˜åŒ–ã€‚

## ç›‘æ§ä½“ç³»æ¶æ„

### ä¸‰å±‚ç›‘æ§æ¨¡å‹
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ä¸šåŠ¡ç›‘æ§      â”‚ â† Agent æ€§èƒ½ã€ç”¨æˆ·ä½“éªŒ
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   åº”ç”¨ç›‘æ§      â”‚ â† è¯·æ±‚é‡ã€å“åº”æ—¶é—´ã€é”™è¯¯ç‡
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   åŸºç¡€è®¾æ–½ç›‘æ§   â”‚ â† CPUã€å†…å­˜ã€ç£ç›˜ã€ç½‘ç»œ
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒç»„ä»¶
- **LangSmith**: å®˜æ–¹ Agent è¿½è¸ªå¹³å°
- **Prometheus**: æŒ‡æ ‡æ”¶é›†å’Œå­˜å‚¨
- **Grafana**: å¯è§†åŒ–ä»ªè¡¨æ¿
- **ELK Stack**: æ—¥å¿—åˆ†æ
- **Jaeger**: åˆ†å¸ƒå¼è¿½è¸ª
- **Sentry**: é”™è¯¯ç›‘æ§

## LangSmith é›†æˆ

### åŸºç¡€é…ç½®
```python
# ç¯å¢ƒå˜é‡é…ç½®
import os

os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = "your_langsmith_api_key"
os.environ["LANGCHAIN_PROJECT"] = "my-agent-production"
os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"
```

### å›¾çº§åˆ«è¿½è¸ª
```python
from langgraph.graph import StateGraph
from langchain_core.tracers import LangChainTracer

def create_traced_graph():
    graph = StateGraph(AgentState)

    # æ·»åŠ èŠ‚ç‚¹
    graph.add_node("input_processor", process_input)
    graph.add_node("llm_call", call_llm)
    graph.add_node("output_formatter", format_output)

    # è®¾ç½®æµç¨‹
    graph.set_entry_point("input_processor")
    graph.add_edge("input_processor", "llm_call")
    graph.add_edge("llm_call", "output_formatter")
    graph.set_finish_point("output_formatter")

    return graph.compile()

# ä½¿ç”¨è¿½è¸ª
app = create_traced_graph()
result = app.invoke(
    {"input": "ç”¨æˆ·é—®é¢˜"},
    config={
        "configurable": {
            "thread_id": "user_123",
        },
        "tags": ["production", "v1.0"],
        "metadata": {
            "user_id": "user_123",
            "session_id": "session_456",
            "environment": "production"
        }
    }
)
```

### è‡ªå®šä¹‰è¿½è¸ª
```python
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.messages import BaseMessage
from typing import Any, Dict, List

class CustomTracer(BaseCallbackHandler):
    """è‡ªå®šä¹‰è¿½è¸ªå™¨"""

    def on_chain_start(
        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs
    ) -> None:
        """é“¾å¼€å§‹æ—¶çš„å›è°ƒ"""
        print(f"å¼€å§‹æ‰§è¡Œé“¾: {serialized.get('name', 'Unknown')}")
        print(f"è¾“å…¥å‚æ•°: {inputs}")

    def on_chain_end(self, outputs: Dict[str, Any], **kwargs) -> None:
        """é“¾ç»“æŸæ—¶çš„å›è°ƒ"""
        print(f"é“¾æ‰§è¡Œå®Œæˆï¼Œè¾“å‡º: {outputs}")

    def on_chain_error(self, error: Exception, **kwargs) -> None:
        """é“¾æ‰§è¡Œé”™è¯¯æ—¶çš„å›è°ƒ"""
        print(f"é“¾æ‰§è¡Œé”™è¯¯: {error}")

    def on_llm_start(
        self, serialized: Dict[str, Any], prompts: List[str], **kwargs
    ) -> None:
        """LLM è°ƒç”¨å¼€å§‹"""
        print(f"å¼€å§‹ LLM è°ƒç”¨: {serialized.get('name', 'Unknown')}")
        print(f"æç¤ºè¯: {prompts[0][:100]}...")

    def on_llm_end(self, response: Any, **kwargs) -> None:
        """LLM è°ƒç”¨ç»“æŸ"""
        print(f"LLM å“åº”å®Œæˆ")

    def on_tool_start(
        self, serialized: Dict[str, Any], input_str: str, **kwargs
    ) -> None:
        """å·¥å…·è°ƒç”¨å¼€å§‹"""
        print(f"å¼€å§‹è°ƒç”¨å·¥å…·: {serialized.get('name', 'Unknown')}")
        print(f"è¾“å…¥: {input_str}")

# ä½¿ç”¨è‡ªå®šä¹‰è¿½è¸ªå™¨
app = create_traced_graph()
result = app.invoke(
    {"input": "æµ‹è¯•é—®é¢˜"},
    config={"callbacks": [CustomTracer()]}
)
```

### æ‰¹é‡è¿½è¸ªåˆ†æ
```python
from langsmith import Client

client = Client()

# æŸ¥è¯¢è¿½è¸ªè®°å½•
runs = client.list_runs(
    project_name="my-agent-production",
    start_time=datetime.now() - timedelta(hours=24),
    end_time=datetime.now()
)

# åˆ†ææ€§èƒ½æŒ‡æ ‡
total_runs = len(list(runs))
successful_runs = len([r for r in runs if not r.error])
success_rate = successful_runs / total_runs * 100

print(f"24å°æ—¶å†…æ€»è¿è¡Œæ¬¡æ•°: {total_runs}")
print(f"æˆåŠŸç‡: {success_rate:.2f}%")

# åˆ†æå“åº”æ—¶é—´
response_times = [
    (r.end_time - r.start_time).total_seconds()
    for r in runs if r.end_time and r.start_time
]

if response_times:
    avg_time = sum(response_times) / len(response_times)
    print(f"å¹³å‡å“åº”æ—¶é—´: {avg_time:.2f}ç§’")
```

## Prometheus æŒ‡æ ‡æ”¶é›†

### åº”ç”¨æŒ‡æ ‡å®šä¹‰
```python
from prometheus_client import Counter, Histogram, Gauge, generate_latest, CONTENT_TYPE_LATEST
import time
from functools import wraps

# å®šä¹‰æŒ‡æ ‡
REQUEST_COUNT = Counter(
    'langgraph_requests_total',
    'Total number of requests',
    ['method', 'endpoint', 'status']
)

REQUEST_DURATION = Histogram(
    'langgraph_request_duration_seconds',
    'Request duration in seconds',
    ['method', 'endpoint']
)

ACTIVE_CONNECTIONS = Gauge(
    'langgraph_active_connections',
    'Number of active connections'
)

GRAPH_EXECUTION_TIME = Histogram(
    'langgraph_graph_execution_seconds',
    'Graph execution time in seconds',
    ['graph_name', 'node_name']
)

LLM_TOKEN_USAGE = Counter(
    'langgraph_llm_tokens_total',
    'Total LLM tokens used',
    ['model', 'type']  # type: prompt_tokens, completion_tokens
)

ERROR_COUNT = Counter(
    'langgraph_errors_total',
    'Total number of errors',
    ['error_type', 'node_name']
)

def track_metrics(func):
    """è£…é¥°å™¨ï¼šè¿½è¸ªå‡½æ•°æ‰§è¡ŒæŒ‡æ ‡"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()

        try:
            result = func(*args, **kwargs)
            REQUEST_COUNT.labels(
                method='POST',
                endpoint=func.__name__,
                status='success'
            ).inc()
            return result
        except Exception as e:
            REQUEST_COUNT.labels(
                method='POST',
                endpoint=func.__name__,
                status='error'
            ).inc()
            ERROR_COUNT.labels(
                error_type=type(e).__name__,
                node_name=func.__name__
            ).inc()
            raise
        finally:
            duration = time.time() - start_time
            REQUEST_DURATION.labels(
                method='POST',
                endpoint=func.__name__
            ).observe(duration)

    return wrapper

@track_metrics
def process_input_node(state):
    """å¤„ç†è¾“å…¥èŠ‚ç‚¹"""
    GRAPH_EXECUTION_TIME.labels(
        graph_name='main_graph',
        node_name='process_input'
    ).time()

    # èŠ‚ç‚¹é€»è¾‘
    return {"processed_input": state["input"]}
```

### FastAPI é›†æˆ
```python
from fastapi import FastAPI, Response
from prometheus_client import generate_latest, CONTENT_TYPE_LATEST

app = FastAPI()

@app.middleware("http")
async def track_requests(request, call_next):
    """è¯·æ±‚è¿½è¸ªä¸­é—´ä»¶"""
    start_time = time.time()
    ACTIVE_CONNECTIONS.inc()

    try:
        response = await call_next(request)
        REQUEST_COUNT.labels(
            method=request.method,
            endpoint=request.url.path,
            status=response.status_code
        ).inc()
        return response
    finally:
        duration = time.time() - start_time
        REQUEST_DURATION.labels(
            method=request.method,
            endpoint=request.url.path
        ).observe(duration)
        ACTIVE_CONNECTIONS.dec()

@app.get("/metrics")
async def metrics():
    """Prometheus æŒ‡æ ‡ç«¯ç‚¹"""
    return Response(
        generate_latest(),
        media_type=CONTENT_TYPE_LATEST
    )

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "healthy", "timestamp": time.time()}
```

### LLM è°ƒç”¨è¿½è¸ª
```python
from langchain_openai import ChatOpenAI
from langchain_core.callbacks import BaseCallbackHandler

class TokenTracker(BaseCallbackHandler):
    """Token ä½¿ç”¨è¿½è¸ª"""

    def on_llm_end(self, response, **kwargs):
        if hasattr(response, 'llm_output') and response.llm_output:
            token_usage = response.llm_output.get('token_usage', {})
            model = response.llm_output.get('model_name', 'unknown')

            prompt_tokens = token_usage.get('prompt_tokens', 0)
            completion_tokens = token_usage.get('completion_tokens', 0)

            LLM_TOKEN_USAGE.labels(
                model=model,
                type='prompt_tokens'
            ).inc(prompt_tokens)

            LLM_TOKEN_USAGE.labels(
                model=model,
                type='completion_tokens'
            ).inc(completion_tokens)

# ä½¿ç”¨ Token è¿½è¸ª
llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    callbacks=[TokenTracker()]
)
```

## Grafana ä»ªè¡¨æ¿

### ä»ªè¡¨æ¿é…ç½®
```json
{
  "dashboard": {
    "id": null,
    "title": "LangGraph ç›‘æ§ä»ªè¡¨æ¿",
    "tags": ["langgraph", "production"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "è¯·æ±‚ç‡",
        "type": "stat",
        "targets": [
          {
            "expr": "rate(langgraph_requests_total[5m])",
            "legendFormat": "è¯·æ±‚/ç§’"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "reqps"
          }
        }
      },
      {
        "id": 2,
        "title": "æˆåŠŸç‡",
        "type": "stat",
        "targets": [
          {
            "expr": "rate(langgraph_requests_total{status=\"success\"}[5m]) / rate(langgraph_requests_total[5m]) * 100",
            "legendFormat": "æˆåŠŸç‡"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percent",
            "min": 0,
            "max": 100
          }
        }
      },
      {
        "id": 3,
        "title": "å“åº”æ—¶é—´åˆ†å¸ƒ",
        "type": "heatmap",
        "targets": [
          {
            "expr": "increase(langgraph_request_duration_seconds_bucket[5m])",
            "format": "heatmap",
            "legendFormat": "{{le}}"
          }
        ]
      },
      {
        "id": 4,
        "title": "é”™è¯¯è¶‹åŠ¿",
        "type": "timeseries",
        "targets": [
          {
            "expr": "rate(langgraph_errors_total[5m])",
            "legendFormat": "{{error_type}}"
          }
        ]
      },
      {
        "id": 5,
        "title": "Token ä½¿ç”¨é‡",
        "type": "timeseries",
        "targets": [
          {
            "expr": "rate(langgraph_llm_tokens_total[5m])",
            "legendFormat": "{{model}} - {{type}}"
          }
        ]
      }
    ],
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "refresh": "30s"
  }
}
```

### æŠ¥è­¦è§„åˆ™
```yaml
# alerting-rules.yml
groups:
  - name: langgraph-alerts
    rules:
      - alert: HighErrorRate
        expr: rate(langgraph_requests_total{status="error"}[5m]) / rate(langgraph_requests_total[5m]) > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "LangGraph é”™è¯¯ç‡è¿‡é«˜"
          description: "é”™è¯¯ç‡åœ¨è¿‡å»5åˆ†é’Ÿå†…è¶…è¿‡10%"

      - alert: HighResponseTime
        expr: histogram_quantile(0.95, rate(langgraph_request_duration_seconds_bucket[5m])) > 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "LangGraph å“åº”æ—¶é—´è¿‡é•¿"
          description: "95åˆ†ä½å“åº”æ—¶é—´è¶…è¿‡10ç§’"

      - alert: ServiceDown
        expr: up{job="langgraph"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "LangGraph æœåŠ¡ä¸å¯ç”¨"
          description: "LangGraph æœåŠ¡å·²åœæ­¢å“åº”"

      - alert: HighTokenUsage
        expr: rate(langgraph_llm_tokens_total[1h]) > 100000
        for: 10m
        labels:
          severity: info
        annotations:
          summary: "Token ä½¿ç”¨é‡è¾ƒé«˜"
          description: "æ¯å°æ—¶ Token ä½¿ç”¨é‡è¶…è¿‡10ä¸‡"
```

## æ—¥å¿—ç®¡ç†

### ç»“æ„åŒ–æ—¥å¿—
```python
import logging
import json
from datetime import datetime
from typing import Any, Dict

class StructuredLogger:
    """ç»“æ„åŒ–æ—¥å¿—å™¨"""

    def __init__(self, name: str):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(logging.INFO)

        # åˆ›å»ºæ ¼å¼åŒ–å™¨
        formatter = logging.Formatter('%(message)s')

        # åˆ›å»ºå¤„ç†å™¨
        handler = logging.StreamHandler()
        handler.setFormatter(formatter)

        self.logger.addHandler(handler)

    def _log(self, level: str, message: str, **kwargs):
        """ç»Ÿä¸€æ—¥å¿—æ ¼å¼"""
        log_data = {
            "timestamp": datetime.utcnow().isoformat(),
            "level": level,
            "message": message,
            "service": "langgraph",
            **kwargs
        }

        log_line = json.dumps(log_data, ensure_ascii=False)
        getattr(self.logger, level.lower())(log_line)

    def info(self, message: str, **kwargs):
        self._log("INFO", message, **kwargs)

    def warning(self, message: str, **kwargs):
        self._log("WARNING", message, **kwargs)

    def error(self, message: str, **kwargs):
        self._log("ERROR", message, **kwargs)

    def debug(self, message: str, **kwargs):
        self._log("DEBUG", message, **kwargs)

# ä½¿ç”¨ç»“æ„åŒ–æ—¥å¿—
logger = StructuredLogger("langgraph.agent")

def process_request(user_input: str, user_id: str):
    """å¤„ç†ç”¨æˆ·è¯·æ±‚"""
    logger.info(
        "å¼€å§‹å¤„ç†ç”¨æˆ·è¯·æ±‚",
        user_id=user_id,
        input_length=len(user_input),
        request_id="req_123"
    )

    try:
        # å¤„ç†é€»è¾‘
        result = "å¤„ç†ç»“æœ"

        logger.info(
            "è¯·æ±‚å¤„ç†æˆåŠŸ",
            user_id=user_id,
            request_id="req_123",
            output_length=len(result)
        )

        return result

    except Exception as e:
        logger.error(
            "è¯·æ±‚å¤„ç†å¤±è´¥",
            user_id=user_id,
            request_id="req_123",
            error_type=type(e).__name__,
            error_message=str(e)
        )
        raise
```

### èŠ‚ç‚¹çº§æ—¥å¿—
```python
def logged_node(node_name: str):
    """èŠ‚ç‚¹æ—¥å¿—è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        def wrapper(state):
            request_id = state.get("request_id", "unknown")

            logger.info(
                f"èŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ",
                node_name=node_name,
                request_id=request_id,
                input_keys=list(state.keys())
            )

            start_time = time.time()

            try:
                result = func(state)
                duration = time.time() - start_time

                logger.info(
                    f"èŠ‚ç‚¹æ‰§è¡ŒæˆåŠŸ",
                    node_name=node_name,
                    request_id=request_id,
                    duration=duration,
                    output_keys=list(result.keys())
                )

                return result

            except Exception as e:
                duration = time.time() - start_time

                logger.error(
                    f"èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥",
                    node_name=node_name,
                    request_id=request_id,
                    duration=duration,
                    error_type=type(e).__name__,
                    error_message=str(e)
                )
                raise

        return wrapper
    return decorator

@logged_node("input_processor")
def process_input(state):
    """å¤„ç†è¾“å…¥"""
    return {"processed_input": state["input"].strip()}

@logged_node("llm_caller")
def call_llm(state):
    """è°ƒç”¨ LLM"""
    # LLM è°ƒç”¨é€»è¾‘
    return {"llm_response": "LLM å“åº”"}
```

### ELK Stack é…ç½®

#### Logstash é…ç½®
```ruby
# logstash.conf
input {
  beats {
    port => 5044
  }
}

filter {
  if [fields][service] == "langgraph" {
    json {
      source => "message"
    }

    date {
      match => [ "timestamp", "ISO8601" ]
    }

    mutate {
      add_field => { "service" => "langgraph" }
    }
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "langgraph-logs-%{+YYYY.MM.dd}"
  }
}
```

#### Filebeat é…ç½®
```yaml
# filebeat.yml
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /var/log/langgraph/*.log
  fields:
    service: langgraph
  fields_under_root: true
  json.keys_under_root: true
  json.add_error_key: true

output.logstash:
  hosts: ["logstash:5044"]

logging.level: info
```

## åˆ†å¸ƒå¼è¿½è¸ª

### Jaeger é›†æˆ
```python
from opentelemetry import trace
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.instrumentation.requests import RequestsInstrumentor
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor

# é…ç½® Jaeger
jaeger_exporter = JaegerExporter(
    agent_host_name="jaeger",
    agent_port=6831,
)

trace.set_tracer_provider(TracerProvider())
tracer = trace.get_tracer(__name__)

span_processor = BatchSpanProcessor(jaeger_exporter)
trace.get_tracer_provider().add_span_processor(span_processor)

# è‡ªåŠ¨è¿½è¸ª HTTP è¯·æ±‚
RequestsInstrumentor().instrument()

def traced_node(node_name: str):
    """åˆ†å¸ƒå¼è¿½è¸ªè£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        def wrapper(state):
            with tracer.start_as_current_span(node_name) as span:
                # æ·»åŠ  span å±æ€§
                span.set_attribute("node.name", node_name)
                span.set_attribute("state.keys", ",".join(state.keys()))

                try:
                    result = func(state)
                    span.set_attribute("node.status", "success")
                    span.set_attribute("output.keys", ",".join(result.keys()))
                    return result
                except Exception as e:
                    span.set_attribute("node.status", "error")
                    span.set_attribute("error.type", type(e).__name__)
                    span.set_attribute("error.message", str(e))
                    raise

        return wrapper
    return decorator

@traced_node("input_processor")
def process_input(state):
    """å¤„ç†è¾“å…¥ï¼ˆå¸¦è¿½è¸ªï¼‰"""
    return {"processed_input": state["input"]}
```

## é”™è¯¯ç›‘æ§

### Sentry é›†æˆ
```python
import sentry_sdk
from sentry_sdk.integrations.logging import LoggingIntegration

# é…ç½® Sentry
sentry_logging = LoggingIntegration(
    level=logging.INFO,        # æ•è· info åŠä»¥ä¸Šçº§åˆ«
    event_level=logging.ERROR  # å‘é€ error åŠä»¥ä¸Šçº§åˆ«åˆ° Sentry
)

sentry_sdk.init(
    dsn="your_sentry_dsn",
    integrations=[sentry_logging],
    traces_sample_rate=1.0,
    environment="production"
)

def monitored_node(func):
    """é”™è¯¯ç›‘æ§è£…é¥°å™¨"""
    @wraps(func)
    def wrapper(state):
        try:
            return func(state)
        except Exception as e:
            # æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
            sentry_sdk.set_context("state", {
                "keys": list(state.keys()),
                "request_id": state.get("request_id")
            })

            sentry_sdk.set_tag("node_name", func.__name__)
            sentry_sdk.capture_exception(e)
            raise

    return wrapper

@monitored_node
def risky_node(state):
    """å¯èƒ½å‡ºé”™çš„èŠ‚ç‚¹"""
    if not state.get("input"):
        raise ValueError("è¾“å…¥ä¸èƒ½ä¸ºç©º")

    return {"output": "å¤„ç†ç»“æœ"}
```

## æ€§èƒ½ç›‘æ§

### èµ„æºä½¿ç”¨ç›‘æ§
```python
import psutil
import threading
import time

class ResourceMonitor:
    """èµ„æºä½¿ç”¨ç›‘æ§"""

    def __init__(self):
        self.running = False
        self.thread = None

    def start(self):
        """å¼€å§‹ç›‘æ§"""
        self.running = True
        self.thread = threading.Thread(target=self._monitor_loop)
        self.thread.start()

    def stop(self):
        """åœæ­¢ç›‘æ§"""
        self.running = False
        if self.thread:
            self.thread.join()

    def _monitor_loop(self):
        """ç›‘æ§å¾ªç¯"""
        while self.running:
            # CPU ä½¿ç”¨ç‡
            cpu_percent = psutil.cpu_percent(interval=1)

            # å†…å­˜ä½¿ç”¨
            memory = psutil.virtual_memory()
            memory_percent = memory.percent
            memory_available = memory.available / 1024 / 1024  # MB

            # ç£ç›˜ä½¿ç”¨
            disk = psutil.disk_usage('/')
            disk_percent = disk.percent

            # æ›´æ–° Prometheus æŒ‡æ ‡
            CPU_USAGE.set(cpu_percent)
            MEMORY_USAGE.set(memory_percent)
            MEMORY_AVAILABLE.set(memory_available)
            DISK_USAGE.set(disk_percent)

            logger.debug(
                "ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ",
                cpu_percent=cpu_percent,
                memory_percent=memory_percent,
                memory_available_mb=memory_available,
                disk_percent=disk_percent
            )

            time.sleep(30)  # æ¯30ç§’ç›‘æ§ä¸€æ¬¡

# å¯åŠ¨èµ„æºç›‘æ§
monitor = ResourceMonitor()
monitor.start()
```

### ç¼“å­˜æ€§èƒ½ç›‘æ§
```python
from functools import wraps
import time

class CacheMonitor:
    """ç¼“å­˜æ€§èƒ½ç›‘æ§"""

    def __init__(self):
        self.hits = 0
        self.misses = 0
        self.total_time = 0

    def record_hit(self, duration):
        self.hits += 1
        self.total_time += duration

    def record_miss(self, duration):
        self.misses += 1
        self.total_time += duration

    @property
    def hit_rate(self):
        total = self.hits + self.misses
        return self.hits / total if total > 0 else 0

    @property
    def avg_time(self):
        total = self.hits + self.misses
        return self.total_time / total if total > 0 else 0

cache_monitor = CacheMonitor()

def monitored_cache(cache_dict):
    """ç›‘æ§ç¼“å­˜è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            cache_key = f"{func.__name__}:{hash(str(args) + str(kwargs))}"
            start_time = time.time()

            if cache_key in cache_dict:
                result = cache_dict[cache_key]
                duration = time.time() - start_time
                cache_monitor.record_hit(duration)

                logger.debug(
                    "ç¼“å­˜å‘½ä¸­",
                    function=func.__name__,
                    cache_key=cache_key,
                    duration=duration
                )
            else:
                result = func(*args, **kwargs)
                cache_dict[cache_key] = result
                duration = time.time() - start_time
                cache_monitor.record_miss(duration)

                logger.debug(
                    "ç¼“å­˜æœªå‘½ä¸­",
                    function=func.__name__,
                    cache_key=cache_key,
                    duration=duration
                )

            return result

        return wrapper
    return decorator

# ä½¿ç”¨ç¼“å­˜ç›‘æ§
function_cache = {}

@monitored_cache(function_cache)
def expensive_operation(data):
    """è€—æ—¶æ“ä½œ"""
    time.sleep(1)  # æ¨¡æ‹Ÿè€—æ—¶æ“ä½œ
    return f"å¤„ç†ç»“æœ: {data}"
```

## éƒ¨ç½²ç›‘æ§é…ç½®

### Docker Compose ç›‘æ§æ ˆ
```yaml
# monitoring-stack.yml
version: '3.8'

services:
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/dashboards:/var/lib/grafana/dashboards
      - ./grafana/provisioning:/etc/grafana/provisioning

  jaeger:
    image: jaegertracing/all-in-one:latest
    ports:
      - "16686:16686"
      - "14268:14268"
    environment:
      - COLLECTOR_OTLP_ENABLED=true

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.5.0
    environment:
      - node.name=elasticsearch
      - cluster.name=langgraph-logs
      - discovery.type=single-node
      - xpack.security.enabled=false
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data

  kibana:
    image: docker.elastic.co/kibana/kibana:8.5.0
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200

volumes:
  prometheus_data:
  grafana_data:
  elasticsearch_data:
```

### Kubernetes ç›‘æ§éƒ¨ç½²
```yaml
# monitoring-namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: monitoring

---
# prometheus-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      containers:
      - name: prometheus
        image: prom/prometheus:latest
        ports:
        - containerPort: 9090
        volumeMounts:
        - name: config
          mountPath: /etc/prometheus
        - name: data
          mountPath: /prometheus
      volumes:
      - name: config
        configMap:
          name: prometheus-config
      - name: data
        persistentVolumeClaim:
          claimName: prometheus-pvc

---
# grafana-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafana
  template:
    metadata:
      labels:
        app: grafana
    spec:
      containers:
      - name: grafana
        image: grafana/grafana:latest
        ports:
        - containerPort: 3000
        env:
        - name: GF_SECURITY_ADMIN_PASSWORD
          valueFrom:
            secretKeyRef:
              name: grafana-secret
              key: admin-password
        volumeMounts:
        - name: data
          mountPath: /var/lib/grafana
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: grafana-pvc
```

## æ•…éšœæ’é™¤

### å¸¸è§ç›‘æ§é—®é¢˜

#### 1. æŒ‡æ ‡æ”¶é›†å¤±è´¥
```bash
# æ£€æŸ¥ Prometheus ç›®æ ‡çŠ¶æ€
curl http://localhost:9090/api/v1/targets

# æ£€æŸ¥åº”ç”¨æŒ‡æ ‡ç«¯ç‚¹
curl http://your-app:8000/metrics

# éªŒè¯æŒ‡æ ‡æ ¼å¼
prometheus_client.parser.text_string_to_metric_families(metrics_text)
```

#### 2. è¿½è¸ªæ•°æ®ç¼ºå¤±
```python
# éªŒè¯ LangSmith é…ç½®
import os
print("LANGCHAIN_TRACING_V2:", os.getenv("LANGCHAIN_TRACING_V2"))
print("LANGCHAIN_API_KEY:", os.getenv("LANGCHAIN_API_KEY")[:10] + "...")
print("LANGCHAIN_PROJECT:", os.getenv("LANGCHAIN_PROJECT"))

# æµ‹è¯• LangSmith è¿æ¥
from langsmith import Client
client = Client()
try:
    client.create_run(
        name="test-run",
        run_type="chain",
        inputs={"test": "data"}
    )
    print("LangSmith è¿æ¥æ­£å¸¸")
except Exception as e:
    print(f"LangSmith è¿æ¥å¤±è´¥: {e}")
```

#### 3. æ—¥å¿—æ ¼å¼é—®é¢˜
```python
# éªŒè¯æ—¥å¿—æ ¼å¼
import json

def validate_log_format(log_line):
    try:
        log_data = json.loads(log_line)
        required_fields = ['timestamp', 'level', 'message', 'service']

        for field in required_fields:
            if field not in log_data:
                return False, f"ç¼ºå°‘å­—æ®µ: {field}"

        return True, "æ ¼å¼æ­£ç¡®"
    except json.JSONDecodeError as e:
        return False, f"JSON æ ¼å¼é”™è¯¯: {e}"

# æµ‹è¯•æ—¥å¿—è¡Œ
log_line = '{"timestamp": "2024-01-01T00:00:00", "level": "INFO", "message": "test", "service": "langgraph"}'
is_valid, message = validate_log_format(log_line)
print(f"æ—¥å¿—æ ¼å¼éªŒè¯: {is_valid}, {message}")
```

## ä¸‹ä¸€æ­¥

- ğŸš€ å­¦ä¹  [05-æ‰©å±•æ€§ä¸éŸ§æ€§](./05-æ‰©å±•æ€§ä¸éŸ§æ€§.md) - æ„å»ºé«˜å¯ç”¨ç³»ç»Ÿ
- ğŸ” æ¢ç´¢ [07-è®¤è¯ä¸æˆæƒ](./07-è®¤è¯ä¸æˆæƒ.md) - å®‰å…¨è®¿é—®æ§åˆ¶
- ğŸ”Œ äº†è§£ [08-Webhooksé›†æˆ](./08-Webhooksé›†æˆ.md) - äº‹ä»¶é©±åŠ¨æ¶æ„

## ç›¸å…³é“¾æ¥

- [LangSmith è¿½è¸ªæŒ‡å—](https://docs.smith.langchain.com/)
- [Prometheus ç›‘æ§æœ€ä½³å®è·µ](https://prometheus.io/docs/practices/)
- [Grafana ä»ªè¡¨æ¿è®¾è®¡](https://grafana.com/docs/grafana/latest/dashboards/)
- [åˆ†å¸ƒå¼è¿½è¸ªæ ‡å‡†](https://opentelemetry.io/docs/)