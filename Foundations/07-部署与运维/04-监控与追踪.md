# 04-监控与追踪

## 概述

监控与追踪是生产环境中 LangGraph 应用的重要保障。通过 LangSmith 集成、自定义指标、日志系统和性能监控，实现应用的可观测性、故障诊断和性能优化。

## 监控体系架构

### 三层监控模型
```
┌─────────────────┐
│   业务监控      │ ← Agent 性能、用户体验
├─────────────────┤
│   应用监控      │ ← 请求量、响应时间、错误率
├─────────────────┤
│   基础设施监控   │ ← CPU、内存、磁盘、网络
└─────────────────┘
```

### 核心组件
- **LangSmith**: 官方 Agent 追踪平台
- **Prometheus**: 指标收集和存储
- **Grafana**: 可视化仪表板
- **ELK Stack**: 日志分析
- **Jaeger**: 分布式追踪
- **Sentry**: 错误监控

## LangSmith 集成

### 基础配置
```python
# 环境变量配置
import os

os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = "your_langsmith_api_key"
os.environ["LANGCHAIN_PROJECT"] = "my-agent-production"
os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"
```

### 图级别追踪
```python
from langgraph.graph import StateGraph
from langchain_core.tracers import LangChainTracer

def create_traced_graph():
    graph = StateGraph(AgentState)

    # 添加节点
    graph.add_node("input_processor", process_input)
    graph.add_node("llm_call", call_llm)
    graph.add_node("output_formatter", format_output)

    # 设置流程
    graph.set_entry_point("input_processor")
    graph.add_edge("input_processor", "llm_call")
    graph.add_edge("llm_call", "output_formatter")
    graph.set_finish_point("output_formatter")

    return graph.compile()

# 使用追踪
app = create_traced_graph()
result = app.invoke(
    {"input": "用户问题"},
    config={
        "configurable": {
            "thread_id": "user_123",
        },
        "tags": ["production", "v1.0"],
        "metadata": {
            "user_id": "user_123",
            "session_id": "session_456",
            "environment": "production"
        }
    }
)
```

### 自定义追踪
```python
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.messages import BaseMessage
from typing import Any, Dict, List

class CustomTracer(BaseCallbackHandler):
    """自定义追踪器"""

    def on_chain_start(
        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs
    ) -> None:
        """链开始时的回调"""
        print(f"开始执行链: {serialized.get('name', 'Unknown')}")
        print(f"输入参数: {inputs}")

    def on_chain_end(self, outputs: Dict[str, Any], **kwargs) -> None:
        """链结束时的回调"""
        print(f"链执行完成，输出: {outputs}")

    def on_chain_error(self, error: Exception, **kwargs) -> None:
        """链执行错误时的回调"""
        print(f"链执行错误: {error}")

    def on_llm_start(
        self, serialized: Dict[str, Any], prompts: List[str], **kwargs
    ) -> None:
        """LLM 调用开始"""
        print(f"开始 LLM 调用: {serialized.get('name', 'Unknown')}")
        print(f"提示词: {prompts[0][:100]}...")

    def on_llm_end(self, response: Any, **kwargs) -> None:
        """LLM 调用结束"""
        print(f"LLM 响应完成")

    def on_tool_start(
        self, serialized: Dict[str, Any], input_str: str, **kwargs
    ) -> None:
        """工具调用开始"""
        print(f"开始调用工具: {serialized.get('name', 'Unknown')}")
        print(f"输入: {input_str}")

# 使用自定义追踪器
app = create_traced_graph()
result = app.invoke(
    {"input": "测试问题"},
    config={"callbacks": [CustomTracer()]}
)
```

### 批量追踪分析
```python
from langsmith import Client

client = Client()

# 查询追踪记录
runs = client.list_runs(
    project_name="my-agent-production",
    start_time=datetime.now() - timedelta(hours=24),
    end_time=datetime.now()
)

# 分析性能指标
total_runs = len(list(runs))
successful_runs = len([r for r in runs if not r.error])
success_rate = successful_runs / total_runs * 100

print(f"24小时内总运行次数: {total_runs}")
print(f"成功率: {success_rate:.2f}%")

# 分析响应时间
response_times = [
    (r.end_time - r.start_time).total_seconds()
    for r in runs if r.end_time and r.start_time
]

if response_times:
    avg_time = sum(response_times) / len(response_times)
    print(f"平均响应时间: {avg_time:.2f}秒")
```

## Prometheus 指标收集

### 应用指标定义
```python
from prometheus_client import Counter, Histogram, Gauge, generate_latest, CONTENT_TYPE_LATEST
import time
from functools import wraps

# 定义指标
REQUEST_COUNT = Counter(
    'langgraph_requests_total',
    'Total number of requests',
    ['method', 'endpoint', 'status']
)

REQUEST_DURATION = Histogram(
    'langgraph_request_duration_seconds',
    'Request duration in seconds',
    ['method', 'endpoint']
)

ACTIVE_CONNECTIONS = Gauge(
    'langgraph_active_connections',
    'Number of active connections'
)

GRAPH_EXECUTION_TIME = Histogram(
    'langgraph_graph_execution_seconds',
    'Graph execution time in seconds',
    ['graph_name', 'node_name']
)

LLM_TOKEN_USAGE = Counter(
    'langgraph_llm_tokens_total',
    'Total LLM tokens used',
    ['model', 'type']  # type: prompt_tokens, completion_tokens
)

ERROR_COUNT = Counter(
    'langgraph_errors_total',
    'Total number of errors',
    ['error_type', 'node_name']
)

def track_metrics(func):
    """装饰器：追踪函数执行指标"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()

        try:
            result = func(*args, **kwargs)
            REQUEST_COUNT.labels(
                method='POST',
                endpoint=func.__name__,
                status='success'
            ).inc()
            return result
        except Exception as e:
            REQUEST_COUNT.labels(
                method='POST',
                endpoint=func.__name__,
                status='error'
            ).inc()
            ERROR_COUNT.labels(
                error_type=type(e).__name__,
                node_name=func.__name__
            ).inc()
            raise
        finally:
            duration = time.time() - start_time
            REQUEST_DURATION.labels(
                method='POST',
                endpoint=func.__name__
            ).observe(duration)

    return wrapper

@track_metrics
def process_input_node(state):
    """处理输入节点"""
    GRAPH_EXECUTION_TIME.labels(
        graph_name='main_graph',
        node_name='process_input'
    ).time()

    # 节点逻辑
    return {"processed_input": state["input"]}
```

### FastAPI 集成
```python
from fastapi import FastAPI, Response
from prometheus_client import generate_latest, CONTENT_TYPE_LATEST

app = FastAPI()

@app.middleware("http")
async def track_requests(request, call_next):
    """请求追踪中间件"""
    start_time = time.time()
    ACTIVE_CONNECTIONS.inc()

    try:
        response = await call_next(request)
        REQUEST_COUNT.labels(
            method=request.method,
            endpoint=request.url.path,
            status=response.status_code
        ).inc()
        return response
    finally:
        duration = time.time() - start_time
        REQUEST_DURATION.labels(
            method=request.method,
            endpoint=request.url.path
        ).observe(duration)
        ACTIVE_CONNECTIONS.dec()

@app.get("/metrics")
async def metrics():
    """Prometheus 指标端点"""
    return Response(
        generate_latest(),
        media_type=CONTENT_TYPE_LATEST
    )

@app.get("/health")
async def health_check():
    """健康检查"""
    return {"status": "healthy", "timestamp": time.time()}
```

### LLM 调用追踪
```python
from langchain_openai import ChatOpenAI
from langchain_core.callbacks import BaseCallbackHandler

class TokenTracker(BaseCallbackHandler):
    """Token 使用追踪"""

    def on_llm_end(self, response, **kwargs):
        if hasattr(response, 'llm_output') and response.llm_output:
            token_usage = response.llm_output.get('token_usage', {})
            model = response.llm_output.get('model_name', 'unknown')

            prompt_tokens = token_usage.get('prompt_tokens', 0)
            completion_tokens = token_usage.get('completion_tokens', 0)

            LLM_TOKEN_USAGE.labels(
                model=model,
                type='prompt_tokens'
            ).inc(prompt_tokens)

            LLM_TOKEN_USAGE.labels(
                model=model,
                type='completion_tokens'
            ).inc(completion_tokens)

# 使用 Token 追踪
llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    callbacks=[TokenTracker()]
)
```

## Grafana 仪表板

### 仪表板配置
```json
{
  "dashboard": {
    "id": null,
    "title": "LangGraph 监控仪表板",
    "tags": ["langgraph", "production"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "请求率",
        "type": "stat",
        "targets": [
          {
            "expr": "rate(langgraph_requests_total[5m])",
            "legendFormat": "请求/秒"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "reqps"
          }
        }
      },
      {
        "id": 2,
        "title": "成功率",
        "type": "stat",
        "targets": [
          {
            "expr": "rate(langgraph_requests_total{status=\"success\"}[5m]) / rate(langgraph_requests_total[5m]) * 100",
            "legendFormat": "成功率"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percent",
            "min": 0,
            "max": 100
          }
        }
      },
      {
        "id": 3,
        "title": "响应时间分布",
        "type": "heatmap",
        "targets": [
          {
            "expr": "increase(langgraph_request_duration_seconds_bucket[5m])",
            "format": "heatmap",
            "legendFormat": "{{le}}"
          }
        ]
      },
      {
        "id": 4,
        "title": "错误趋势",
        "type": "timeseries",
        "targets": [
          {
            "expr": "rate(langgraph_errors_total[5m])",
            "legendFormat": "{{error_type}}"
          }
        ]
      },
      {
        "id": 5,
        "title": "Token 使用量",
        "type": "timeseries",
        "targets": [
          {
            "expr": "rate(langgraph_llm_tokens_total[5m])",
            "legendFormat": "{{model}} - {{type}}"
          }
        ]
      }
    ],
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "refresh": "30s"
  }
}
```

### 报警规则
```yaml
# alerting-rules.yml
groups:
  - name: langgraph-alerts
    rules:
      - alert: HighErrorRate
        expr: rate(langgraph_requests_total{status="error"}[5m]) / rate(langgraph_requests_total[5m]) > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "LangGraph 错误率过高"
          description: "错误率在过去5分钟内超过10%"

      - alert: HighResponseTime
        expr: histogram_quantile(0.95, rate(langgraph_request_duration_seconds_bucket[5m])) > 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "LangGraph 响应时间过长"
          description: "95分位响应时间超过10秒"

      - alert: ServiceDown
        expr: up{job="langgraph"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "LangGraph 服务不可用"
          description: "LangGraph 服务已停止响应"

      - alert: HighTokenUsage
        expr: rate(langgraph_llm_tokens_total[1h]) > 100000
        for: 10m
        labels:
          severity: info
        annotations:
          summary: "Token 使用量较高"
          description: "每小时 Token 使用量超过10万"
```

## 日志管理

### 结构化日志
```python
import logging
import json
from datetime import datetime
from typing import Any, Dict

class StructuredLogger:
    """结构化日志器"""

    def __init__(self, name: str):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(logging.INFO)

        # 创建格式化器
        formatter = logging.Formatter('%(message)s')

        # 创建处理器
        handler = logging.StreamHandler()
        handler.setFormatter(formatter)

        self.logger.addHandler(handler)

    def _log(self, level: str, message: str, **kwargs):
        """统一日志格式"""
        log_data = {
            "timestamp": datetime.utcnow().isoformat(),
            "level": level,
            "message": message,
            "service": "langgraph",
            **kwargs
        }

        log_line = json.dumps(log_data, ensure_ascii=False)
        getattr(self.logger, level.lower())(log_line)

    def info(self, message: str, **kwargs):
        self._log("INFO", message, **kwargs)

    def warning(self, message: str, **kwargs):
        self._log("WARNING", message, **kwargs)

    def error(self, message: str, **kwargs):
        self._log("ERROR", message, **kwargs)

    def debug(self, message: str, **kwargs):
        self._log("DEBUG", message, **kwargs)

# 使用结构化日志
logger = StructuredLogger("langgraph.agent")

def process_request(user_input: str, user_id: str):
    """处理用户请求"""
    logger.info(
        "开始处理用户请求",
        user_id=user_id,
        input_length=len(user_input),
        request_id="req_123"
    )

    try:
        # 处理逻辑
        result = "处理结果"

        logger.info(
            "请求处理成功",
            user_id=user_id,
            request_id="req_123",
            output_length=len(result)
        )

        return result

    except Exception as e:
        logger.error(
            "请求处理失败",
            user_id=user_id,
            request_id="req_123",
            error_type=type(e).__name__,
            error_message=str(e)
        )
        raise
```

### 节点级日志
```python
def logged_node(node_name: str):
    """节点日志装饰器"""
    def decorator(func):
        @wraps(func)
        def wrapper(state):
            request_id = state.get("request_id", "unknown")

            logger.info(
                f"节点开始执行",
                node_name=node_name,
                request_id=request_id,
                input_keys=list(state.keys())
            )

            start_time = time.time()

            try:
                result = func(state)
                duration = time.time() - start_time

                logger.info(
                    f"节点执行成功",
                    node_name=node_name,
                    request_id=request_id,
                    duration=duration,
                    output_keys=list(result.keys())
                )

                return result

            except Exception as e:
                duration = time.time() - start_time

                logger.error(
                    f"节点执行失败",
                    node_name=node_name,
                    request_id=request_id,
                    duration=duration,
                    error_type=type(e).__name__,
                    error_message=str(e)
                )
                raise

        return wrapper
    return decorator

@logged_node("input_processor")
def process_input(state):
    """处理输入"""
    return {"processed_input": state["input"].strip()}

@logged_node("llm_caller")
def call_llm(state):
    """调用 LLM"""
    # LLM 调用逻辑
    return {"llm_response": "LLM 响应"}
```

### ELK Stack 配置

#### Logstash 配置
```ruby
# logstash.conf
input {
  beats {
    port => 5044
  }
}

filter {
  if [fields][service] == "langgraph" {
    json {
      source => "message"
    }

    date {
      match => [ "timestamp", "ISO8601" ]
    }

    mutate {
      add_field => { "service" => "langgraph" }
    }
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "langgraph-logs-%{+YYYY.MM.dd}"
  }
}
```

#### Filebeat 配置
```yaml
# filebeat.yml
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /var/log/langgraph/*.log
  fields:
    service: langgraph
  fields_under_root: true
  json.keys_under_root: true
  json.add_error_key: true

output.logstash:
  hosts: ["logstash:5044"]

logging.level: info
```

## 分布式追踪

### Jaeger 集成
```python
from opentelemetry import trace
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.instrumentation.requests import RequestsInstrumentor
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor

# 配置 Jaeger
jaeger_exporter = JaegerExporter(
    agent_host_name="jaeger",
    agent_port=6831,
)

trace.set_tracer_provider(TracerProvider())
tracer = trace.get_tracer(__name__)

span_processor = BatchSpanProcessor(jaeger_exporter)
trace.get_tracer_provider().add_span_processor(span_processor)

# 自动追踪 HTTP 请求
RequestsInstrumentor().instrument()

def traced_node(node_name: str):
    """分布式追踪装饰器"""
    def decorator(func):
        @wraps(func)
        def wrapper(state):
            with tracer.start_as_current_span(node_name) as span:
                # 添加 span 属性
                span.set_attribute("node.name", node_name)
                span.set_attribute("state.keys", ",".join(state.keys()))

                try:
                    result = func(state)
                    span.set_attribute("node.status", "success")
                    span.set_attribute("output.keys", ",".join(result.keys()))
                    return result
                except Exception as e:
                    span.set_attribute("node.status", "error")
                    span.set_attribute("error.type", type(e).__name__)
                    span.set_attribute("error.message", str(e))
                    raise

        return wrapper
    return decorator

@traced_node("input_processor")
def process_input(state):
    """处理输入（带追踪）"""
    return {"processed_input": state["input"]}
```

## 错误监控

### Sentry 集成
```python
import sentry_sdk
from sentry_sdk.integrations.logging import LoggingIntegration

# 配置 Sentry
sentry_logging = LoggingIntegration(
    level=logging.INFO,        # 捕获 info 及以上级别
    event_level=logging.ERROR  # 发送 error 及以上级别到 Sentry
)

sentry_sdk.init(
    dsn="your_sentry_dsn",
    integrations=[sentry_logging],
    traces_sample_rate=1.0,
    environment="production"
)

def monitored_node(func):
    """错误监控装饰器"""
    @wraps(func)
    def wrapper(state):
        try:
            return func(state)
        except Exception as e:
            # 添加上下文信息
            sentry_sdk.set_context("state", {
                "keys": list(state.keys()),
                "request_id": state.get("request_id")
            })

            sentry_sdk.set_tag("node_name", func.__name__)
            sentry_sdk.capture_exception(e)
            raise

    return wrapper

@monitored_node
def risky_node(state):
    """可能出错的节点"""
    if not state.get("input"):
        raise ValueError("输入不能为空")

    return {"output": "处理结果"}
```

## 性能监控

### 资源使用监控
```python
import psutil
import threading
import time

class ResourceMonitor:
    """资源使用监控"""

    def __init__(self):
        self.running = False
        self.thread = None

    def start(self):
        """开始监控"""
        self.running = True
        self.thread = threading.Thread(target=self._monitor_loop)
        self.thread.start()

    def stop(self):
        """停止监控"""
        self.running = False
        if self.thread:
            self.thread.join()

    def _monitor_loop(self):
        """监控循环"""
        while self.running:
            # CPU 使用率
            cpu_percent = psutil.cpu_percent(interval=1)

            # 内存使用
            memory = psutil.virtual_memory()
            memory_percent = memory.percent
            memory_available = memory.available / 1024 / 1024  # MB

            # 磁盘使用
            disk = psutil.disk_usage('/')
            disk_percent = disk.percent

            # 更新 Prometheus 指标
            CPU_USAGE.set(cpu_percent)
            MEMORY_USAGE.set(memory_percent)
            MEMORY_AVAILABLE.set(memory_available)
            DISK_USAGE.set(disk_percent)

            logger.debug(
                "系统资源使用情况",
                cpu_percent=cpu_percent,
                memory_percent=memory_percent,
                memory_available_mb=memory_available,
                disk_percent=disk_percent
            )

            time.sleep(30)  # 每30秒监控一次

# 启动资源监控
monitor = ResourceMonitor()
monitor.start()
```

### 缓存性能监控
```python
from functools import wraps
import time

class CacheMonitor:
    """缓存性能监控"""

    def __init__(self):
        self.hits = 0
        self.misses = 0
        self.total_time = 0

    def record_hit(self, duration):
        self.hits += 1
        self.total_time += duration

    def record_miss(self, duration):
        self.misses += 1
        self.total_time += duration

    @property
    def hit_rate(self):
        total = self.hits + self.misses
        return self.hits / total if total > 0 else 0

    @property
    def avg_time(self):
        total = self.hits + self.misses
        return self.total_time / total if total > 0 else 0

cache_monitor = CacheMonitor()

def monitored_cache(cache_dict):
    """监控缓存装饰器"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            cache_key = f"{func.__name__}:{hash(str(args) + str(kwargs))}"
            start_time = time.time()

            if cache_key in cache_dict:
                result = cache_dict[cache_key]
                duration = time.time() - start_time
                cache_monitor.record_hit(duration)

                logger.debug(
                    "缓存命中",
                    function=func.__name__,
                    cache_key=cache_key,
                    duration=duration
                )
            else:
                result = func(*args, **kwargs)
                cache_dict[cache_key] = result
                duration = time.time() - start_time
                cache_monitor.record_miss(duration)

                logger.debug(
                    "缓存未命中",
                    function=func.__name__,
                    cache_key=cache_key,
                    duration=duration
                )

            return result

        return wrapper
    return decorator

# 使用缓存监控
function_cache = {}

@monitored_cache(function_cache)
def expensive_operation(data):
    """耗时操作"""
    time.sleep(1)  # 模拟耗时操作
    return f"处理结果: {data}"
```

## 部署监控配置

### Docker Compose 监控栈
```yaml
# monitoring-stack.yml
version: '3.8'

services:
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/dashboards:/var/lib/grafana/dashboards
      - ./grafana/provisioning:/etc/grafana/provisioning

  jaeger:
    image: jaegertracing/all-in-one:latest
    ports:
      - "16686:16686"
      - "14268:14268"
    environment:
      - COLLECTOR_OTLP_ENABLED=true

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.5.0
    environment:
      - node.name=elasticsearch
      - cluster.name=langgraph-logs
      - discovery.type=single-node
      - xpack.security.enabled=false
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data

  kibana:
    image: docker.elastic.co/kibana/kibana:8.5.0
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200

volumes:
  prometheus_data:
  grafana_data:
  elasticsearch_data:
```

### Kubernetes 监控部署
```yaml
# monitoring-namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: monitoring

---
# prometheus-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      containers:
      - name: prometheus
        image: prom/prometheus:latest
        ports:
        - containerPort: 9090
        volumeMounts:
        - name: config
          mountPath: /etc/prometheus
        - name: data
          mountPath: /prometheus
      volumes:
      - name: config
        configMap:
          name: prometheus-config
      - name: data
        persistentVolumeClaim:
          claimName: prometheus-pvc

---
# grafana-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafana
  template:
    metadata:
      labels:
        app: grafana
    spec:
      containers:
      - name: grafana
        image: grafana/grafana:latest
        ports:
        - containerPort: 3000
        env:
        - name: GF_SECURITY_ADMIN_PASSWORD
          valueFrom:
            secretKeyRef:
              name: grafana-secret
              key: admin-password
        volumeMounts:
        - name: data
          mountPath: /var/lib/grafana
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: grafana-pvc
```

## 故障排除

### 常见监控问题

#### 1. 指标收集失败
```bash
# 检查 Prometheus 目标状态
curl http://localhost:9090/api/v1/targets

# 检查应用指标端点
curl http://your-app:8000/metrics

# 验证指标格式
prometheus_client.parser.text_string_to_metric_families(metrics_text)
```

#### 2. 追踪数据缺失
```python
# 验证 LangSmith 配置
import os
print("LANGCHAIN_TRACING_V2:", os.getenv("LANGCHAIN_TRACING_V2"))
print("LANGCHAIN_API_KEY:", os.getenv("LANGCHAIN_API_KEY")[:10] + "...")
print("LANGCHAIN_PROJECT:", os.getenv("LANGCHAIN_PROJECT"))

# 测试 LangSmith 连接
from langsmith import Client
client = Client()
try:
    client.create_run(
        name="test-run",
        run_type="chain",
        inputs={"test": "data"}
    )
    print("LangSmith 连接正常")
except Exception as e:
    print(f"LangSmith 连接失败: {e}")
```

#### 3. 日志格式问题
```python
# 验证日志格式
import json

def validate_log_format(log_line):
    try:
        log_data = json.loads(log_line)
        required_fields = ['timestamp', 'level', 'message', 'service']

        for field in required_fields:
            if field not in log_data:
                return False, f"缺少字段: {field}"

        return True, "格式正确"
    except json.JSONDecodeError as e:
        return False, f"JSON 格式错误: {e}"

# 测试日志行
log_line = '{"timestamp": "2024-01-01T00:00:00", "level": "INFO", "message": "test", "service": "langgraph"}'
is_valid, message = validate_log_format(log_line)
print(f"日志格式验证: {is_valid}, {message}")
```

## 下一步

- 🚀 学习 [05-扩展性与韧性](./05-扩展性与韧性.md) - 构建高可用系统
- 🔐 探索 [07-认证与授权](./07-认证与授权.md) - 安全访问控制
- 🔌 了解 [08-Webhooks集成](./08-Webhooks集成.md) - 事件驱动架构

## 相关链接

- [LangSmith 追踪指南](https://docs.smith.langchain.com/)
- [Prometheus 监控最佳实践](https://prometheus.io/docs/practices/)
- [Grafana 仪表板设计](https://grafana.com/docs/grafana/latest/dashboards/)
- [分布式追踪标准](https://opentelemetry.io/docs/)