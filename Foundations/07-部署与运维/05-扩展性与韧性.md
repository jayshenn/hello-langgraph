# 05-扩展性与韧性

## 概述

扩展性与韧性是生产环境中 LangGraph 应用的核心要求。通过水平扩展、负载均衡、故障恢复、数据一致性和容灾设计，确保系统在高负载和异常情况下的稳定运行。

## 系统架构设计

### 高可用架构图
```
                    ┌─────────────────┐
                    │   Load Balancer │
                    │    (HAProxy)    │
                    └─────────┬───────┘
                              │
                 ┌────────────┼────────────┐
                 │            │            │
         ┌───────▼───┐ ┌──────▼───┐ ┌──────▼───┐
         │LangGraph  │ │LangGraph │ │LangGraph │
         │Server 1   │ │Server 2  │ │Server 3  │
         └───────┬───┘ └──────┬───┘ └──────┬───┘
                 │            │            │
                 └────────────┼────────────┘
                              │
              ┌───────────────▼───────────────┐
              │          Message Queue        │
              │         (Redis Cluster)       │
              └───────────────┬───────────────┘
                              │
              ┌───────────────▼───────────────┐
              │         Database Pool         │
              │    (PostgreSQL Cluster)      │
              └───────────────────────────────┘
```

### 核心设计原则
1. **无单点故障**: 所有组件都有冗余
2. **水平扩展**: 支持动态添加实例
3. **优雅降级**: 部分功能失效时保持核心服务
4. **快速恢复**: 自动故障检测和恢复
5. **数据一致性**: 保证分布式数据的一致性

## 水平扩展

### Kubernetes 自动扩展

#### HPA (Horizontal Pod Autoscaler)
```yaml
# hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: langgraph-hpa
  namespace: langgraph
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: langgraph-server
  minReplicas: 2
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "1000"
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 120
      policies:
      - type: Percent
        value: 100
        periodSeconds: 60
      - type: Pods
        value: 4
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
```

#### VPA (Vertical Pod Autoscaler)
```yaml
# vpa.yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: langgraph-vpa
  namespace: langgraph
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: langgraph-server
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: langgraph-server
      minAllowed:
        cpu: 100m
        memory: 128Mi
      maxAllowed:
        cpu: 2
        memory: 4Gi
      controlledResources: ["cpu", "memory"]
```

### 应用层扩展

#### 无状态设计
```python
# stateless_agent.py
from typing import Dict, Any
import uuid
from langgraph.graph import StateGraph
from langgraph.checkpoint import MemorySaver
import redis

class StatelessAgent:
    """无状态 Agent 设计"""

    def __init__(self, redis_url: str):
        self.redis_client = redis.from_url(redis_url)
        self.graph = self._create_graph()

    def _create_graph(self) -> StateGraph:
        """创建无状态图"""
        graph = StateGraph(AgentState)

        # 所有状态都通过外部存储管理
        graph.add_node("load_context", self.load_context)
        graph.add_node("process", self.process_request)
        graph.add_node("save_context", self.save_context)

        graph.set_entry_point("load_context")
        graph.add_edge("load_context", "process")
        graph.add_edge("process", "save_context")
        graph.set_finish_point("save_context")

        return graph.compile()

    def load_context(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """从外部存储加载上下文"""
        session_id = state.get("session_id")
        if session_id:
            context = self.redis_client.get(f"context:{session_id}")
            if context:
                state.update(json.loads(context))

        return state

    def save_context(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """保存上下文到外部存储"""
        session_id = state.get("session_id")
        if session_id:
            # 只保存必要的上下文信息
            context = {
                "last_response": state.get("response"),
                "conversation_history": state.get("messages", [])[-10:],  # 只保留最近10条
                "user_preferences": state.get("user_preferences", {})
            }
            self.redis_client.setex(
                f"context:{session_id}",
                3600,  # 1小时过期
                json.dumps(context)
            )

        return state

    def process_request(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """处理请求"""
        # 无状态处理逻辑
        user_input = state.get("input", "")
        response = f"处理结果: {user_input}"

        return {
            "response": response,
            "timestamp": time.time()
        }

    def invoke(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """调用 Agent"""
        # 为每个请求生成唯一标识
        if "request_id" not in input_data:
            input_data["request_id"] = str(uuid.uuid4())

        return self.graph.invoke(input_data)
```

#### 连接池管理
```python
# connection_pool.py
from typing import Dict, Any, Optional
import asyncio
import aioredis
import asyncpg
from contextlib import asynccontextmanager

class ConnectionPoolManager:
    """连接池管理器"""

    def __init__(self):
        self.redis_pool: Optional[aioredis.ConnectionPool] = None
        self.postgres_pool: Optional[asyncpg.Pool] = None

    async def initialize(
        self,
        redis_url: str,
        postgres_url: str,
        max_connections: int = 20
    ):
        """初始化连接池"""
        # Redis 连接池
        self.redis_pool = aioredis.ConnectionPool.from_url(
            redis_url,
            max_connections=max_connections,
            socket_keepalive=True,
            socket_keepalive_options={
                "TCP_KEEPIDLE": 30,
                "TCP_KEEPINTVL": 5,
                "TCP_KEEPCNT": 5
            }
        )

        # PostgreSQL 连接池
        self.postgres_pool = await asyncpg.create_pool(
            postgres_url,
            min_size=5,
            max_size=max_connections,
            command_timeout=60,
            server_settings={
                "application_name": "langgraph_agent",
                "tcp_keepalives_idle": "300",
                "tcp_keepalives_interval": "30",
                "tcp_keepalives_count": "3"
            }
        )

    async def close(self):
        """关闭连接池"""
        if self.redis_pool:
            await self.redis_pool.disconnect()

        if self.postgres_pool:
            await self.postgres_pool.close()

    @asynccontextmanager
    async def get_redis_connection(self):
        """获取 Redis 连接"""
        connection = None
        try:
            connection = aioredis.Redis(connection_pool=self.redis_pool)
            yield connection
        finally:
            if connection:
                await connection.close()

    @asynccontextmanager
    async def get_postgres_connection(self):
        """获取 PostgreSQL 连接"""
        connection = None
        try:
            connection = await self.postgres_pool.acquire()
            yield connection
        finally:
            if connection:
                await self.postgres_pool.release(connection)

# 全局连接池实例
pool_manager = ConnectionPoolManager()

class ScalableAgent:
    """可扩展的 Agent"""

    async def process_with_db(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """使用数据库连接池处理请求"""
        async with pool_manager.get_postgres_connection() as conn:
            # 数据库操作
            result = await conn.fetchrow(
                "SELECT * FROM user_preferences WHERE user_id = $1",
                state.get("user_id")
            )

            if result:
                state["user_preferences"] = dict(result)

        async with pool_manager.get_redis_connection() as redis:
            # 缓存操作
            cache_key = f"processed:{state.get('request_id')}"
            await redis.setex(cache_key, 300, "processed")

        return state
```

## 负载均衡

### HAProxy 配置
```
# haproxy.cfg
global
    daemon
    maxconn 4096
    log stdout local0 info

defaults
    mode http
    timeout connect 5000ms
    timeout client 50000ms
    timeout server 50000ms
    option httplog
    option dontlognull
    option redispatch
    retries 3

# 前端配置
frontend langgraph_frontend
    bind *:80
    bind *:443 ssl crt /etc/ssl/certs/langgraph.pem
    redirect scheme https if !{ ssl_fc }

    # 健康检查
    acl health_check path_beg /health

    # 路由规则
    use_backend langgraph_backend if !health_check
    use_backend health_backend if health_check

# 后端配置
backend langgraph_backend
    balance roundrobin
    option httpchk GET /health
    http-check expect status 200

    # 服务器列表
    server langgraph1 langgraph-server-1:8000 check inter 30s rise 2 fall 3
    server langgraph2 langgraph-server-2:8000 check inter 30s rise 2 fall 3
    server langgraph3 langgraph-server-3:8000 check inter 30s rise 2 fall 3

backend health_backend
    balance roundrobin
    server health1 langgraph-server-1:8000 check
    server health2 langgraph-server-2:8000 check

# 统计页面
stats enable
stats uri /stats
stats realm HAProxy\ Statistics
stats auth admin:admin123
```

### Nginx 负载均衡
```nginx
# nginx.conf
upstream langgraph_backend {
    least_conn;
    server langgraph-server-1:8000 max_fails=3 fail_timeout=30s;
    server langgraph-server-2:8000 max_fails=3 fail_timeout=30s;
    server langgraph-server-3:8000 max_fails=3 fail_timeout=30s;
}

server {
    listen 80;
    listen 443 ssl http2;
    server_name langgraph.example.com;

    # SSL 配置
    ssl_certificate /etc/ssl/certs/langgraph.crt;
    ssl_certificate_key /etc/ssl/private/langgraph.key;
    ssl_protocols TLSv1.2 TLSv1.3;

    # 限流配置
    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;
    limit_req zone=api burst=20 nodelay;

    # 健康检查
    location /health {
        access_log off;
        proxy_pass http://langgraph_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }

    # 主要服务
    location / {
        proxy_pass http://langgraph_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;

        # 超时配置
        proxy_connect_timeout 60s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;

        # 缓冲配置
        proxy_buffering on;
        proxy_buffer_size 128k;
        proxy_buffers 4 256k;
        proxy_busy_buffers_size 256k;
    }

    # WebSocket 支持
    location /ws {
        proxy_pass http://langgraph_backend;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

## 故障恢复

### 断路器模式
```python
# circuit_breaker.py
import time
import asyncio
from enum import Enum
from typing import Callable, Any, Optional
from dataclasses import dataclass

class CircuitState(Enum):
    CLOSED = "closed"      # 正常状态
    OPEN = "open"          # 熔断状态
    HALF_OPEN = "half_open"  # 半开状态

@dataclass
class CircuitBreakerConfig:
    failure_threshold: int = 5      # 失败阈值
    recovery_timeout: int = 60      # 恢复超时（秒）
    expected_exception: type = Exception  # 期望的异常类型
    name: str = "default"

class CircuitBreaker:
    """断路器实现"""

    def __init__(self, config: CircuitBreakerConfig):
        self.config = config
        self.failure_count = 0
        self.last_failure_time: Optional[float] = None
        self.state = CircuitState.CLOSED

    async def call(self, func: Callable, *args, **kwargs) -> Any:
        """执行函数调用"""
        if self.state == CircuitState.OPEN:
            if self._should_attempt_reset():
                self.state = CircuitState.HALF_OPEN
            else:
                raise CircuitOpenException(
                    f"断路器 {self.config.name} 处于开启状态"
                )

        try:
            result = await func(*args, **kwargs) if asyncio.iscoroutinefunction(func) else func(*args, **kwargs)
            self._on_success()
            return result

        except self.config.expected_exception as e:
            self._on_failure()
            raise

    def _should_attempt_reset(self) -> bool:
        """是否应该尝试重置"""
        return (
            self.last_failure_time is not None and
            time.time() - self.last_failure_time >= self.config.recovery_timeout
        )

    def _on_success(self):
        """成功回调"""
        self.failure_count = 0
        self.state = CircuitState.CLOSED

    def _on_failure(self):
        """失败回调"""
        self.failure_count += 1
        self.last_failure_time = time.time()

        if self.failure_count >= self.config.failure_threshold:
            self.state = CircuitState.OPEN

class CircuitOpenException(Exception):
    """断路器开启异常"""
    pass

# 使用断路器
llm_circuit_breaker = CircuitBreaker(
    CircuitBreakerConfig(
        failure_threshold=3,
        recovery_timeout=30,
        name="llm_service"
    )
)

async def call_llm_with_circuit_breaker(prompt: str) -> str:
    """使用断路器调用 LLM"""
    async def llm_call():
        # 模拟 LLM 调用
        if random.random() < 0.3:  # 30% 失败率
            raise Exception("LLM 服务不可用")
        return f"LLM 响应: {prompt}"

    try:
        return await llm_circuit_breaker.call(llm_call)
    except CircuitOpenException:
        # 断路器开启时的降级处理
        return "抱歉，服务暂时不可用，请稍后重试"
```

### 重试机制
```python
# retry_mechanism.py
import asyncio
import random
from typing import Callable, Any, List, Type
from functools import wraps

class RetryConfig:
    """重试配置"""

    def __init__(
        self,
        max_attempts: int = 3,
        base_delay: float = 1.0,
        max_delay: float = 60.0,
        exponential_base: float = 2.0,
        jitter: bool = True,
        retriable_exceptions: List[Type[Exception]] = None
    ):
        self.max_attempts = max_attempts
        self.base_delay = base_delay
        self.max_delay = max_delay
        self.exponential_base = exponential_base
        self.jitter = jitter
        self.retriable_exceptions = retriable_exceptions or [Exception]

def async_retry(config: RetryConfig):
    """异步重试装饰器"""
    def decorator(func: Callable):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            last_exception = None

            for attempt in range(config.max_attempts):
                try:
                    return await func(*args, **kwargs)
                except Exception as e:
                    last_exception = e

                    # 检查是否为可重试异常
                    if not any(isinstance(e, exc_type) for exc_type in config.retriable_exceptions):
                        raise

                    # 最后一次尝试，不再等待
                    if attempt == config.max_attempts - 1:
                        break

                    # 计算延迟时间
                    delay = min(
                        config.base_delay * (config.exponential_base ** attempt),
                        config.max_delay
                    )

                    # 添加抖动
                    if config.jitter:
                        delay *= (0.5 + random.random() * 0.5)

                    logger.warning(
                        f"函数 {func.__name__} 第 {attempt + 1} 次尝试失败，"
                        f"{delay:.2f}秒后重试: {e}"
                    )

                    await asyncio.sleep(delay)

            # 所有重试都失败
            raise last_exception

        return wrapper
    return decorator

# 使用重试机制
@async_retry(RetryConfig(
    max_attempts=3,
    base_delay=1.0,
    exponential_base=2.0,
    retriable_exceptions=[ConnectionError, TimeoutError]
))
async def unreliable_api_call(data: str) -> str:
    """不可靠的 API 调用"""
    if random.random() < 0.5:  # 50% 失败率
        raise ConnectionError("网络连接失败")

    return f"API 响应: {data}"
```

### 优雅降级
```python
# graceful_degradation.py
from typing import Dict, Any, Optional, Callable
from enum import Enum

class ServiceLevel(Enum):
    FULL = "full"           # 完整服务
    DEGRADED = "degraded"   # 降级服务
    MINIMAL = "minimal"     # 最小服务

class DegradationManager:
    """降级管理器"""

    def __init__(self):
        self.service_level = ServiceLevel.FULL
        self.degraded_features = set()

    def degrade_service(self, level: ServiceLevel, features: set = None):
        """降级服务"""
        self.service_level = level
        if features:
            self.degraded_features.update(features)

        logger.warning(f"服务降级到 {level.value} 级别")

    def restore_service(self):
        """恢复服务"""
        self.service_level = ServiceLevel.FULL
        self.degraded_features.clear()
        logger.info("服务已恢复到完整级别")

    def is_feature_available(self, feature: str) -> bool:
        """检查功能是否可用"""
        return feature not in self.degraded_features

degradation_manager = DegradationManager()

def with_degradation(
    feature: str,
    fallback: Optional[Callable] = None
):
    """降级装饰器"""
    def decorator(func: Callable):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            if not degradation_manager.is_feature_available(feature):
                if fallback:
                    logger.info(f"功能 {feature} 不可用，使用降级方案")
                    return await fallback(*args, **kwargs) if asyncio.iscoroutinefunction(fallback) else fallback(*args, **kwargs)
                else:
                    raise FeatureUnavailableException(f"功能 {feature} 暂时不可用")

            return await func(*args, **kwargs) if asyncio.iscoroutinefunction(func) else func(*args, **kwargs)

        return wrapper
    return decorator

class FeatureUnavailableException(Exception):
    """功能不可用异常"""
    pass

# 使用降级机制
@with_degradation("advanced_analysis", fallback=lambda state: {"result": "基础分析结果"})
async def advanced_analysis_node(state: Dict[str, Any]) -> Dict[str, Any]:
    """高级分析节点"""
    # 复杂的分析逻辑
    return {"result": "高级分析结果"}

@with_degradation("external_api")
async def external_api_node(state: Dict[str, Any]) -> Dict[str, Any]:
    """外部 API 调用节点"""
    # 外部 API 调用
    return {"api_result": "外部数据"}

# 健康检查和自动降级
class HealthChecker:
    """健康检查器"""

    def __init__(self):
        self.checks = {}

    async def check_service_health(self):
        """检查服务健康状态"""
        failed_services = []

        # 检查数据库连接
        try:
            await self._check_database()
        except Exception as e:
            failed_services.append("database")
            logger.error(f"数据库健康检查失败: {e}")

        # 检查外部 API
        try:
            await self._check_external_apis()
        except Exception as e:
            failed_services.append("external_api")
            logger.error(f"外部 API 健康检查失败: {e}")

        # 根据失败服务进行降级
        if len(failed_services) >= 2:
            degradation_manager.degrade_service(
                ServiceLevel.MINIMAL,
                {"advanced_analysis", "external_api"}
            )
        elif failed_services:
            degradation_manager.degrade_service(
                ServiceLevel.DEGRADED,
                set(failed_services)
            )
        else:
            degradation_manager.restore_service()

    async def _check_database(self):
        """检查数据库"""
        # 数据库健康检查逻辑
        pass

    async def _check_external_apis(self):
        """检查外部 API"""
        # 外部 API 健康检查逻辑
        pass

health_checker = HealthChecker()

# 定期健康检查
async def periodic_health_check():
    """定期健康检查"""
    while True:
        try:
            await health_checker.check_service_health()
        except Exception as e:
            logger.error(f"健康检查失败: {e}")

        await asyncio.sleep(30)  # 每30秒检查一次
```

## 数据一致性

### 分布式锁
```python
# distributed_lock.py
import asyncio
import time
import uuid
from typing import Optional

class DistributedLock:
    """基于 Redis 的分布式锁"""

    def __init__(self, redis_client, key: str, timeout: int = 30):
        self.redis = redis_client
        self.key = f"lock:{key}"
        self.timeout = timeout
        self.identifier = str(uuid.uuid4())

    async def acquire(self, blocking: bool = True, timeout: Optional[int] = None) -> bool:
        """获取锁"""
        end_time = time.time() + (timeout or self.timeout)

        while True:
            # 尝试设置锁
            result = await self.redis.set(
                self.key,
                self.identifier,
                ex=self.timeout,
                nx=True  # 只在键不存在时设置
            )

            if result:
                return True

            if not blocking or time.time() > end_time:
                return False

            await asyncio.sleep(0.1)

    async def release(self) -> bool:
        """释放锁"""
        # 使用 Lua 脚本确保原子性
        lua_script = """
        if redis.call('get', KEYS[1]) == ARGV[1] then
            return redis.call('del', KEYS[1])
        else
            return 0
        end
        """

        result = await self.redis.eval(
            lua_script,
            1,
            self.key,
            self.identifier
        )

        return bool(result)

    async def __aenter__(self):
        """异步上下文管理器入口"""
        if not await self.acquire():
            raise LockAcquisitionError(f"无法获取锁: {self.key}")
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """异步上下文管理器出口"""
        await self.release()

class LockAcquisitionError(Exception):
    """锁获取失败异常"""
    pass

# 使用分布式锁
async def atomic_operation(user_id: str, redis_client):
    """原子操作示例"""
    lock = DistributedLock(redis_client, f"user:{user_id}")

    async with lock:
        # 原子操作逻辑
        current_value = await redis_client.get(f"user:{user_id}:balance")
        current_value = int(current_value) if current_value else 0

        # 模拟业务逻辑
        new_value = current_value + 100

        await redis_client.set(f"user:{user_id}:balance", new_value)

        logger.info(f"用户 {user_id} 余额更新: {current_value} -> {new_value}")
```

### 事务处理
```python
# transaction_manager.py
import asyncio
from typing import List, Callable, Any, Dict
from contextlib import asynccontextmanager

class TransactionManager:
    """事务管理器"""

    def __init__(self, postgres_pool):
        self.postgres_pool = postgres_pool

    @asynccontextmanager
    async def transaction(self):
        """事务上下文管理器"""
        connection = await self.postgres_pool.acquire()
        transaction = connection.transaction()

        try:
            await transaction.start()
            yield connection
            await transaction.commit()
        except Exception:
            await transaction.rollback()
            raise
        finally:
            await self.postgres_pool.release(connection)

    async def execute_in_transaction(
        self,
        operations: List[Callable],
        *args,
        **kwargs
    ) -> List[Any]:
        """在事务中执行多个操作"""
        async with self.transaction() as conn:
            results = []
            for operation in operations:
                result = await operation(conn, *args, **kwargs)
                results.append(result)
            return results

# 使用事务管理器
transaction_manager = TransactionManager(postgres_pool)

async def update_user_and_log(user_id: str, changes: Dict[str, Any]):
    """更新用户信息并记录日志"""

    async def update_user(conn, user_id: str, changes: Dict[str, Any]):
        """更新用户信息"""
        set_clause = ", ".join([f"{k} = ${i+2}" for i, k in enumerate(changes.keys())])
        query = f"UPDATE users SET {set_clause} WHERE id = $1"
        values = [user_id] + list(changes.values())
        await conn.execute(query, *values)

    async def log_change(conn, user_id: str, changes: Dict[str, Any]):
        """记录变更日志"""
        await conn.execute(
            "INSERT INTO user_change_log (user_id, changes, timestamp) VALUES ($1, $2, NOW())",
            user_id,
            json.dumps(changes)
        )

    try:
        await transaction_manager.execute_in_transaction(
            [update_user, log_change],
            user_id,
            changes
        )
        logger.info(f"用户 {user_id} 信息更新成功")
    except Exception as e:
        logger.error(f"用户 {user_id} 信息更新失败: {e}")
        raise
```

## 容灾设计

### 多区域部署
```yaml
# multi-region-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: langgraph-us-east
  namespace: langgraph
  labels:
    region: us-east-1
spec:
  replicas: 3
  selector:
    matchLabels:
      app: langgraph-server
      region: us-east-1
  template:
    metadata:
      labels:
        app: langgraph-server
        region: us-east-1
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: topology.kubernetes.io/zone
                operator: In
                values:
                - us-east-1a
                - us-east-1b
                - us-east-1c
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - langgraph-server
              topologyKey: kubernetes.io/hostname
      containers:
      - name: langgraph-server
        image: langgraph/langgraph-server:latest
        env:
        - name: REGION
          value: "us-east-1"
        - name: POSTGRES_URL
          value: "postgresql://user:pass@postgres-us-east:5432/langgraph"
        - name: REDIS_URL
          value: "redis://redis-us-east:6379"
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"

---
# 西海岸部署
apiVersion: apps/v1
kind: Deployment
metadata:
  name: langgraph-us-west
  namespace: langgraph
  labels:
    region: us-west-2
spec:
  replicas: 3
  selector:
    matchLabels:
      app: langgraph-server
      region: us-west-2
  template:
    metadata:
      labels:
        app: langgraph-server
        region: us-west-2
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: topology.kubernetes.io/zone
                operator: In
                values:
                - us-west-2a
                - us-west-2b
                - us-west-2c
      containers:
      - name: langgraph-server
        image: langgraph/langgraph-server:latest
        env:
        - name: REGION
          value: "us-west-2"
        - name: POSTGRES_URL
          value: "postgresql://user:pass@postgres-us-west:5432/langgraph"
        - name: REDIS_URL
          value: "redis://redis-us-west:6379"
```

### 数据备份与恢复
```bash
#!/bin/bash
# backup-restore.sh

# 配置参数
BACKUP_DIR="/backups"
S3_BUCKET="langgraph-backups"
RETENTION_DAYS=30

# 备份函数
backup_postgres() {
    local timestamp=$(date +%Y%m%d_%H%M%S)
    local backup_file="${BACKUP_DIR}/postgres_${timestamp}.sql"

    echo "开始 PostgreSQL 备份..."
    pg_dump $POSTGRES_URL > $backup_file

    # 压缩备份文件
    gzip $backup_file

    # 上传到 S3
    aws s3 cp ${backup_file}.gz s3://${S3_BUCKET}/postgres/

    echo "PostgreSQL 备份完成: ${backup_file}.gz"
}

backup_redis() {
    local timestamp=$(date +%Y%m%d_%H%M%S)
    local backup_file="${BACKUP_DIR}/redis_${timestamp}.rdb"

    echo "开始 Redis 备份..."
    redis-cli --rdb $backup_file

    # 上传到 S3
    aws s3 cp $backup_file s3://${S3_BUCKET}/redis/

    echo "Redis 备份完成: $backup_file"
}

# 恢复函数
restore_postgres() {
    local backup_file=$1

    if [[ -z "$backup_file" ]]; then
        echo "请指定备份文件"
        exit 1
    fi

    echo "开始恢复 PostgreSQL: $backup_file"

    # 从 S3 下载
    if [[ $backup_file =~ ^s3:// ]]; then
        aws s3 cp $backup_file /tmp/restore.sql.gz
        backup_file="/tmp/restore.sql.gz"
    fi

    # 解压并恢复
    if [[ $backup_file =~ \.gz$ ]]; then
        gunzip -c $backup_file | psql $POSTGRES_URL
    else
        psql $POSTGRES_URL < $backup_file
    fi

    echo "PostgreSQL 恢复完成"
}

# 清理旧备份
cleanup_old_backups() {
    echo "清理 ${RETENTION_DAYS} 天前的备份..."

    # 清理本地备份
    find $BACKUP_DIR -name "*.gz" -mtime +$RETENTION_DAYS -delete
    find $BACKUP_DIR -name "*.rdb" -mtime +$RETENTION_DAYS -delete

    # 清理 S3 备份
    aws s3 ls s3://${S3_BUCKET}/postgres/ | while read -r line; do
        createDate=$(echo $line | awk '{print $1" "$2}')
        createDate=$(date -d "$createDate" +%s)
        olderThan=$(date -d "$RETENTION_DAYS days ago" +%s)

        if [[ $createDate -lt $olderThan ]]; then
            fileName=$(echo $line | awk '{print $4}')
            aws s3 rm s3://${S3_BUCKET}/postgres/$fileName
        fi
    done

    echo "清理完成"
}

# 主函数
main() {
    case "$1" in
        backup)
            backup_postgres
            backup_redis
            cleanup_old_backups
            ;;
        restore-postgres)
            restore_postgres "$2"
            ;;
        cleanup)
            cleanup_old_backups
            ;;
        *)
            echo "用法: $0 {backup|restore-postgres <backup_file>|cleanup}"
            exit 1
            ;;
    esac
}

main "$@"
```

### 故障转移
```python
# failover_manager.py
import asyncio
import aiohttp
from typing import List, Dict, Optional
from dataclasses import dataclass

@dataclass
class ServiceEndpoint:
    """服务端点"""
    url: str
    region: str
    priority: int
    healthy: bool = True

class FailoverManager:
    """故障转移管理器"""

    def __init__(self):
        self.endpoints: List[ServiceEndpoint] = []
        self.current_endpoint: Optional[ServiceEndpoint] = None

    def add_endpoint(self, endpoint: ServiceEndpoint):
        """添加服务端点"""
        self.endpoints.append(endpoint)
        self.endpoints.sort(key=lambda x: x.priority)

        if not self.current_endpoint:
            self.current_endpoint = endpoint

    async def health_check(self, endpoint: ServiceEndpoint) -> bool:
        """健康检查"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    f"{endpoint.url}/health",
                    timeout=aiohttp.ClientTimeout(total=5)
                ) as response:
                    return response.status == 200
        except Exception:
            return False

    async def monitor_endpoints(self):
        """监控所有端点"""
        while True:
            tasks = [
                self.health_check(endpoint)
                for endpoint in self.endpoints
            ]

            results = await asyncio.gather(*tasks, return_exceptions=True)

            # 更新健康状态
            for endpoint, result in zip(self.endpoints, results):
                endpoint.healthy = isinstance(result, bool) and result

            # 检查当前端点是否健康
            if self.current_endpoint and not self.current_endpoint.healthy:
                await self.failover()

            await asyncio.sleep(30)  # 每30秒检查一次

    async def failover(self):
        """故障转移"""
        # 找到下一个健康的端点
        for endpoint in self.endpoints:
            if endpoint.healthy and endpoint != self.current_endpoint:
                old_endpoint = self.current_endpoint
                self.current_endpoint = endpoint

                logger.warning(
                    f"故障转移: {old_endpoint.url} -> {endpoint.url}"
                )
                break
        else:
            logger.critical("所有服务端点都不可用！")

    async def make_request(self, path: str, **kwargs) -> Dict:
        """发起请求（带故障转移）"""
        if not self.current_endpoint:
            raise Exception("没有可用的服务端点")

        try:
            async with aiohttp.ClientSession() as session:
                url = f"{self.current_endpoint.url}{path}"
                async with session.request(**kwargs, url=url) as response:
                    return await response.json()

        except Exception as e:
            logger.error(f"请求失败: {e}")

            # 尝试故障转移
            await self.failover()

            if self.current_endpoint:
                # 重试一次
                async with aiohttp.ClientSession() as session:
                    url = f"{self.current_endpoint.url}{path}"
                    async with session.request(**kwargs, url=url) as response:
                        return await response.json()

            raise

# 使用故障转移管理器
failover_manager = FailoverManager()

# 添加服务端点
failover_manager.add_endpoint(
    ServiceEndpoint("https://api-us-east.example.com", "us-east-1", 1)
)
failover_manager.add_endpoint(
    ServiceEndpoint("https://api-us-west.example.com", "us-west-2", 2)
)

# 启动监控
asyncio.create_task(failover_manager.monitor_endpoints())
```

## 性能优化

### 缓存策略
```python
# caching_strategy.py
import asyncio
import json
import hashlib
from typing import Any, Optional, Dict, Callable
from functools import wraps

class MultiLevelCache:
    """多级缓存"""

    def __init__(self, redis_client, local_cache_size: int = 1000):
        self.redis = redis_client
        self.local_cache: Dict[str, Any] = {}
        self.local_cache_size = local_cache_size

    def _generate_key(self, *args, **kwargs) -> str:
        """生成缓存键"""
        key_data = str(args) + str(sorted(kwargs.items()))
        return hashlib.md5(key_data.encode()).hexdigest()

    async def get(self, key: str) -> Optional[Any]:
        """获取缓存值"""
        # 1. 检查本地缓存
        if key in self.local_cache:
            return self.local_cache[key]

        # 2. 检查 Redis 缓存
        redis_value = await self.redis.get(key)
        if redis_value:
            value = json.loads(redis_value)
            # 将值放入本地缓存
            self._set_local_cache(key, value)
            return value

        return None

    async def set(self, key: str, value: Any, ttl: int = 3600):
        """设置缓存值"""
        # 设置 Redis 缓存
        await self.redis.setex(key, ttl, json.dumps(value))

        # 设置本地缓存
        self._set_local_cache(key, value)

    def _set_local_cache(self, key: str, value: Any):
        """设置本地缓存"""
        # 如果缓存已满，删除最旧的条目
        if len(self.local_cache) >= self.local_cache_size:
            oldest_key = next(iter(self.local_cache))
            del self.local_cache[oldest_key]

        self.local_cache[key] = value

    async def delete(self, key: str):
        """删除缓存"""
        # 删除本地缓存
        self.local_cache.pop(key, None)

        # 删除 Redis 缓存
        await self.redis.delete(key)

# 缓存装饰器
def cached(cache: MultiLevelCache, ttl: int = 3600):
    """缓存装饰器"""
    def decorator(func: Callable):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # 生成缓存键
            cache_key = f"{func.__name__}:{cache._generate_key(*args, **kwargs)}"

            # 尝试从缓存获取
            cached_result = await cache.get(cache_key)
            if cached_result is not None:
                return cached_result

            # 执行函数
            result = await func(*args, **kwargs) if asyncio.iscoroutinefunction(func) else func(*args, **kwargs)

            # 缓存结果
            await cache.set(cache_key, result, ttl)

            return result

        return wrapper
    return decorator

# 使用缓存
cache = MultiLevelCache(redis_client)

@cached(cache, ttl=1800)
async def expensive_computation(data: str) -> str:
    """耗时计算"""
    await asyncio.sleep(2)  # 模拟耗时操作
    return f"计算结果: {data}"
```

## 监控指标

### 关键性能指标 (KPI)
```python
# kpi_metrics.py
from prometheus_client import Counter, Histogram, Gauge, Summary

# 业务指标
REQUEST_TOTAL = Counter(
    'langgraph_requests_total',
    'Total requests',
    ['method', 'endpoint', 'status']
)

REQUEST_DURATION = Histogram(
    'langgraph_request_duration_seconds',
    'Request duration',
    ['endpoint']
)

ACTIVE_USERS = Gauge(
    'langgraph_active_users',
    'Number of active users'
)

# 扩展性指标
PODS_RUNNING = Gauge(
    'langgraph_pods_running',
    'Number of running pods'
)

HORIZONTAL_SCALE_EVENTS = Counter(
    'langgraph_horizontal_scale_events_total',
    'Horizontal scaling events',
    ['direction']  # up, down
)

# 韧性指标
CIRCUIT_BREAKER_STATE = Gauge(
    'langgraph_circuit_breaker_state',
    'Circuit breaker state',
    ['name', 'state']  # closed=0, open=1, half_open=0.5
)

RETRY_ATTEMPTS = Counter(
    'langgraph_retry_attempts_total',
    'Retry attempts',
    ['function', 'attempt']
)

FAILOVER_EVENTS = Counter(
    'langgraph_failover_events_total',
    'Failover events',
    ['from_region', 'to_region']
)

# 数据库指标
DB_CONNECTIONS_ACTIVE = Gauge(
    'langgraph_db_connections_active',
    'Active database connections'
)

DB_QUERY_DURATION = Histogram(
    'langgraph_db_query_duration_seconds',
    'Database query duration',
    ['query_type']
)

def track_scaling_event(direction: str):
    """记录扩展事件"""
    HORIZONTAL_SCALE_EVENTS.labels(direction=direction).inc()

def track_circuit_breaker_state(name: str, state: str):
    """记录断路器状态"""
    state_value = {
        'closed': 0,
        'open': 1,
        'half_open': 0.5
    }[state]
    CIRCUIT_BREAKER_STATE.labels(name=name, state=state).set(state_value)

def track_failover(from_region: str, to_region: str):
    """记录故障转移"""
    FAILOVER_EVENTS.labels(
        from_region=from_region,
        to_region=to_region
    ).inc()
```

## 下一步

- 🐳 学习 [06-Docker容器化](./06-Docker容器化.md) - 容器化部署方案
- 🔐 探索 [07-认证与授权](./07-认证与授权.md) - 企业级安全控制
- 🔗 了解 [08-Webhooks集成](./08-Webhooks集成.md) - 事件驱动架构

## 相关链接

- [LangGraph Platform 扩展性](https://langchain-ai.github.io/langgraph/concepts/scalability_and_resilience/)
- [Kubernetes 自动扩展](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)
- [分布式系统模式](https://microservices.io/patterns/)
- [可观测性最佳实践](https://sre.google/books/)