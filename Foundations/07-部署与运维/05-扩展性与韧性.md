# 05-æ‰©å±•æ€§ä¸éŸ§æ€§

## æ¦‚è¿°

æ‰©å±•æ€§ä¸éŸ§æ€§æ˜¯ç”Ÿäº§ç¯å¢ƒä¸­ LangGraph åº”ç”¨çš„æ ¸å¿ƒè¦æ±‚ã€‚é€šè¿‡æ°´å¹³æ‰©å±•ã€è´Ÿè½½å‡è¡¡ã€æ•…éšœæ¢å¤ã€æ•°æ®ä¸€è‡´æ€§å’Œå®¹ç¾è®¾è®¡ï¼Œç¡®ä¿ç³»ç»Ÿåœ¨é«˜è´Ÿè½½å’Œå¼‚å¸¸æƒ…å†µä¸‹çš„ç¨³å®šè¿è¡Œã€‚

## ç³»ç»Ÿæ¶æ„è®¾è®¡

### é«˜å¯ç”¨æ¶æ„å›¾
```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Load Balancer â”‚
                    â”‚    (HAProxy)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚            â”‚            â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”
         â”‚LangGraph  â”‚ â”‚LangGraph â”‚ â”‚LangGraph â”‚
         â”‚Server 1   â”‚ â”‚Server 2  â”‚ â”‚Server 3  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
                 â”‚            â”‚            â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚          Message Queue        â”‚
              â”‚         (Redis Cluster)       â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚         Database Pool         â”‚
              â”‚    (PostgreSQL Cluster)      â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒè®¾è®¡åŸåˆ™
1. **æ— å•ç‚¹æ•…éšœ**: æ‰€æœ‰ç»„ä»¶éƒ½æœ‰å†—ä½™
2. **æ°´å¹³æ‰©å±•**: æ”¯æŒåŠ¨æ€æ·»åŠ å®ä¾‹
3. **ä¼˜é›…é™çº§**: éƒ¨åˆ†åŠŸèƒ½å¤±æ•ˆæ—¶ä¿æŒæ ¸å¿ƒæœåŠ¡
4. **å¿«é€Ÿæ¢å¤**: è‡ªåŠ¨æ•…éšœæ£€æµ‹å’Œæ¢å¤
5. **æ•°æ®ä¸€è‡´æ€§**: ä¿è¯åˆ†å¸ƒå¼æ•°æ®çš„ä¸€è‡´æ€§

## æ°´å¹³æ‰©å±•

### Kubernetes è‡ªåŠ¨æ‰©å±•

#### HPA (Horizontal Pod Autoscaler)
```yaml
# hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: langgraph-hpa
  namespace: langgraph
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: langgraph-server
  minReplicas: 2
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "1000"
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 120
      policies:
      - type: Percent
        value: 100
        periodSeconds: 60
      - type: Pods
        value: 4
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
```

#### VPA (Vertical Pod Autoscaler)
```yaml
# vpa.yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: langgraph-vpa
  namespace: langgraph
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: langgraph-server
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: langgraph-server
      minAllowed:
        cpu: 100m
        memory: 128Mi
      maxAllowed:
        cpu: 2
        memory: 4Gi
      controlledResources: ["cpu", "memory"]
```

### åº”ç”¨å±‚æ‰©å±•

#### æ— çŠ¶æ€è®¾è®¡
```python
# stateless_agent.py
from typing import Dict, Any
import uuid
from langgraph.graph import StateGraph
from langgraph.checkpoint import MemorySaver
import redis

class StatelessAgent:
    """æ— çŠ¶æ€ Agent è®¾è®¡"""

    def __init__(self, redis_url: str):
        self.redis_client = redis.from_url(redis_url)
        self.graph = self._create_graph()

    def _create_graph(self) -> StateGraph:
        """åˆ›å»ºæ— çŠ¶æ€å›¾"""
        graph = StateGraph(AgentState)

        # æ‰€æœ‰çŠ¶æ€éƒ½é€šè¿‡å¤–éƒ¨å­˜å‚¨ç®¡ç†
        graph.add_node("load_context", self.load_context)
        graph.add_node("process", self.process_request)
        graph.add_node("save_context", self.save_context)

        graph.set_entry_point("load_context")
        graph.add_edge("load_context", "process")
        graph.add_edge("process", "save_context")
        graph.set_finish_point("save_context")

        return graph.compile()

    def load_context(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """ä»å¤–éƒ¨å­˜å‚¨åŠ è½½ä¸Šä¸‹æ–‡"""
        session_id = state.get("session_id")
        if session_id:
            context = self.redis_client.get(f"context:{session_id}")
            if context:
                state.update(json.loads(context))

        return state

    def save_context(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """ä¿å­˜ä¸Šä¸‹æ–‡åˆ°å¤–éƒ¨å­˜å‚¨"""
        session_id = state.get("session_id")
        if session_id:
            # åªä¿å­˜å¿…è¦çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
            context = {
                "last_response": state.get("response"),
                "conversation_history": state.get("messages", [])[-10:],  # åªä¿ç•™æœ€è¿‘10æ¡
                "user_preferences": state.get("user_preferences", {})
            }
            self.redis_client.setex(
                f"context:{session_id}",
                3600,  # 1å°æ—¶è¿‡æœŸ
                json.dumps(context)
            )

        return state

    def process_request(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†è¯·æ±‚"""
        # æ— çŠ¶æ€å¤„ç†é€»è¾‘
        user_input = state.get("input", "")
        response = f"å¤„ç†ç»“æœ: {user_input}"

        return {
            "response": response,
            "timestamp": time.time()
        }

    def invoke(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """è°ƒç”¨ Agent"""
        # ä¸ºæ¯ä¸ªè¯·æ±‚ç”Ÿæˆå”¯ä¸€æ ‡è¯†
        if "request_id" not in input_data:
            input_data["request_id"] = str(uuid.uuid4())

        return self.graph.invoke(input_data)
```

#### è¿æ¥æ± ç®¡ç†
```python
# connection_pool.py
from typing import Dict, Any, Optional
import asyncio
import aioredis
import asyncpg
from contextlib import asynccontextmanager

class ConnectionPoolManager:
    """è¿æ¥æ± ç®¡ç†å™¨"""

    def __init__(self):
        self.redis_pool: Optional[aioredis.ConnectionPool] = None
        self.postgres_pool: Optional[asyncpg.Pool] = None

    async def initialize(
        self,
        redis_url: str,
        postgres_url: str,
        max_connections: int = 20
    ):
        """åˆå§‹åŒ–è¿æ¥æ± """
        # Redis è¿æ¥æ± 
        self.redis_pool = aioredis.ConnectionPool.from_url(
            redis_url,
            max_connections=max_connections,
            socket_keepalive=True,
            socket_keepalive_options={
                "TCP_KEEPIDLE": 30,
                "TCP_KEEPINTVL": 5,
                "TCP_KEEPCNT": 5
            }
        )

        # PostgreSQL è¿æ¥æ± 
        self.postgres_pool = await asyncpg.create_pool(
            postgres_url,
            min_size=5,
            max_size=max_connections,
            command_timeout=60,
            server_settings={
                "application_name": "langgraph_agent",
                "tcp_keepalives_idle": "300",
                "tcp_keepalives_interval": "30",
                "tcp_keepalives_count": "3"
            }
        )

    async def close(self):
        """å…³é—­è¿æ¥æ± """
        if self.redis_pool:
            await self.redis_pool.disconnect()

        if self.postgres_pool:
            await self.postgres_pool.close()

    @asynccontextmanager
    async def get_redis_connection(self):
        """è·å– Redis è¿æ¥"""
        connection = None
        try:
            connection = aioredis.Redis(connection_pool=self.redis_pool)
            yield connection
        finally:
            if connection:
                await connection.close()

    @asynccontextmanager
    async def get_postgres_connection(self):
        """è·å– PostgreSQL è¿æ¥"""
        connection = None
        try:
            connection = await self.postgres_pool.acquire()
            yield connection
        finally:
            if connection:
                await self.postgres_pool.release(connection)

# å…¨å±€è¿æ¥æ± å®ä¾‹
pool_manager = ConnectionPoolManager()

class ScalableAgent:
    """å¯æ‰©å±•çš„ Agent"""

    async def process_with_db(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """ä½¿ç”¨æ•°æ®åº“è¿æ¥æ± å¤„ç†è¯·æ±‚"""
        async with pool_manager.get_postgres_connection() as conn:
            # æ•°æ®åº“æ“ä½œ
            result = await conn.fetchrow(
                "SELECT * FROM user_preferences WHERE user_id = $1",
                state.get("user_id")
            )

            if result:
                state["user_preferences"] = dict(result)

        async with pool_manager.get_redis_connection() as redis:
            # ç¼“å­˜æ“ä½œ
            cache_key = f"processed:{state.get('request_id')}"
            await redis.setex(cache_key, 300, "processed")

        return state
```

## è´Ÿè½½å‡è¡¡

### HAProxy é…ç½®
```
# haproxy.cfg
global
    daemon
    maxconn 4096
    log stdout local0 info

defaults
    mode http
    timeout connect 5000ms
    timeout client 50000ms
    timeout server 50000ms
    option httplog
    option dontlognull
    option redispatch
    retries 3

# å‰ç«¯é…ç½®
frontend langgraph_frontend
    bind *:80
    bind *:443 ssl crt /etc/ssl/certs/langgraph.pem
    redirect scheme https if !{ ssl_fc }

    # å¥åº·æ£€æŸ¥
    acl health_check path_beg /health

    # è·¯ç”±è§„åˆ™
    use_backend langgraph_backend if !health_check
    use_backend health_backend if health_check

# åç«¯é…ç½®
backend langgraph_backend
    balance roundrobin
    option httpchk GET /health
    http-check expect status 200

    # æœåŠ¡å™¨åˆ—è¡¨
    server langgraph1 langgraph-server-1:8000 check inter 30s rise 2 fall 3
    server langgraph2 langgraph-server-2:8000 check inter 30s rise 2 fall 3
    server langgraph3 langgraph-server-3:8000 check inter 30s rise 2 fall 3

backend health_backend
    balance roundrobin
    server health1 langgraph-server-1:8000 check
    server health2 langgraph-server-2:8000 check

# ç»Ÿè®¡é¡µé¢
stats enable
stats uri /stats
stats realm HAProxy\ Statistics
stats auth admin:admin123
```

### Nginx è´Ÿè½½å‡è¡¡
```nginx
# nginx.conf
upstream langgraph_backend {
    least_conn;
    server langgraph-server-1:8000 max_fails=3 fail_timeout=30s;
    server langgraph-server-2:8000 max_fails=3 fail_timeout=30s;
    server langgraph-server-3:8000 max_fails=3 fail_timeout=30s;
}

server {
    listen 80;
    listen 443 ssl http2;
    server_name langgraph.example.com;

    # SSL é…ç½®
    ssl_certificate /etc/ssl/certs/langgraph.crt;
    ssl_certificate_key /etc/ssl/private/langgraph.key;
    ssl_protocols TLSv1.2 TLSv1.3;

    # é™æµé…ç½®
    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;
    limit_req zone=api burst=20 nodelay;

    # å¥åº·æ£€æŸ¥
    location /health {
        access_log off;
        proxy_pass http://langgraph_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }

    # ä¸»è¦æœåŠ¡
    location / {
        proxy_pass http://langgraph_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;

        # è¶…æ—¶é…ç½®
        proxy_connect_timeout 60s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;

        # ç¼“å†²é…ç½®
        proxy_buffering on;
        proxy_buffer_size 128k;
        proxy_buffers 4 256k;
        proxy_busy_buffers_size 256k;
    }

    # WebSocket æ”¯æŒ
    location /ws {
        proxy_pass http://langgraph_backend;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

## æ•…éšœæ¢å¤

### æ–­è·¯å™¨æ¨¡å¼
```python
# circuit_breaker.py
import time
import asyncio
from enum import Enum
from typing import Callable, Any, Optional
from dataclasses import dataclass

class CircuitState(Enum):
    CLOSED = "closed"      # æ­£å¸¸çŠ¶æ€
    OPEN = "open"          # ç†”æ–­çŠ¶æ€
    HALF_OPEN = "half_open"  # åŠå¼€çŠ¶æ€

@dataclass
class CircuitBreakerConfig:
    failure_threshold: int = 5      # å¤±è´¥é˜ˆå€¼
    recovery_timeout: int = 60      # æ¢å¤è¶…æ—¶ï¼ˆç§’ï¼‰
    expected_exception: type = Exception  # æœŸæœ›çš„å¼‚å¸¸ç±»å‹
    name: str = "default"

class CircuitBreaker:
    """æ–­è·¯å™¨å®ç°"""

    def __init__(self, config: CircuitBreakerConfig):
        self.config = config
        self.failure_count = 0
        self.last_failure_time: Optional[float] = None
        self.state = CircuitState.CLOSED

    async def call(self, func: Callable, *args, **kwargs) -> Any:
        """æ‰§è¡Œå‡½æ•°è°ƒç”¨"""
        if self.state == CircuitState.OPEN:
            if self._should_attempt_reset():
                self.state = CircuitState.HALF_OPEN
            else:
                raise CircuitOpenException(
                    f"æ–­è·¯å™¨ {self.config.name} å¤„äºå¼€å¯çŠ¶æ€"
                )

        try:
            result = await func(*args, **kwargs) if asyncio.iscoroutinefunction(func) else func(*args, **kwargs)
            self._on_success()
            return result

        except self.config.expected_exception as e:
            self._on_failure()
            raise

    def _should_attempt_reset(self) -> bool:
        """æ˜¯å¦åº”è¯¥å°è¯•é‡ç½®"""
        return (
            self.last_failure_time is not None and
            time.time() - self.last_failure_time >= self.config.recovery_timeout
        )

    def _on_success(self):
        """æˆåŠŸå›è°ƒ"""
        self.failure_count = 0
        self.state = CircuitState.CLOSED

    def _on_failure(self):
        """å¤±è´¥å›è°ƒ"""
        self.failure_count += 1
        self.last_failure_time = time.time()

        if self.failure_count >= self.config.failure_threshold:
            self.state = CircuitState.OPEN

class CircuitOpenException(Exception):
    """æ–­è·¯å™¨å¼€å¯å¼‚å¸¸"""
    pass

# ä½¿ç”¨æ–­è·¯å™¨
llm_circuit_breaker = CircuitBreaker(
    CircuitBreakerConfig(
        failure_threshold=3,
        recovery_timeout=30,
        name="llm_service"
    )
)

async def call_llm_with_circuit_breaker(prompt: str) -> str:
    """ä½¿ç”¨æ–­è·¯å™¨è°ƒç”¨ LLM"""
    async def llm_call():
        # æ¨¡æ‹Ÿ LLM è°ƒç”¨
        if random.random() < 0.3:  # 30% å¤±è´¥ç‡
            raise Exception("LLM æœåŠ¡ä¸å¯ç”¨")
        return f"LLM å“åº”: {prompt}"

    try:
        return await llm_circuit_breaker.call(llm_call)
    except CircuitOpenException:
        # æ–­è·¯å™¨å¼€å¯æ—¶çš„é™çº§å¤„ç†
        return "æŠ±æ­‰ï¼ŒæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•"
```

### é‡è¯•æœºåˆ¶
```python
# retry_mechanism.py
import asyncio
import random
from typing import Callable, Any, List, Type
from functools import wraps

class RetryConfig:
    """é‡è¯•é…ç½®"""

    def __init__(
        self,
        max_attempts: int = 3,
        base_delay: float = 1.0,
        max_delay: float = 60.0,
        exponential_base: float = 2.0,
        jitter: bool = True,
        retriable_exceptions: List[Type[Exception]] = None
    ):
        self.max_attempts = max_attempts
        self.base_delay = base_delay
        self.max_delay = max_delay
        self.exponential_base = exponential_base
        self.jitter = jitter
        self.retriable_exceptions = retriable_exceptions or [Exception]

def async_retry(config: RetryConfig):
    """å¼‚æ­¥é‡è¯•è£…é¥°å™¨"""
    def decorator(func: Callable):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            last_exception = None

            for attempt in range(config.max_attempts):
                try:
                    return await func(*args, **kwargs)
                except Exception as e:
                    last_exception = e

                    # æ£€æŸ¥æ˜¯å¦ä¸ºå¯é‡è¯•å¼‚å¸¸
                    if not any(isinstance(e, exc_type) for exc_type in config.retriable_exceptions):
                        raise

                    # æœ€åä¸€æ¬¡å°è¯•ï¼Œä¸å†ç­‰å¾…
                    if attempt == config.max_attempts - 1:
                        break

                    # è®¡ç®—å»¶è¿Ÿæ—¶é—´
                    delay = min(
                        config.base_delay * (config.exponential_base ** attempt),
                        config.max_delay
                    )

                    # æ·»åŠ æŠ–åŠ¨
                    if config.jitter:
                        delay *= (0.5 + random.random() * 0.5)

                    logger.warning(
                        f"å‡½æ•° {func.__name__} ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥ï¼Œ"
                        f"{delay:.2f}ç§’åé‡è¯•: {e}"
                    )

                    await asyncio.sleep(delay)

            # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
            raise last_exception

        return wrapper
    return decorator

# ä½¿ç”¨é‡è¯•æœºåˆ¶
@async_retry(RetryConfig(
    max_attempts=3,
    base_delay=1.0,
    exponential_base=2.0,
    retriable_exceptions=[ConnectionError, TimeoutError]
))
async def unreliable_api_call(data: str) -> str:
    """ä¸å¯é çš„ API è°ƒç”¨"""
    if random.random() < 0.5:  # 50% å¤±è´¥ç‡
        raise ConnectionError("ç½‘ç»œè¿æ¥å¤±è´¥")

    return f"API å“åº”: {data}"
```

### ä¼˜é›…é™çº§
```python
# graceful_degradation.py
from typing import Dict, Any, Optional, Callable
from enum import Enum

class ServiceLevel(Enum):
    FULL = "full"           # å®Œæ•´æœåŠ¡
    DEGRADED = "degraded"   # é™çº§æœåŠ¡
    MINIMAL = "minimal"     # æœ€å°æœåŠ¡

class DegradationManager:
    """é™çº§ç®¡ç†å™¨"""

    def __init__(self):
        self.service_level = ServiceLevel.FULL
        self.degraded_features = set()

    def degrade_service(self, level: ServiceLevel, features: set = None):
        """é™çº§æœåŠ¡"""
        self.service_level = level
        if features:
            self.degraded_features.update(features)

        logger.warning(f"æœåŠ¡é™çº§åˆ° {level.value} çº§åˆ«")

    def restore_service(self):
        """æ¢å¤æœåŠ¡"""
        self.service_level = ServiceLevel.FULL
        self.degraded_features.clear()
        logger.info("æœåŠ¡å·²æ¢å¤åˆ°å®Œæ•´çº§åˆ«")

    def is_feature_available(self, feature: str) -> bool:
        """æ£€æŸ¥åŠŸèƒ½æ˜¯å¦å¯ç”¨"""
        return feature not in self.degraded_features

degradation_manager = DegradationManager()

def with_degradation(
    feature: str,
    fallback: Optional[Callable] = None
):
    """é™çº§è£…é¥°å™¨"""
    def decorator(func: Callable):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            if not degradation_manager.is_feature_available(feature):
                if fallback:
                    logger.info(f"åŠŸèƒ½ {feature} ä¸å¯ç”¨ï¼Œä½¿ç”¨é™çº§æ–¹æ¡ˆ")
                    return await fallback(*args, **kwargs) if asyncio.iscoroutinefunction(fallback) else fallback(*args, **kwargs)
                else:
                    raise FeatureUnavailableException(f"åŠŸèƒ½ {feature} æš‚æ—¶ä¸å¯ç”¨")

            return await func(*args, **kwargs) if asyncio.iscoroutinefunction(func) else func(*args, **kwargs)

        return wrapper
    return decorator

class FeatureUnavailableException(Exception):
    """åŠŸèƒ½ä¸å¯ç”¨å¼‚å¸¸"""
    pass

# ä½¿ç”¨é™çº§æœºåˆ¶
@with_degradation("advanced_analysis", fallback=lambda state: {"result": "åŸºç¡€åˆ†æç»“æœ"})
async def advanced_analysis_node(state: Dict[str, Any]) -> Dict[str, Any]:
    """é«˜çº§åˆ†æèŠ‚ç‚¹"""
    # å¤æ‚çš„åˆ†æé€»è¾‘
    return {"result": "é«˜çº§åˆ†æç»“æœ"}

@with_degradation("external_api")
async def external_api_node(state: Dict[str, Any]) -> Dict[str, Any]:
    """å¤–éƒ¨ API è°ƒç”¨èŠ‚ç‚¹"""
    # å¤–éƒ¨ API è°ƒç”¨
    return {"api_result": "å¤–éƒ¨æ•°æ®"}

# å¥åº·æ£€æŸ¥å’Œè‡ªåŠ¨é™çº§
class HealthChecker:
    """å¥åº·æ£€æŸ¥å™¨"""

    def __init__(self):
        self.checks = {}

    async def check_service_health(self):
        """æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€"""
        failed_services = []

        # æ£€æŸ¥æ•°æ®åº“è¿æ¥
        try:
            await self._check_database()
        except Exception as e:
            failed_services.append("database")
            logger.error(f"æ•°æ®åº“å¥åº·æ£€æŸ¥å¤±è´¥: {e}")

        # æ£€æŸ¥å¤–éƒ¨ API
        try:
            await self._check_external_apis()
        except Exception as e:
            failed_services.append("external_api")
            logger.error(f"å¤–éƒ¨ API å¥åº·æ£€æŸ¥å¤±è´¥: {e}")

        # æ ¹æ®å¤±è´¥æœåŠ¡è¿›è¡Œé™çº§
        if len(failed_services) >= 2:
            degradation_manager.degrade_service(
                ServiceLevel.MINIMAL,
                {"advanced_analysis", "external_api"}
            )
        elif failed_services:
            degradation_manager.degrade_service(
                ServiceLevel.DEGRADED,
                set(failed_services)
            )
        else:
            degradation_manager.restore_service()

    async def _check_database(self):
        """æ£€æŸ¥æ•°æ®åº“"""
        # æ•°æ®åº“å¥åº·æ£€æŸ¥é€»è¾‘
        pass

    async def _check_external_apis(self):
        """æ£€æŸ¥å¤–éƒ¨ API"""
        # å¤–éƒ¨ API å¥åº·æ£€æŸ¥é€»è¾‘
        pass

health_checker = HealthChecker()

# å®šæœŸå¥åº·æ£€æŸ¥
async def periodic_health_check():
    """å®šæœŸå¥åº·æ£€æŸ¥"""
    while True:
        try:
            await health_checker.check_service_health()
        except Exception as e:
            logger.error(f"å¥åº·æ£€æŸ¥å¤±è´¥: {e}")

        await asyncio.sleep(30)  # æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡
```

## æ•°æ®ä¸€è‡´æ€§

### åˆ†å¸ƒå¼é”
```python
# distributed_lock.py
import asyncio
import time
import uuid
from typing import Optional

class DistributedLock:
    """åŸºäº Redis çš„åˆ†å¸ƒå¼é”"""

    def __init__(self, redis_client, key: str, timeout: int = 30):
        self.redis = redis_client
        self.key = f"lock:{key}"
        self.timeout = timeout
        self.identifier = str(uuid.uuid4())

    async def acquire(self, blocking: bool = True, timeout: Optional[int] = None) -> bool:
        """è·å–é”"""
        end_time = time.time() + (timeout or self.timeout)

        while True:
            # å°è¯•è®¾ç½®é”
            result = await self.redis.set(
                self.key,
                self.identifier,
                ex=self.timeout,
                nx=True  # åªåœ¨é”®ä¸å­˜åœ¨æ—¶è®¾ç½®
            )

            if result:
                return True

            if not blocking or time.time() > end_time:
                return False

            await asyncio.sleep(0.1)

    async def release(self) -> bool:
        """é‡Šæ”¾é”"""
        # ä½¿ç”¨ Lua è„šæœ¬ç¡®ä¿åŸå­æ€§
        lua_script = """
        if redis.call('get', KEYS[1]) == ARGV[1] then
            return redis.call('del', KEYS[1])
        else
            return 0
        end
        """

        result = await self.redis.eval(
            lua_script,
            1,
            self.key,
            self.identifier
        )

        return bool(result)

    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        if not await self.acquire():
            raise LockAcquisitionError(f"æ— æ³•è·å–é”: {self.key}")
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        await self.release()

class LockAcquisitionError(Exception):
    """é”è·å–å¤±è´¥å¼‚å¸¸"""
    pass

# ä½¿ç”¨åˆ†å¸ƒå¼é”
async def atomic_operation(user_id: str, redis_client):
    """åŸå­æ“ä½œç¤ºä¾‹"""
    lock = DistributedLock(redis_client, f"user:{user_id}")

    async with lock:
        # åŸå­æ“ä½œé€»è¾‘
        current_value = await redis_client.get(f"user:{user_id}:balance")
        current_value = int(current_value) if current_value else 0

        # æ¨¡æ‹Ÿä¸šåŠ¡é€»è¾‘
        new_value = current_value + 100

        await redis_client.set(f"user:{user_id}:balance", new_value)

        logger.info(f"ç”¨æˆ· {user_id} ä½™é¢æ›´æ–°: {current_value} -> {new_value}")
```

### äº‹åŠ¡å¤„ç†
```python
# transaction_manager.py
import asyncio
from typing import List, Callable, Any, Dict
from contextlib import asynccontextmanager

class TransactionManager:
    """äº‹åŠ¡ç®¡ç†å™¨"""

    def __init__(self, postgres_pool):
        self.postgres_pool = postgres_pool

    @asynccontextmanager
    async def transaction(self):
        """äº‹åŠ¡ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        connection = await self.postgres_pool.acquire()
        transaction = connection.transaction()

        try:
            await transaction.start()
            yield connection
            await transaction.commit()
        except Exception:
            await transaction.rollback()
            raise
        finally:
            await self.postgres_pool.release(connection)

    async def execute_in_transaction(
        self,
        operations: List[Callable],
        *args,
        **kwargs
    ) -> List[Any]:
        """åœ¨äº‹åŠ¡ä¸­æ‰§è¡Œå¤šä¸ªæ“ä½œ"""
        async with self.transaction() as conn:
            results = []
            for operation in operations:
                result = await operation(conn, *args, **kwargs)
                results.append(result)
            return results

# ä½¿ç”¨äº‹åŠ¡ç®¡ç†å™¨
transaction_manager = TransactionManager(postgres_pool)

async def update_user_and_log(user_id: str, changes: Dict[str, Any]):
    """æ›´æ–°ç”¨æˆ·ä¿¡æ¯å¹¶è®°å½•æ—¥å¿—"""

    async def update_user(conn, user_id: str, changes: Dict[str, Any]):
        """æ›´æ–°ç”¨æˆ·ä¿¡æ¯"""
        set_clause = ", ".join([f"{k} = ${i+2}" for i, k in enumerate(changes.keys())])
        query = f"UPDATE users SET {set_clause} WHERE id = $1"
        values = [user_id] + list(changes.values())
        await conn.execute(query, *values)

    async def log_change(conn, user_id: str, changes: Dict[str, Any]):
        """è®°å½•å˜æ›´æ—¥å¿—"""
        await conn.execute(
            "INSERT INTO user_change_log (user_id, changes, timestamp) VALUES ($1, $2, NOW())",
            user_id,
            json.dumps(changes)
        )

    try:
        await transaction_manager.execute_in_transaction(
            [update_user, log_change],
            user_id,
            changes
        )
        logger.info(f"ç”¨æˆ· {user_id} ä¿¡æ¯æ›´æ–°æˆåŠŸ")
    except Exception as e:
        logger.error(f"ç”¨æˆ· {user_id} ä¿¡æ¯æ›´æ–°å¤±è´¥: {e}")
        raise
```

## å®¹ç¾è®¾è®¡

### å¤šåŒºåŸŸéƒ¨ç½²
```yaml
# multi-region-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: langgraph-us-east
  namespace: langgraph
  labels:
    region: us-east-1
spec:
  replicas: 3
  selector:
    matchLabels:
      app: langgraph-server
      region: us-east-1
  template:
    metadata:
      labels:
        app: langgraph-server
        region: us-east-1
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: topology.kubernetes.io/zone
                operator: In
                values:
                - us-east-1a
                - us-east-1b
                - us-east-1c
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - langgraph-server
              topologyKey: kubernetes.io/hostname
      containers:
      - name: langgraph-server
        image: langgraph/langgraph-server:latest
        env:
        - name: REGION
          value: "us-east-1"
        - name: POSTGRES_URL
          value: "postgresql://user:pass@postgres-us-east:5432/langgraph"
        - name: REDIS_URL
          value: "redis://redis-us-east:6379"
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"

---
# è¥¿æµ·å²¸éƒ¨ç½²
apiVersion: apps/v1
kind: Deployment
metadata:
  name: langgraph-us-west
  namespace: langgraph
  labels:
    region: us-west-2
spec:
  replicas: 3
  selector:
    matchLabels:
      app: langgraph-server
      region: us-west-2
  template:
    metadata:
      labels:
        app: langgraph-server
        region: us-west-2
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: topology.kubernetes.io/zone
                operator: In
                values:
                - us-west-2a
                - us-west-2b
                - us-west-2c
      containers:
      - name: langgraph-server
        image: langgraph/langgraph-server:latest
        env:
        - name: REGION
          value: "us-west-2"
        - name: POSTGRES_URL
          value: "postgresql://user:pass@postgres-us-west:5432/langgraph"
        - name: REDIS_URL
          value: "redis://redis-us-west:6379"
```

### æ•°æ®å¤‡ä»½ä¸æ¢å¤
```bash
#!/bin/bash
# backup-restore.sh

# é…ç½®å‚æ•°
BACKUP_DIR="/backups"
S3_BUCKET="langgraph-backups"
RETENTION_DAYS=30

# å¤‡ä»½å‡½æ•°
backup_postgres() {
    local timestamp=$(date +%Y%m%d_%H%M%S)
    local backup_file="${BACKUP_DIR}/postgres_${timestamp}.sql"

    echo "å¼€å§‹ PostgreSQL å¤‡ä»½..."
    pg_dump $POSTGRES_URL > $backup_file

    # å‹ç¼©å¤‡ä»½æ–‡ä»¶
    gzip $backup_file

    # ä¸Šä¼ åˆ° S3
    aws s3 cp ${backup_file}.gz s3://${S3_BUCKET}/postgres/

    echo "PostgreSQL å¤‡ä»½å®Œæˆ: ${backup_file}.gz"
}

backup_redis() {
    local timestamp=$(date +%Y%m%d_%H%M%S)
    local backup_file="${BACKUP_DIR}/redis_${timestamp}.rdb"

    echo "å¼€å§‹ Redis å¤‡ä»½..."
    redis-cli --rdb $backup_file

    # ä¸Šä¼ åˆ° S3
    aws s3 cp $backup_file s3://${S3_BUCKET}/redis/

    echo "Redis å¤‡ä»½å®Œæˆ: $backup_file"
}

# æ¢å¤å‡½æ•°
restore_postgres() {
    local backup_file=$1

    if [[ -z "$backup_file" ]]; then
        echo "è¯·æŒ‡å®šå¤‡ä»½æ–‡ä»¶"
        exit 1
    fi

    echo "å¼€å§‹æ¢å¤ PostgreSQL: $backup_file"

    # ä» S3 ä¸‹è½½
    if [[ $backup_file =~ ^s3:// ]]; then
        aws s3 cp $backup_file /tmp/restore.sql.gz
        backup_file="/tmp/restore.sql.gz"
    fi

    # è§£å‹å¹¶æ¢å¤
    if [[ $backup_file =~ \.gz$ ]]; then
        gunzip -c $backup_file | psql $POSTGRES_URL
    else
        psql $POSTGRES_URL < $backup_file
    fi

    echo "PostgreSQL æ¢å¤å®Œæˆ"
}

# æ¸…ç†æ—§å¤‡ä»½
cleanup_old_backups() {
    echo "æ¸…ç† ${RETENTION_DAYS} å¤©å‰çš„å¤‡ä»½..."

    # æ¸…ç†æœ¬åœ°å¤‡ä»½
    find $BACKUP_DIR -name "*.gz" -mtime +$RETENTION_DAYS -delete
    find $BACKUP_DIR -name "*.rdb" -mtime +$RETENTION_DAYS -delete

    # æ¸…ç† S3 å¤‡ä»½
    aws s3 ls s3://${S3_BUCKET}/postgres/ | while read -r line; do
        createDate=$(echo $line | awk '{print $1" "$2}')
        createDate=$(date -d "$createDate" +%s)
        olderThan=$(date -d "$RETENTION_DAYS days ago" +%s)

        if [[ $createDate -lt $olderThan ]]; then
            fileName=$(echo $line | awk '{print $4}')
            aws s3 rm s3://${S3_BUCKET}/postgres/$fileName
        fi
    done

    echo "æ¸…ç†å®Œæˆ"
}

# ä¸»å‡½æ•°
main() {
    case "$1" in
        backup)
            backup_postgres
            backup_redis
            cleanup_old_backups
            ;;
        restore-postgres)
            restore_postgres "$2"
            ;;
        cleanup)
            cleanup_old_backups
            ;;
        *)
            echo "ç”¨æ³•: $0 {backup|restore-postgres <backup_file>|cleanup}"
            exit 1
            ;;
    esac
}

main "$@"
```

### æ•…éšœè½¬ç§»
```python
# failover_manager.py
import asyncio
import aiohttp
from typing import List, Dict, Optional
from dataclasses import dataclass

@dataclass
class ServiceEndpoint:
    """æœåŠ¡ç«¯ç‚¹"""
    url: str
    region: str
    priority: int
    healthy: bool = True

class FailoverManager:
    """æ•…éšœè½¬ç§»ç®¡ç†å™¨"""

    def __init__(self):
        self.endpoints: List[ServiceEndpoint] = []
        self.current_endpoint: Optional[ServiceEndpoint] = None

    def add_endpoint(self, endpoint: ServiceEndpoint):
        """æ·»åŠ æœåŠ¡ç«¯ç‚¹"""
        self.endpoints.append(endpoint)
        self.endpoints.sort(key=lambda x: x.priority)

        if not self.current_endpoint:
            self.current_endpoint = endpoint

    async def health_check(self, endpoint: ServiceEndpoint) -> bool:
        """å¥åº·æ£€æŸ¥"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    f"{endpoint.url}/health",
                    timeout=aiohttp.ClientTimeout(total=5)
                ) as response:
                    return response.status == 200
        except Exception:
            return False

    async def monitor_endpoints(self):
        """ç›‘æ§æ‰€æœ‰ç«¯ç‚¹"""
        while True:
            tasks = [
                self.health_check(endpoint)
                for endpoint in self.endpoints
            ]

            results = await asyncio.gather(*tasks, return_exceptions=True)

            # æ›´æ–°å¥åº·çŠ¶æ€
            for endpoint, result in zip(self.endpoints, results):
                endpoint.healthy = isinstance(result, bool) and result

            # æ£€æŸ¥å½“å‰ç«¯ç‚¹æ˜¯å¦å¥åº·
            if self.current_endpoint and not self.current_endpoint.healthy:
                await self.failover()

            await asyncio.sleep(30)  # æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡

    async def failover(self):
        """æ•…éšœè½¬ç§»"""
        # æ‰¾åˆ°ä¸‹ä¸€ä¸ªå¥åº·çš„ç«¯ç‚¹
        for endpoint in self.endpoints:
            if endpoint.healthy and endpoint != self.current_endpoint:
                old_endpoint = self.current_endpoint
                self.current_endpoint = endpoint

                logger.warning(
                    f"æ•…éšœè½¬ç§»: {old_endpoint.url} -> {endpoint.url}"
                )
                break
        else:
            logger.critical("æ‰€æœ‰æœåŠ¡ç«¯ç‚¹éƒ½ä¸å¯ç”¨ï¼")

    async def make_request(self, path: str, **kwargs) -> Dict:
        """å‘èµ·è¯·æ±‚ï¼ˆå¸¦æ•…éšœè½¬ç§»ï¼‰"""
        if not self.current_endpoint:
            raise Exception("æ²¡æœ‰å¯ç”¨çš„æœåŠ¡ç«¯ç‚¹")

        try:
            async with aiohttp.ClientSession() as session:
                url = f"{self.current_endpoint.url}{path}"
                async with session.request(**kwargs, url=url) as response:
                    return await response.json()

        except Exception as e:
            logger.error(f"è¯·æ±‚å¤±è´¥: {e}")

            # å°è¯•æ•…éšœè½¬ç§»
            await self.failover()

            if self.current_endpoint:
                # é‡è¯•ä¸€æ¬¡
                async with aiohttp.ClientSession() as session:
                    url = f"{self.current_endpoint.url}{path}"
                    async with session.request(**kwargs, url=url) as response:
                        return await response.json()

            raise

# ä½¿ç”¨æ•…éšœè½¬ç§»ç®¡ç†å™¨
failover_manager = FailoverManager()

# æ·»åŠ æœåŠ¡ç«¯ç‚¹
failover_manager.add_endpoint(
    ServiceEndpoint("https://api-us-east.example.com", "us-east-1", 1)
)
failover_manager.add_endpoint(
    ServiceEndpoint("https://api-us-west.example.com", "us-west-2", 2)
)

# å¯åŠ¨ç›‘æ§
asyncio.create_task(failover_manager.monitor_endpoints())
```

## æ€§èƒ½ä¼˜åŒ–

### ç¼“å­˜ç­–ç•¥
```python
# caching_strategy.py
import asyncio
import json
import hashlib
from typing import Any, Optional, Dict, Callable
from functools import wraps

class MultiLevelCache:
    """å¤šçº§ç¼“å­˜"""

    def __init__(self, redis_client, local_cache_size: int = 1000):
        self.redis = redis_client
        self.local_cache: Dict[str, Any] = {}
        self.local_cache_size = local_cache_size

    def _generate_key(self, *args, **kwargs) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_data = str(args) + str(sorted(kwargs.items()))
        return hashlib.md5(key_data.encode()).hexdigest()

    async def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜å€¼"""
        # 1. æ£€æŸ¥æœ¬åœ°ç¼“å­˜
        if key in self.local_cache:
            return self.local_cache[key]

        # 2. æ£€æŸ¥ Redis ç¼“å­˜
        redis_value = await self.redis.get(key)
        if redis_value:
            value = json.loads(redis_value)
            # å°†å€¼æ”¾å…¥æœ¬åœ°ç¼“å­˜
            self._set_local_cache(key, value)
            return value

        return None

    async def set(self, key: str, value: Any, ttl: int = 3600):
        """è®¾ç½®ç¼“å­˜å€¼"""
        # è®¾ç½® Redis ç¼“å­˜
        await self.redis.setex(key, ttl, json.dumps(value))

        # è®¾ç½®æœ¬åœ°ç¼“å­˜
        self._set_local_cache(key, value)

    def _set_local_cache(self, key: str, value: Any):
        """è®¾ç½®æœ¬åœ°ç¼“å­˜"""
        # å¦‚æœç¼“å­˜å·²æ»¡ï¼Œåˆ é™¤æœ€æ—§çš„æ¡ç›®
        if len(self.local_cache) >= self.local_cache_size:
            oldest_key = next(iter(self.local_cache))
            del self.local_cache[oldest_key]

        self.local_cache[key] = value

    async def delete(self, key: str):
        """åˆ é™¤ç¼“å­˜"""
        # åˆ é™¤æœ¬åœ°ç¼“å­˜
        self.local_cache.pop(key, None)

        # åˆ é™¤ Redis ç¼“å­˜
        await self.redis.delete(key)

# ç¼“å­˜è£…é¥°å™¨
def cached(cache: MultiLevelCache, ttl: int = 3600):
    """ç¼“å­˜è£…é¥°å™¨"""
    def decorator(func: Callable):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # ç”Ÿæˆç¼“å­˜é”®
            cache_key = f"{func.__name__}:{cache._generate_key(*args, **kwargs)}"

            # å°è¯•ä»ç¼“å­˜è·å–
            cached_result = await cache.get(cache_key)
            if cached_result is not None:
                return cached_result

            # æ‰§è¡Œå‡½æ•°
            result = await func(*args, **kwargs) if asyncio.iscoroutinefunction(func) else func(*args, **kwargs)

            # ç¼“å­˜ç»“æœ
            await cache.set(cache_key, result, ttl)

            return result

        return wrapper
    return decorator

# ä½¿ç”¨ç¼“å­˜
cache = MultiLevelCache(redis_client)

@cached(cache, ttl=1800)
async def expensive_computation(data: str) -> str:
    """è€—æ—¶è®¡ç®—"""
    await asyncio.sleep(2)  # æ¨¡æ‹Ÿè€—æ—¶æ“ä½œ
    return f"è®¡ç®—ç»“æœ: {data}"
```

## ç›‘æ§æŒ‡æ ‡

### å…³é”®æ€§èƒ½æŒ‡æ ‡ (KPI)
```python
# kpi_metrics.py
from prometheus_client import Counter, Histogram, Gauge, Summary

# ä¸šåŠ¡æŒ‡æ ‡
REQUEST_TOTAL = Counter(
    'langgraph_requests_total',
    'Total requests',
    ['method', 'endpoint', 'status']
)

REQUEST_DURATION = Histogram(
    'langgraph_request_duration_seconds',
    'Request duration',
    ['endpoint']
)

ACTIVE_USERS = Gauge(
    'langgraph_active_users',
    'Number of active users'
)

# æ‰©å±•æ€§æŒ‡æ ‡
PODS_RUNNING = Gauge(
    'langgraph_pods_running',
    'Number of running pods'
)

HORIZONTAL_SCALE_EVENTS = Counter(
    'langgraph_horizontal_scale_events_total',
    'Horizontal scaling events',
    ['direction']  # up, down
)

# éŸ§æ€§æŒ‡æ ‡
CIRCUIT_BREAKER_STATE = Gauge(
    'langgraph_circuit_breaker_state',
    'Circuit breaker state',
    ['name', 'state']  # closed=0, open=1, half_open=0.5
)

RETRY_ATTEMPTS = Counter(
    'langgraph_retry_attempts_total',
    'Retry attempts',
    ['function', 'attempt']
)

FAILOVER_EVENTS = Counter(
    'langgraph_failover_events_total',
    'Failover events',
    ['from_region', 'to_region']
)

# æ•°æ®åº“æŒ‡æ ‡
DB_CONNECTIONS_ACTIVE = Gauge(
    'langgraph_db_connections_active',
    'Active database connections'
)

DB_QUERY_DURATION = Histogram(
    'langgraph_db_query_duration_seconds',
    'Database query duration',
    ['query_type']
)

def track_scaling_event(direction: str):
    """è®°å½•æ‰©å±•äº‹ä»¶"""
    HORIZONTAL_SCALE_EVENTS.labels(direction=direction).inc()

def track_circuit_breaker_state(name: str, state: str):
    """è®°å½•æ–­è·¯å™¨çŠ¶æ€"""
    state_value = {
        'closed': 0,
        'open': 1,
        'half_open': 0.5
    }[state]
    CIRCUIT_BREAKER_STATE.labels(name=name, state=state).set(state_value)

def track_failover(from_region: str, to_region: str):
    """è®°å½•æ•…éšœè½¬ç§»"""
    FAILOVER_EVENTS.labels(
        from_region=from_region,
        to_region=to_region
    ).inc()
```

## ä¸‹ä¸€æ­¥

- ğŸ³ å­¦ä¹  [06-Dockerå®¹å™¨åŒ–](./06-Dockerå®¹å™¨åŒ–.md) - å®¹å™¨åŒ–éƒ¨ç½²æ–¹æ¡ˆ
- ğŸ” æ¢ç´¢ [07-è®¤è¯ä¸æˆæƒ](./07-è®¤è¯ä¸æˆæƒ.md) - ä¼ä¸šçº§å®‰å…¨æ§åˆ¶
- ğŸ”— äº†è§£ [08-Webhooksé›†æˆ](./08-Webhooksé›†æˆ.md) - äº‹ä»¶é©±åŠ¨æ¶æ„

## ç›¸å…³é“¾æ¥

- [LangGraph Platform æ‰©å±•æ€§](https://langchain-ai.github.io/langgraph/concepts/scalability_and_resilience/)
- [Kubernetes è‡ªåŠ¨æ‰©å±•](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)
- [åˆ†å¸ƒå¼ç³»ç»Ÿæ¨¡å¼](https://microservices.io/patterns/)
- [å¯è§‚æµ‹æ€§æœ€ä½³å®è·µ](https://sre.google/books/)