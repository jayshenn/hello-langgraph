# LangGraph API é€ŸæŸ¥è¡¨

> ğŸš€ **å¿«é€Ÿå‚è€ƒ**ï¼šå¸¸ç”¨ LangGraph API çš„ç­¾åã€å‚æ•°å’ŒåŸºç¡€ç”¨æ³•

## ğŸ“Š ç›®å½•

- [å›¾æ„å»º APIs](#å›¾æ„å»º-apis)
- [çŠ¶æ€ç®¡ç† APIs](#çŠ¶æ€ç®¡ç†-apis)
- [æ‰§è¡Œ APIs](#æ‰§è¡Œ-apis)
- [å·¥å…·ä¸é›†æˆ APIs](#å·¥å…·ä¸é›†æˆ-apis)
- [é…ç½® APIs](#é…ç½®-apis)
- [æµå¼å¤„ç† APIs](#æµå¼å¤„ç†-apis)
- [å¹³å°ç›¸å…³ APIs](#å¹³å°ç›¸å…³-apis)

---

## ğŸ—ï¸ å›¾æ„å»º APIs

### StateGraph

åˆ›å»ºçŠ¶æ€å›¾çš„æ ¸å¿ƒç±»ã€‚

```python
from langgraph.graph import StateGraph

# ç­¾å
StateGraph(schema: Type[TypedDict])

# åŸºç¡€ç”¨æ³•
class MyState(TypedDict):
    message: str

graph = StateGraph(MyState)
```

### add_node

å‘å›¾ä¸­æ·»åŠ èŠ‚ç‚¹ã€‚

```python
# ç­¾å
graph.add_node(name: str, func: Callable[[State], State])

# åŸºç¡€ç”¨æ³•
def my_node(state: MyState) -> MyState:
    return {"message": "å¤„ç†å®Œæˆ"}

graph.add_node("process", my_node)
```

### add_edge

æ·»åŠ æ— æ¡ä»¶è¾¹ã€‚

```python
# ç­¾å
graph.add_edge(from_node: str, to_node: str)
# æˆ–
graph.add_edge(from_nodes: List[str], to_node: str)

# åŸºç¡€ç”¨æ³•
graph.add_edge("start", "process")
graph.add_edge(["node1", "node2"], "merge")
```

### add_conditional_edges

æ·»åŠ æ¡ä»¶è¾¹ã€‚

```python
# ç­¾å
graph.add_conditional_edges(
    source: str,
    condition: Callable[[State], str],
    mapping: Dict[str, str]
)

# åŸºç¡€ç”¨æ³•
def router(state: MyState) -> str:
    return "success" if state["message"] else "error"

graph.add_conditional_edges(
    "process",
    router,
    {"success": "success_node", "error": "error_node"}
)
```

### set_entry_point & set_finish_point

è®¾ç½®å›¾çš„å…¥å£å’Œå‡ºå£ã€‚

```python
# ç­¾å
graph.set_entry_point(node_name: str)
graph.set_finish_point(node_name: str)

# åŸºç¡€ç”¨æ³•
graph.set_entry_point("start")
graph.set_finish_point("end")

# æˆ–ä½¿ç”¨é¢„å®šä¹‰å¸¸é‡
from langgraph.graph import START, END
graph.add_edge(START, "first_node")
graph.add_edge("last_node", END)
```

### compile

ç¼–è¯‘å›¾ä¸ºå¯æ‰§è¡Œçš„åº”ç”¨ã€‚

```python
# ç­¾å
graph.compile(
    checkpointer: Optional[BaseCheckpointer] = None,
    interrupt_before: Optional[List[str]] = None,
    interrupt_after: Optional[List[str]] = None,
    debug: bool = False
) -> CompiledGraph

# åŸºç¡€ç”¨æ³•
app = graph.compile()

# å¸¦æ£€æŸ¥ç‚¹
from langgraph.checkpoint.memory import MemorySaver
memory = MemorySaver()
app = graph.compile(checkpointer=memory)
```

---

## ğŸ—‚ï¸ çŠ¶æ€ç®¡ç† APIs

### TypedDict

å®šä¹‰çŠ¶æ€ç»“æ„çš„åŸºç¡€ç±»ã€‚

```python
from typing import TypedDict, List, Optional

# åŸºç¡€çŠ¶æ€
class AgentState(TypedDict):
    message: str
    count: int

# å¤æ‚çŠ¶æ€
class ComplexState(TypedDict):
    user_input: str
    search_results: List[Dict[str, Any]]
    metadata: Optional[Dict[str, str]]
```

### Annotated & add_messages

ç”¨äºæ¶ˆæ¯ç±»å‹çŠ¶æ€çš„ç‰¹æ®Šæ³¨è§£ã€‚

```python
from typing import Annotated, Sequence
from langchain_core.messages import BaseMessage
from langgraph.graph.message import add_messages

class MessageState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]
```

### RemoveMessage

ç”¨äºä»æ¶ˆæ¯å†å²ä¸­åˆ é™¤ç‰¹å®šæ¶ˆæ¯ã€‚

```python
from langgraph.graph.message import RemoveMessage

def trim_messages(state: MessageState) -> MessageState:
    # åˆ é™¤ç¬¬ä¸€æ¡æ¶ˆæ¯
    return {"messages": [RemoveMessage(id=state["messages"][0].id)]}
```

---

## â–¶ï¸ æ‰§è¡Œ APIs

### invoke

åŒæ­¥æ‰§è¡Œå›¾ã€‚

```python
# ç­¾å
app.invoke(
    input: Dict[str, Any],
    config: Optional[RunnableConfig] = None
) -> Dict[str, Any]

# åŸºç¡€ç”¨æ³•
result = app.invoke({"message": "Hello"})

# å¸¦é…ç½®
config = {"configurable": {"thread_id": "123"}}
result = app.invoke({"message": "Hello"}, config=config)
```

### ainvoke

å¼‚æ­¥æ‰§è¡Œå›¾ã€‚

```python
# ç­¾å
await app.ainvoke(
    input: Dict[str, Any],
    config: Optional[RunnableConfig] = None
) -> Dict[str, Any]

# åŸºç¡€ç”¨æ³•
result = await app.ainvoke({"message": "Hello"})
```

### stream

åŒæ­¥æµå¼æ‰§è¡Œã€‚

```python
# ç­¾å
app.stream(
    input: Dict[str, Any],
    config: Optional[RunnableConfig] = None,
    stream_mode: str = "values"
) -> Iterator[Dict[str, Any]]

# åŸºç¡€ç”¨æ³•
for chunk in app.stream({"message": "Hello"}):
    print(chunk)

# ä¸åŒæµæ¨¡å¼
for update in app.stream({"message": "Hello"}, stream_mode="updates"):
    print(update)
```

### astream

å¼‚æ­¥æµå¼æ‰§è¡Œã€‚

```python
# ç­¾å
app.astream(
    input: Dict[str, Any],
    config: Optional[RunnableConfig] = None,
    stream_mode: str = "values"
) -> AsyncIterator[Dict[str, Any]]

# åŸºç¡€ç”¨æ³•
async for chunk in app.astream({"message": "Hello"}):
    print(chunk)
```

### batch

æ‰¹é‡æ‰§è¡Œã€‚

```python
# ç­¾å
app.batch(
    inputs: List[Dict[str, Any]],
    config: Optional[RunnableConfig] = None
) -> List[Dict[str, Any]]

# åŸºç¡€ç”¨æ³•
inputs = [{"message": "Hello"}, {"message": "World"}]
results = app.batch(inputs)
```

---

## ğŸ› ï¸ å·¥å…·ä¸é›†æˆ APIs

### ToolNode

åˆ›å»ºå·¥å…·èŠ‚ç‚¹ã€‚

```python
from langgraph.prebuilt import ToolNode
from langchain_core.tools import tool

@tool
def search_tool(query: str) -> str:
    """æœç´¢å·¥å…·"""
    return f"æœç´¢ç»“æœ: {query}"

# åˆ›å»ºå·¥å…·èŠ‚ç‚¹
tools = [search_tool]
tool_node = ToolNode(tools)

# æ·»åŠ åˆ°å›¾
graph.add_node("tools", tool_node)
```

### interrupt

æš‚åœæ‰§è¡Œä»¥è·å–äººå·¥è¾“å…¥ã€‚

```python
from langgraph.prebuilt import interrupt

def human_review_node(state: MyState) -> MyState:
    if needs_review(state):
        human_input = interrupt("è¯·å®¡æ ¸å†…å®¹")
        return {**state, "review": human_input}
    return state
```

### create_react_agent

åˆ›å»º ReAct é£æ ¼çš„æ™ºèƒ½ä½“ã€‚

```python
from langgraph.prebuilt import create_react_agent
from langchain_openai import ChatOpenAI

model = ChatOpenAI(model="gpt-4")
tools = [search_tool]

agent = create_react_agent(model, tools)
```

---

## âš™ï¸ é…ç½® APIs

### RunnableConfig

æ‰§è¡Œé…ç½®å¯¹è±¡ã€‚

```python
from langchain_core.runnables import RunnableConfig

config = RunnableConfig(
    configurable={
        "thread_id": "user_123",
        "user_id": "user_456"
    },
    tags=["production"],
    metadata={"version": "1.0"}
)
```

### MemorySaver

å†…å­˜æ£€æŸ¥ç‚¹ä¿å­˜å™¨ã€‚

```python
from langgraph.checkpoint.memory import MemorySaver

memory = MemorySaver()
app = graph.compile(checkpointer=memory)
```

### SqliteSaver

SQLite æ£€æŸ¥ç‚¹ä¿å­˜å™¨ã€‚

```python
from langgraph.checkpoint.sqlite import SqliteSaver

checkpointer = SqliteSaver.from_conn_string("checkpoints.db")
app = graph.compile(checkpointer=checkpointer)
```

---

## ğŸŒŠ æµå¼å¤„ç† APIs

### æµæ¨¡å¼å‚æ•°

ä¸åŒçš„æµå¼è¾“å‡ºæ¨¡å¼ã€‚

```python
# values: è¾“å‡ºæ¯ä¸ªèŠ‚ç‚¹åçš„å®Œæ•´çŠ¶æ€
for chunk in app.stream(input, stream_mode="values"):
    pass

# updates: è¾“å‡ºæ¯ä¸ªèŠ‚ç‚¹çš„çŠ¶æ€æ›´æ–°
for chunk in app.stream(input, stream_mode="updates"):
    pass

# debug: è¾“å‡ºè°ƒè¯•ä¿¡æ¯
for chunk in app.stream(input, stream_mode="debug"):
    pass

# custom: è¾“å‡ºè‡ªå®šä¹‰æ•°æ®
for chunk in app.stream(input, stream_mode="custom"):
    pass
```

### astream_events

å¼‚æ­¥äº‹ä»¶æµã€‚

```python
async for event in app.astream_events(input, version="v1"):
    if event["event"] == "on_llm_stream":
        print(event["data"]["chunk"])
```

---

## ğŸ¢ å¹³å°ç›¸å…³ APIs

### RemoteGraph

è¿æ¥è¿œç¨‹ LangGraph æœåŠ¡ã€‚

```python
from langgraph.graph import RemoteGraph

# è¿æ¥åˆ°è¿œç¨‹å›¾
remote_graph = RemoteGraph(
    graph_id="my-graph",
    url="https://api.langgraph.dev"
)

# åƒæœ¬åœ°å›¾ä¸€æ ·ä½¿ç”¨
result = await remote_graph.ainvoke({"message": "Hello"})
```

### LangGraphClient

LangGraph Platform å®¢æˆ·ç«¯ã€‚

```python
from langgraph_sdk import LangGraphClient

client = LangGraphClient(
    url="https://your-deployment.langraph.dev",
    api_key="your-api-key"
)

# åˆ›å»ºåŠ©æ‰‹
assistant = await client.assistants.create(
    graph_id="agent",
    config={"configurable": {"model": "gpt-4"}}
)

# åˆ›å»ºçº¿ç¨‹
thread = await client.threads.create()

# è¿è¡Œå¯¹è¯
async for chunk in client.runs.stream(
    thread["thread_id"],
    assistant["assistant_id"],
    input={"messages": [{"role": "user", "content": "Hello!"}]}
):
    print(chunk)
```

---

## ğŸ“š å¿«é€Ÿç¤ºä¾‹

### æœ€ç®€å›¾

```python
from typing import TypedDict
from langgraph.graph import StateGraph, START, END

class State(TypedDict):
    message: str

def process(state: State) -> State:
    return {"message": f"å¤„ç†: {state['message']}"}

graph = StateGraph(State)
graph.add_node("process", process)
graph.add_edge(START, "process")
graph.add_edge("process", END)

app = graph.compile()
result = app.invoke({"message": "Hello"})
```

### å¸¦æ¡ä»¶çš„å›¾

```python
def router(state: State) -> str:
    return "good" if "good" in state["message"] else "bad"

graph.add_conditional_edges(
    "process",
    router,
    {"good": "good_path", "bad": "bad_path"}
)
```

### å¸¦å·¥å…·çš„æ™ºèƒ½ä½“

```python
from langgraph.prebuilt import create_react_agent
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool

@tool
def search(query: str) -> str:
    """æœç´¢å·¥å…·"""
    return f"æœç´¢ç»“æœ: {query}"

model = ChatOpenAI(model="gpt-4")
agent = create_react_agent(model, [search])

result = agent.invoke({"messages": [("user", "æœç´¢Pythonæ•™ç¨‹")]})
```

---

## ğŸ”— ç›¸å…³é“¾æ¥

- ğŸ“– [LangGraph å®˜æ–¹æ–‡æ¡£](https://langchain-ai.github.io/langgraph/)
- ğŸ› [é”™è¯¯ç å¯¹ç…§](./é”™è¯¯ç å¯¹ç…§.md)
- ğŸ’¡ [ä»£ç ç‰‡æ®µåº“](./ä»£ç ç‰‡æ®µ.md)
- â“ [å¸¸è§é—®é¢˜](./FAQ.md)

---

*API é€ŸæŸ¥è¡¨ä¼šæŒç»­æ›´æ–°ï¼Œå»ºè®®æ”¶è—å¤‡ç”¨ï¼* â­