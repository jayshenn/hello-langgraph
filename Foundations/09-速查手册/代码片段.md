# LangGraph ä»£ç ç‰‡æ®µåº“

> ğŸ’¡ **å³æ’å³ç”¨**ï¼šç»è¿‡æµ‹è¯•çš„ä»£ç æ¨¡æ¿ï¼Œå¯ç›´æ¥å¤åˆ¶ä½¿ç”¨

## ğŸ“š æ¨¡æ¿åˆ†ç±»

- [åŸºç¡€å›¾ç»“æ„](#åŸºç¡€å›¾ç»“æ„)
- [çŠ¶æ€ç®¡ç†æ¨¡å¼](#çŠ¶æ€ç®¡ç†æ¨¡å¼)
- [èŠ‚ç‚¹åŠŸèƒ½æ¨¡æ¿](#èŠ‚ç‚¹åŠŸèƒ½æ¨¡æ¿)
- [æ¡ä»¶è·¯ç”±æ¨¡æ¿](#æ¡ä»¶è·¯ç”±æ¨¡æ¿)
- [Memory è®°å¿†æ¨¡æ¿](#memory-è®°å¿†æ¨¡æ¿)
- [Human-in-the-Loop æ¨¡æ¿](#human-in-the-loop-æ¨¡æ¿)
- [æµå¼å¤„ç†æ¨¡æ¿](#æµå¼å¤„ç†æ¨¡æ¿)
- [å·¥å…·é›†æˆæ¨¡æ¿](#å·¥å…·é›†æˆæ¨¡æ¿)
- [éƒ¨ç½²é…ç½®æ¨¡æ¿](#éƒ¨ç½²é…ç½®æ¨¡æ¿)

---

## ğŸ—ï¸ åŸºç¡€å›¾ç»“æ„

### æœ€ç®€å•çš„å›¾

```python
from typing import TypedDict
from langgraph.graph import StateGraph, START, END

class SimpleState(TypedDict):
    message: str

def process_node(state: SimpleState) -> SimpleState:
    return {"message": f"å¤„ç†å®Œæˆ: {state['message']}"}

# æ„å»ºå›¾
graph = StateGraph(SimpleState)
graph.add_node("process", process_node)
graph.add_edge(START, "process")
graph.add_edge("process", END)

# ç¼–è¯‘å¹¶è¿è¡Œ
app = graph.compile()
result = app.invoke({"message": "Hello World"})
print(result)  # {'message': 'å¤„ç†å®Œæˆ: Hello World'}
```

### åºåˆ—åŒ–å¤„ç†å›¾

```python
from typing import TypedDict

class SequentialState(TypedDict):
    input_text: str
    cleaned_text: str
    analyzed_text: str
    final_result: str

def clean_text(state: SequentialState) -> SequentialState:
    cleaned = state["input_text"].strip().lower()
    return {**state, "cleaned_text": cleaned}

def analyze_text(state: SequentialState) -> SequentialState:
    word_count = len(state["cleaned_text"].split())
    analyzed = f"åˆ†æç»“æœ: {word_count} ä¸ªè¯"
    return {**state, "analyzed_text": analyzed}

def generate_result(state: SequentialState) -> SequentialState:
    result = f"æœ€ç»ˆç»“æœ: {state['analyzed_text']}"
    return {**state, "final_result": result}

# æ„å»ºåºåˆ—å›¾
graph = StateGraph(SequentialState)
graph.add_node("clean", clean_text)
graph.add_node("analyze", analyze_text)
graph.add_node("generate", generate_result)

graph.add_edge(START, "clean")
graph.add_edge("clean", "analyze")
graph.add_edge("analyze", "generate")
graph.add_edge("generate", END)

app = graph.compile()
```

### å¸¦åˆ†æ”¯å’Œåˆå¹¶çš„å›¾

```python
from typing import TypedDict, List

class BranchState(TypedDict):
    input_data: str
    branch_a_result: str
    branch_b_result: str
    merged_result: str

def process_branch_a(state: BranchState) -> BranchState:
    result = f"åˆ†æ”¯Aå¤„ç†: {state['input_data']}"
    return {**state, "branch_a_result": result}

def process_branch_b(state: BranchState) -> BranchState:
    result = f"åˆ†æ”¯Bå¤„ç†: {state['input_data']}"
    return {**state, "branch_b_result": result}

def merge_results(state: BranchState) -> BranchState:
    merged = f"åˆå¹¶ç»“æœ: {state['branch_a_result']} + {state['branch_b_result']}"
    return {**state, "merged_result": merged}

# æ„å»ºåˆ†æ”¯å›¾
graph = StateGraph(BranchState)
graph.add_node("branch_a", process_branch_a)
graph.add_node("branch_b", process_branch_b)
graph.add_node("merge", merge_results)

# å¹¶è¡Œåˆ†æ”¯
graph.add_edge(START, "branch_a")
graph.add_edge(START, "branch_b")
# ç­‰å¾…ä¸¤ä¸ªåˆ†æ”¯å®Œæˆååˆå¹¶
graph.add_edge(["branch_a", "branch_b"], "merge")
graph.add_edge("merge", END)

app = graph.compile()
```

---

## ğŸ—‚ï¸ çŠ¶æ€ç®¡ç†æ¨¡å¼

### åŸºç¡€çŠ¶æ€ç»“æ„

```python
from typing import TypedDict, Optional, List, Dict, Any

class BasicState(TypedDict):
    # ç”¨æˆ·è¾“å…¥
    user_input: str

    # å¤„ç†çŠ¶æ€
    is_processed: bool

    # ç»“æœæ•°æ®
    result: Optional[str]

    # é”™è¯¯ä¿¡æ¯
    error: Optional[str]

    # å…ƒæ•°æ®
    timestamp: str
    session_id: str
```

### æ¶ˆæ¯å†å²çŠ¶æ€

```python
from typing import Annotated, Sequence
from langchain_core.messages import BaseMessage
from langgraph.graph.message import add_messages

class ChatState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]
    user_id: str
    conversation_id: str

# ä½¿ç”¨ç¤ºä¾‹
from langchain_core.messages import HumanMessage, AIMessage

def chat_node(state: ChatState) -> ChatState:
    # è·å–æœ€æ–°æ¶ˆæ¯
    last_message = state["messages"][-1]

    # ç”Ÿæˆå›å¤
    response = f"æ”¶åˆ°æ¶ˆæ¯: {last_message.content}"

    # æ·»åŠ AIå›å¤åˆ°æ¶ˆæ¯å†å²
    return {
        "messages": [AIMessage(content=response)]
    }
```

### å¤æ‚çŠ¶æ€ç»“æ„

```python
from typing import TypedDict, List, Dict, Any, Literal
from datetime import datetime

class ComplexState(TypedDict):
    # ç”¨æˆ·ä¿¡æ¯
    user_id: str
    session_id: str

    # è¾“å…¥æ•°æ®
    query: str
    context: Dict[str, Any]

    # å¤„ç†çŠ¶æ€
    current_step: Literal["analyzing", "searching", "generating", "reviewing"]
    step_count: int
    max_steps: int

    # ä¸­é—´ç»“æœ
    analysis_result: Optional[Dict[str, Any]]
    search_results: List[Dict[str, str]]

    # æœ€ç»ˆè¾“å‡º
    generated_content: Optional[str]
    confidence_score: Optional[float]

    # é”™è¯¯å¤„ç†
    errors: List[str]
    retry_count: int

    # å…ƒæ•°æ®
    timestamp: str
    processing_time: Optional[float]
```

### çŠ¶æ€éªŒè¯å™¨

```python
def validate_state(state: ComplexState) -> ComplexState:
    """éªŒè¯çŠ¶æ€çš„æœ‰æ•ˆæ€§"""
    errors = []

    # æ£€æŸ¥å¿…éœ€å­—æ®µ
    if not state.get("user_id"):
        errors.append("ç¼ºå°‘ user_id")

    if not state.get("query"):
        errors.append("ç¼ºå°‘ query")

    # æ£€æŸ¥æ­¥æ•°é™åˆ¶
    if state.get("step_count", 0) > state.get("max_steps", 25):
        errors.append("è¶…è¿‡æœ€å¤§æ­¥æ•°é™åˆ¶")

    # æ›´æ–°é”™è¯¯åˆ—è¡¨
    if errors:
        current_errors = state.get("errors", [])
        return {**state, "errors": current_errors + errors}

    return state

def safe_node(func):
    """çŠ¶æ€éªŒè¯è£…é¥°å™¨"""
    def wrapper(state: ComplexState) -> ComplexState:
        # å‰ç½®éªŒè¯
        validated_state = validate_state(state)
        if validated_state.get("errors"):
            return validated_state

        # æ‰§è¡ŒèŠ‚ç‚¹é€»è¾‘
        try:
            result = func(validated_state)
            return validate_state(result)
        except Exception as e:
            return {
                **validated_state,
                "errors": validated_state.get("errors", []) + [str(e)]
            }

    return wrapper
```

---

## ğŸ¯ èŠ‚ç‚¹åŠŸèƒ½æ¨¡æ¿

### åŸºç¡€èŠ‚ç‚¹æ¨¡æ¿

```python
def basic_node_template(state: MyState) -> MyState:
    """åŸºç¡€èŠ‚ç‚¹æ¨¡æ¿"""
    try:
        # 1. è·å–è¾“å…¥æ•°æ®
        input_data = state["input_field"]

        # 2. æ‰§è¡Œæ ¸å¿ƒé€»è¾‘
        result = process_data(input_data)

        # 3. è¿”å›æ›´æ–°åçš„çŠ¶æ€
        return {
            **state,
            "output_field": result,
            "status": "success",
            "timestamp": datetime.now().isoformat()
        }

    except Exception as e:
        # 4. é”™è¯¯å¤„ç†
        return {
            **state,
            "error": str(e),
            "status": "failed",
            "retry_count": state.get("retry_count", 0) + 1
        }
```

### LLM è°ƒç”¨èŠ‚ç‚¹

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage

def llm_node(state: MyState) -> MyState:
    """LLM è°ƒç”¨èŠ‚ç‚¹"""
    # åˆå§‹åŒ–æ¨¡å‹
    model = ChatOpenAI(model="gpt-4", temperature=0.7)

    # æ„å»ºæ¶ˆæ¯
    messages = [
        SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹"),
        HumanMessage(content=state["user_query"])
    ]

    # è°ƒç”¨ LLM
    try:
        response = model.invoke(messages)
        return {
            **state,
            "llm_response": response.content,
            "tokens_used": response.response_metadata.get("token_usage", {})
        }
    except Exception as e:
        return {**state, "error": f"LLMè°ƒç”¨å¤±è´¥: {str(e)}"}
```

### æ•°æ®å¤„ç†èŠ‚ç‚¹

```python
import json
from typing import Any

def data_processing_node(state: MyState) -> MyState:
    """æ•°æ®å¤„ç†èŠ‚ç‚¹"""
    raw_data = state["raw_data"]

    # æ•°æ®æ¸…æ´—
    cleaned_data = []
    for item in raw_data:
        if isinstance(item, dict) and "content" in item:
            cleaned_item = {
                "content": item["content"].strip(),
                "metadata": item.get("metadata", {}),
                "timestamp": item.get("timestamp", datetime.now().isoformat())
            }
            cleaned_data.append(cleaned_item)

    # æ•°æ®è½¬æ¢
    processed_data = {
        "total_items": len(cleaned_data),
        "items": cleaned_data,
        "summary": f"å¤„ç†äº† {len(cleaned_data)} æ¡æ•°æ®"
    }

    return {
        **state,
        "processed_data": processed_data,
        "processing_status": "completed"
    }
```

### å¼‚æ­¥èŠ‚ç‚¹æ¨¡æ¿

```python
import asyncio
import aiohttp

async def async_api_node(state: MyState) -> MyState:
    """å¼‚æ­¥ API è°ƒç”¨èŠ‚ç‚¹"""
    url = "https://api.example.com/data"
    params = {"query": state["search_query"]}

    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    return {
                        **state,
                        "api_result": data,
                        "api_status": "success"
                    }
                else:
                    return {
                        **state,
                        "api_error": f"APIè¿”å›çŠ¶æ€ç : {response.status}",
                        "api_status": "failed"
                    }
    except Exception as e:
        return {
            **state,
            "api_error": str(e),
            "api_status": "failed"
        }

# åœ¨å›¾ä¸­ä½¿ç”¨å¼‚æ­¥èŠ‚ç‚¹
def sync_wrapper(state: MyState) -> MyState:
    """åŒæ­¥åŒ…è£…å™¨"""
    return asyncio.run(async_api_node(state))
```

---

## ğŸ”€ æ¡ä»¶è·¯ç”±æ¨¡æ¿

### ç®€å•äºŒåˆ†æ”¯è·¯ç”±

```python
def simple_router(state: MyState) -> str:
    """ç®€å•çš„äºŒåˆ†æ”¯è·¯ç”±"""
    if state.get("condition_field", False):
        return "success_path"
    else:
        return "error_path"

# æ·»åŠ åˆ°å›¾
graph.add_conditional_edges(
    "decision_node",
    simple_router,
    {
        "success_path": "success_node",
        "error_path": "error_node"
    }
)
```

### å¤šåˆ†æ”¯è·¯ç”±

```python
def multi_branch_router(state: MyState) -> str:
    """å¤šåˆ†æ”¯è·¯ç”±é€»è¾‘"""
    score = state.get("confidence_score", 0)

    if score >= 0.8:
        return "high_confidence"
    elif score >= 0.5:
        return "medium_confidence"
    elif score >= 0.2:
        return "low_confidence"
    else:
        return "no_confidence"

graph.add_conditional_edges(
    "evaluation_node",
    multi_branch_router,
    {
        "high_confidence": "proceed_node",
        "medium_confidence": "review_node",
        "low_confidence": "retry_node",
        "no_confidence": "error_node"
    }
)
```

### å¾ªç¯æ¡ä»¶è·¯ç”±

```python
def loop_router(state: MyState) -> str:
    """å¾ªç¯æ§åˆ¶è·¯ç”±"""
    current_step = state.get("current_step", 0)
    max_steps = state.get("max_steps", 5)
    has_converged = state.get("has_converged", False)

    # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
    if has_converged:
        return "converged"
    elif current_step >= max_steps:
        return "max_steps_reached"
    else:
        return "continue_loop"

graph.add_conditional_edges(
    "iteration_node",
    loop_router,
    {
        "continue_loop": "iteration_node",  # å¾ªç¯å›è‡ªèº«
        "converged": "success_node",
        "max_steps_reached": "timeout_node"
    }
)
```

### åŸºäºå†…å®¹çš„è·¯ç”±

```python
def content_based_router(state: MyState) -> str:
    """åŸºäºå†…å®¹ç±»å‹çš„è·¯ç”±"""
    content_type = state.get("content_type", "").lower()

    if "question" in content_type or "?" in state.get("user_input", ""):
        return "question_handler"
    elif "command" in content_type or state.get("user_input", "").startswith("/"):
        return "command_handler"
    elif "search" in content_type:
        return "search_handler"
    else:
        return "general_handler"

# ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼çš„é«˜çº§å†…å®¹è·¯ç”±
import re

def advanced_content_router(state: MyState) -> str:
    """é«˜çº§å†…å®¹è·¯ç”±"""
    user_input = state.get("user_input", "").lower()

    # å®šä¹‰æ¨¡å¼
    patterns = {
        "search": r"(æœç´¢|æŸ¥æ‰¾|æ‰¾|search|find)",
        "translate": r"(ç¿»è¯‘|translate|è½¬æ¢)",
        "analyze": r"(åˆ†æ|analyze|è§£æ|è¯„ä¼°)",
        "summarize": r"(æ€»ç»“|æ‘˜è¦|æ¦‚æ‹¬|summarize)"
    }

    # åŒ¹é…æ¨¡å¼
    for intent, pattern in patterns.items():
        if re.search(pattern, user_input):
            return f"{intent}_handler"

    return "default_handler"
```

---

## ğŸ§  Memory è®°å¿†æ¨¡æ¿

### çŸ­æœŸè®°å¿†ç®¡ç†

```python
from langchain_core.messages import BaseMessage, RemoveMessage
from typing import Annotated, Sequence

class ShortTermMemoryState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]
    max_messages: int

def trim_messages(state: ShortTermMemoryState) -> ShortTermMemoryState:
    """ä¿®å‰ªæ¶ˆæ¯å†å²ï¼Œä¿æŒçŸ­æœŸè®°å¿†"""
    messages = state["messages"]
    max_messages = state.get("max_messages", 10)

    if len(messages) > max_messages:
        # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯å’Œæœ€è¿‘çš„æ¶ˆæ¯
        system_messages = [msg for msg in messages if msg.type == "system"]
        recent_messages = messages[-max_messages:]

        # åˆ›å»ºåˆ é™¤åˆ—è¡¨
        messages_to_remove = []
        for msg in messages:
            if msg not in system_messages and msg not in recent_messages:
                messages_to_remove.append(RemoveMessage(id=msg.id))

        return {"messages": messages_to_remove}

    return {}
```

### é•¿æœŸè®°å¿†å­˜å‚¨

```python
import chromadb
from typing import Dict, List

class LongTermMemoryState(TypedDict):
    user_id: str
    current_query: str
    retrieved_memories: List[Dict[str, Any]]

def store_memory(user_id: str, content: str, metadata: Dict[str, Any] = None):
    """å­˜å‚¨é•¿æœŸè®°å¿†"""
    client = chromadb.PersistentClient(path="./memory_db")
    collection = client.get_or_create_collection(f"user_{user_id}_memories")

    collection.add(
        documents=[content],
        metadatas=[metadata or {}],
        ids=[f"memory_{len(collection.get()['documents'])}"]
    )

def retrieve_memories(state: LongTermMemoryState) -> LongTermMemoryState:
    """æ£€ç´¢ç›¸å…³è®°å¿†"""
    user_id = state["user_id"]
    query = state["current_query"]

    try:
        client = chromadb.PersistentClient(path="./memory_db")
        collection = client.get_or_create_collection(f"user_{user_id}_memories")

        results = collection.query(
            query_texts=[query],
            n_results=5
        )

        memories = []
        for i, doc in enumerate(results["documents"][0]):
            memories.append({
                "content": doc,
                "metadata": results["metadatas"][0][i],
                "distance": results["distances"][0][i]
            })

        return {**state, "retrieved_memories": memories}

    except Exception as e:
        return {**state, "retrieved_memories": [], "memory_error": str(e)}

def update_memory_node(state: LongTermMemoryState) -> LongTermMemoryState:
    """æ›´æ–°è®°å¿†èŠ‚ç‚¹"""
    user_id = state["user_id"]
    current_interaction = state["current_query"]

    # å­˜å‚¨å½“å‰äº¤äº’
    store_memory(
        user_id=user_id,
        content=current_interaction,
        metadata={
            "timestamp": datetime.now().isoformat(),
            "interaction_type": "query"
        }
    )

    return retrieve_memories(state)
```

### è®°å¿†ç®¡ç†å·¥å…·

```python
class MemoryManager:
    def __init__(self, user_id: str, max_short_term: int = 10):
        self.user_id = user_id
        self.max_short_term = max_short_term
        self.client = chromadb.PersistentClient(path="./memory_db")
        self.collection = self.client.get_or_create_collection(
            f"user_{user_id}_memories"
        )

    def should_store_long_term(self, content: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥å­˜å‚¨ä¸ºé•¿æœŸè®°å¿†"""
        # ç®€å•çš„å¯å‘å¼è§„åˆ™
        important_keywords = ["é‡è¦", "è®°ä½", "ä¿å­˜", "ä¸‹æ¬¡", "æ€»æ˜¯"]
        return any(keyword in content for keyword in important_keywords)

    def store_if_important(self, content: str, metadata: Dict = None):
        """å¦‚æœé‡è¦åˆ™å­˜å‚¨"""
        if self.should_store_long_term(content):
            self.collection.add(
                documents=[content],
                metadatas=[metadata or {}],
                ids=[f"memory_{datetime.now().timestamp()}"]
            )

    def get_relevant_context(self, query: str, top_k: int = 3) -> str:
        """è·å–ç›¸å…³ä¸Šä¸‹æ–‡"""
        results = self.collection.query(
            query_texts=[query],
            n_results=top_k
        )

        if results["documents"]:
            context = "\n".join(results["documents"][0])
            return f"ç›¸å…³è®°å¿†:\n{context}"
        return ""

# åœ¨èŠ‚ç‚¹ä¸­ä½¿ç”¨è®°å¿†ç®¡ç†å™¨
def memory_enhanced_node(state: MyState) -> MyState:
    manager = MemoryManager(state["user_id"])

    # è·å–ç›¸å…³ä¸Šä¸‹æ–‡
    context = manager.get_relevant_context(state["current_query"])

    # å¤„ç†æŸ¥è¯¢ï¼ˆåŒ…å«ä¸Šä¸‹æ–‡ï¼‰
    enhanced_query = f"{context}\n\nå½“å‰æŸ¥è¯¢: {state['current_query']}"

    # å­˜å‚¨é‡è¦ä¿¡æ¯
    manager.store_if_important(
        state["current_query"],
        {"timestamp": datetime.now().isoformat()}
    )

    return {**state, "enhanced_context": context}
```

---

## ğŸ¤ Human-in-the-Loop æ¨¡æ¿

### åŸºç¡€å®¡æ ¸æ¨¡æ¿

```python
from langgraph.prebuilt import interrupt

def human_review_node(state: MyState) -> MyState:
    """åŸºç¡€äººå·¥å®¡æ ¸èŠ‚ç‚¹"""
    content_to_review = state["generated_content"]

    # æ£€æŸ¥æ˜¯å¦éœ€è¦äººå·¥å®¡æ ¸
    if needs_human_review(content_to_review):
        # æš‚åœæ‰§è¡Œï¼Œç­‰å¾…äººå·¥è¾“å…¥
        human_feedback = interrupt(
            f"è¯·å®¡æ ¸ä»¥ä¸‹å†…å®¹:\n\n{content_to_review}\n\n"
            f"è¾“å…¥ 'approve' æ‰¹å‡†ï¼Œ'reject' æ‹’ç»ï¼Œæˆ–æä¾›ä¿®æ”¹å»ºè®®ï¼š"
        )

        # å¤„ç†äººå·¥åé¦ˆ
        if human_feedback.lower() == "approve":
            return {**state, "approval_status": "approved"}
        elif human_feedback.lower() == "reject":
            return {**state, "approval_status": "rejected"}
        else:
            return {
                **state,
                "approval_status": "needs_revision",
                "revision_instructions": human_feedback
            }

    # è‡ªåŠ¨æ‰¹å‡†
    return {**state, "approval_status": "auto_approved"}

def needs_human_review(content: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦éœ€è¦äººå·¥å®¡æ ¸"""
    sensitive_keywords = ["æ•æ„Ÿ", "æœºå¯†", "ä¸ªäººä¿¡æ¯", "è´¦å·", "å¯†ç "]
    return any(keyword in content for keyword in sensitive_keywords)
```

### äº¤äº’å¼è¾“å…¥æ”¶é›†

```python
def collect_user_info(state: MyState) -> MyState:
    """äº¤äº’å¼æ”¶é›†ç”¨æˆ·ä¿¡æ¯"""
    collected_info = {}

    # æ”¶é›†å§“å
    if not state.get("user_name"):
        name = interrupt("è¯·è¾“å…¥æ‚¨çš„å§“åï¼š")
        collected_info["user_name"] = name

    # æ”¶é›†é‚®ç®±
    if not state.get("user_email"):
        email = interrupt("è¯·è¾“å…¥æ‚¨çš„é‚®ç®±ï¼š")
        collected_info["user_email"] = email

    # æ”¶é›†åå¥½
    if not state.get("preferences"):
        preferences = interrupt(
            "è¯·é€‰æ‹©æ‚¨çš„åå¥½ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰:\n"
            "1. æŠ€æœ¯ç±»\n"
            "2. å•†ä¸šç±»\n"
            "3. å¨±ä¹ç±»\n"
            "4. æ•™è‚²ç±»\n"
            "è¯·è¾“å…¥æ•°å­—ï¼š"
        )
        collected_info["preferences"] = preferences

    return {**state, **collected_info}
```

### åˆ†æ­¥éª¤ç¡®è®¤æ¨¡æ¿

```python
def multi_step_confirmation(state: MyState) -> MyState:
    """å¤šæ­¥éª¤ç¡®è®¤æµç¨‹"""
    plan = state["execution_plan"]

    # æ˜¾ç¤ºæ‰§è¡Œè®¡åˆ’
    plan_text = "\n".join([f"{i+1}. {step}" for i, step in enumerate(plan)])

    confirmation = interrupt(
        f"å³å°†æ‰§è¡Œä»¥ä¸‹æ­¥éª¤:\n\n{plan_text}\n\n"
        f"è¾“å…¥ 'confirm' ç¡®è®¤æ‰§è¡Œï¼Œ'modify' ä¿®æ”¹è®¡åˆ’ï¼Œ'cancel' å–æ¶ˆï¼š"
    )

    if confirmation.lower() == "confirm":
        return {**state, "execution_status": "confirmed"}
    elif confirmation.lower() == "modify":
        # æ”¶é›†ä¿®æ”¹å»ºè®®
        modifications = interrupt("è¯·æè¿°æ‚¨å¸Œæœ›å¦‚ä½•ä¿®æ”¹è®¡åˆ’ï¼š")
        return {
            **state,
            "execution_status": "needs_modification",
            "modification_request": modifications
        }
    else:
        return {**state, "execution_status": "cancelled"}
```

### åŠ¨æ€è¡¨å•ç”Ÿæˆ

```python
def dynamic_form_node(state: MyState) -> MyState:
    """åŠ¨æ€è¡¨å•èŠ‚ç‚¹"""
    form_config = state["form_config"]
    form_data = {}

    for field in form_config["fields"]:
        field_name = field["name"]
        field_type = field["type"]
        field_prompt = field["prompt"]
        required = field.get("required", False)

        # æ”¶é›†å­—æ®µå€¼
        while True:
            value = interrupt(f"{field_prompt}: ")

            # éªŒè¯è¾“å…¥
            if required and not value.strip():
                interrupt("æ­¤å­—æ®µä¸ºå¿…å¡«é¡¹ï¼Œè¯·é‡æ–°è¾“å…¥ã€‚")
                continue

            if field_type == "email" and value and "@" not in value:
                interrupt("è¯·è¾“å…¥æœ‰æ•ˆçš„é‚®ç®±åœ°å€ã€‚")
                continue

            if field_type == "number":
                try:
                    value = int(value) if value else None
                except ValueError:
                    interrupt("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—ã€‚")
                    continue

            form_data[field_name] = value
            break

    return {**state, "form_data": form_data}
```

---

## ğŸŒŠ æµå¼å¤„ç†æ¨¡æ¿

### åŸºç¡€æµå¼è¾“å‡º

```python
def create_streaming_app():
    """åˆ›å»ºæ”¯æŒæµå¼è¾“å‡ºçš„åº”ç”¨"""

    def streaming_node(state: MyState) -> MyState:
        # æ¨¡æ‹Ÿé•¿æ—¶é—´å¤„ç†
        for i in range(5):
            time.sleep(1)
            # è¿™é‡Œå¯ä»¥è¾“å‡ºä¸­é—´ç»“æœ
            yield {**state, "progress": f"æ­¥éª¤ {i+1}/5 å®Œæˆ"}

        return {**state, "final_result": "å¤„ç†å®Œæˆ"}

    graph = StateGraph(MyState)
    graph.add_node("stream", streaming_node)
    graph.add_edge(START, "stream")
    graph.add_edge("stream", END)

    return graph.compile()

# ä½¿ç”¨æµå¼åº”ç”¨
app = create_streaming_app()

# æµå¼æ‰§è¡Œ
for chunk in app.stream({"input": "test data"}):
    print(f"æ”¶åˆ°æ›´æ–°: {chunk}")
```

### LLM Token æµ

```python
from langchain_openai import ChatOpenAI

def streaming_llm_node(state: MyState) -> MyState:
    """æµå¼ LLM è°ƒç”¨èŠ‚ç‚¹"""
    model = ChatOpenAI(
        model="gpt-4",
        streaming=True,
        temperature=0.7
    )

    messages = [
        ("system", "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹"),
        ("human", state["user_query"])
    ]

    # æµå¼è°ƒç”¨
    response_chunks = []
    for chunk in model.stream(messages):
        if chunk.content:
            response_chunks.append(chunk.content)
            # å®æ—¶è¾“å‡º token
            print(chunk.content, end="", flush=True)

    full_response = "".join(response_chunks)
    return {**state, "llm_response": full_response}

# ä½¿ç”¨ astream_events è·å–è¯¦ç»†çš„æµå¼äº‹ä»¶
async def detailed_streaming():
    async for event in app.astream_events(
        {"user_query": "è§£é‡Šé‡å­è®¡ç®—"},
        version="v1"
    ):
        if event["event"] == "on_llm_stream":
            token = event["data"]["chunk"].content
            if token:
                print(token, end="", flush=True)
```

### è‡ªå®šä¹‰æµå¼æ•°æ®

```python
from langgraph.graph import add_messages

def custom_streaming_node(state: MyState) -> MyState:
    """å‘é€è‡ªå®šä¹‰æµå¼æ•°æ®çš„èŠ‚ç‚¹"""

    # å‘é€è‡ªå®šä¹‰æ›´æ–°
    def send_custom_update(data: Dict[str, Any]):
        # è¿™ä¸ªå‡½æ•°æ¨¡æ‹Ÿå‘é€è‡ªå®šä¹‰æ•°æ®
        # åœ¨å®é™…å®ç°ä¸­ï¼ŒLangGraph ä¼šå¤„ç†è¿™äº›æ›´æ–°
        pass

    # æ¨¡æ‹Ÿå¤„ç†æ­¥éª¤
    steps = ["åˆå§‹åŒ–", "æ•°æ®åŠ è½½", "å¤„ç†", "éªŒè¯", "å®Œæˆ"]

    for i, step in enumerate(steps):
        time.sleep(0.5)

        # å‘é€è¿›åº¦æ›´æ–°
        send_custom_update({
            "type": "progress",
            "step": step,
            "progress": (i + 1) / len(steps),
            "message": f"æ­£åœ¨æ‰§è¡Œ: {step}"
        })

    return {**state, "status": "completed"}

# æµæ¨¡å¼é…ç½®
def stream_with_custom_mode():
    for chunk in app.stream(
        {"input": "process data"},
        stream_mode="custom"  # æ¥æ”¶è‡ªå®šä¹‰æµæ•°æ®
    ):
        if chunk.get("type") == "progress":
            print(f"è¿›åº¦: {chunk['progress']:.1%} - {chunk['message']}")
```

### å¤šæ¨¡å¼æµå¼å¤„ç†

```python
def multi_mode_streaming_demo():
    """æ¼”ç¤ºä¸åŒæµæ¨¡å¼çš„ä½¿ç”¨"""
    input_data = {"query": "åˆ†ææ•°æ®"}

    print("=== Values æ¨¡å¼ (å®Œæ•´çŠ¶æ€) ===")
    for chunk in app.stream(input_data, stream_mode="values"):
        print(f"çŠ¶æ€æ›´æ–°: {chunk}")

    print("\n=== Updates æ¨¡å¼ (å¢é‡æ›´æ–°) ===")
    for chunk in app.stream(input_data, stream_mode="updates"):
        print(f"èŠ‚ç‚¹æ›´æ–°: {chunk}")

    print("\n=== Debug æ¨¡å¼ (è°ƒè¯•ä¿¡æ¯) ===")
    for chunk in app.stream(input_data, stream_mode="debug"):
        print(f"è°ƒè¯•ä¿¡æ¯: {chunk}")

# å¼‚æ­¥å¤šæ¨¡å¼æµ
async def async_multi_mode_streaming():
    """å¼‚æ­¥å¤šæ¨¡å¼æµå¼å¤„ç†"""
    input_data = {"query": "å¼‚æ­¥åˆ†æ"}

    # å¹¶å‘æ‰§è¡Œå¤šç§æµæ¨¡å¼
    import asyncio

    async def stream_values():
        async for chunk in app.astream(input_data, stream_mode="values"):
            print(f"Values: {chunk}")

    async def stream_updates():
        async for chunk in app.astream(input_data, stream_mode="updates"):
            print(f"Updates: {chunk}")

    # å¹¶å‘æ‰§è¡Œ
    await asyncio.gather(stream_values(), stream_updates())
```

---

## ğŸ› ï¸ å·¥å…·é›†æˆæ¨¡æ¿

### åŸºç¡€å·¥å…·å®šä¹‰

```python
from langchain_core.tools import tool
from langgraph.prebuilt import ToolNode

@tool
def calculator(expression: str) -> str:
    """è®¡ç®—æ•°å­¦è¡¨è¾¾å¼

    Args:
        expression: æ•°å­¦è¡¨è¾¾å¼ï¼Œå¦‚ '2 + 3 * 4'
    """
    try:
        result = eval(expression)
        return f"è®¡ç®—ç»“æœ: {result}"
    except Exception as e:
        return f"è®¡ç®—é”™è¯¯: {str(e)}"

@tool
def search_web(query: str) -> str:
    """æœç´¢ç½‘ç»œä¿¡æ¯

    Args:
        query: æœç´¢æŸ¥è¯¢è¯
    """
    # æ¨¡æ‹Ÿç½‘ç»œæœç´¢
    return f"æœç´¢ '{query}' çš„ç»“æœ: [æ¨¡æ‹Ÿæœç´¢ç»“æœ]"

@tool
def save_file(filename: str, content: str) -> str:
    """ä¿å­˜æ–‡ä»¶

    Args:
        filename: æ–‡ä»¶å
        content: æ–‡ä»¶å†…å®¹
    """
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(content)
        return f"æ–‡ä»¶ {filename} ä¿å­˜æˆåŠŸ"
    except Exception as e:
        return f"ä¿å­˜å¤±è´¥: {str(e)}"

# åˆ›å»ºå·¥å…·èŠ‚ç‚¹
tools = [calculator, search_web, save_file]
tool_node = ToolNode(tools)
```

### æ™ºèƒ½ä½“å·¥å…·è°ƒç”¨

```python
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent

def create_tool_agent():
    """åˆ›å»ºå¸¦å·¥å…·çš„æ™ºèƒ½ä½“"""

    # åˆå§‹åŒ–æ¨¡å‹
    model = ChatOpenAI(model="gpt-4", temperature=0)

    # ç»‘å®šå·¥å…·
    model_with_tools = model.bind_tools(tools)

    # åˆ›å»º ReAct æ™ºèƒ½ä½“
    agent = create_react_agent(model, tools)

    return agent

# ä½¿ç”¨å·¥å…·æ™ºèƒ½ä½“
def tool_agent_demo():
    agent = create_tool_agent()

    # æµ‹è¯•æ•°å­¦è®¡ç®—
    result = agent.invoke({
        "messages": [("user", "è®¡ç®— 15 * 8 + 23")]
    })
    print(result["messages"][-1].content)

    # æµ‹è¯•æœç´¢åŠŸèƒ½
    result = agent.invoke({
        "messages": [("user", "æœç´¢Pythonæ•™ç¨‹")]
    })
    print(result["messages"][-1].content)
```

### å·¥å…·é”™è¯¯å¤„ç†

```python
@tool
def robust_api_call(url: str, method: str = "GET") -> str:
    """å¥å£®çš„ API è°ƒç”¨å·¥å…·

    Args:
        url: API ç«¯ç‚¹ URL
        method: HTTP æ–¹æ³• (GET, POST, etc.)
    """
    import requests
    from requests.adapters import HTTPAdapter
    from urllib3.util.retry import Retry

    # é…ç½®é‡è¯•ç­–ç•¥
    session = requests.Session()
    retry_strategy = Retry(
        total=3,
        backoff_factor=1,
        status_forcelist=[429, 500, 502, 503, 504],
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)

    try:
        response = session.request(method, url, timeout=10)
        response.raise_for_status()
        return f"APIè°ƒç”¨æˆåŠŸ: {response.status_code}"
    except requests.exceptions.RequestException as e:
        return f"APIè°ƒç”¨å¤±è´¥: {str(e)}"

def create_error_handling_tool_node():
    """åˆ›å»ºå¸¦é”™è¯¯å¤„ç†çš„å·¥å…·èŠ‚ç‚¹"""

    def safe_tool_execution(state: MyState) -> MyState:
        """å®‰å…¨çš„å·¥å…·æ‰§è¡ŒåŒ…è£…å™¨"""
        try:
            # æ‰§è¡Œå·¥å…·è°ƒç”¨
            tool_result = tool_node.invoke(state)
            return {**state, **tool_result}
        except Exception as e:
            # å·¥å…·æ‰§è¡Œå¤±è´¥çš„é™çº§å¤„ç†
            return {
                **state,
                "tool_error": str(e),
                "fallback_response": "å·¥å…·æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•"
            }

    return safe_tool_execution
```

### åŠ¨æ€å·¥å…·é€‰æ‹©

```python
class DynamicToolSelector:
    def __init__(self):
        self.tools = {
            "math": calculator,
            "search": search_web,
            "file": save_file
        }

    def select_tools(self, intent: str) -> List:
        """æ ¹æ®æ„å›¾é€‰æ‹©åˆé€‚çš„å·¥å…·"""
        tool_mapping = {
            "calculate": ["math"],
            "search": ["search"],
            "save": ["file"],
            "general": ["math", "search", "file"]
        }

        selected_tool_names = tool_mapping.get(intent, ["general"])
        return [self.tools[name] for name in selected_tool_names if name in self.tools]

def intent_based_tool_node(state: MyState) -> MyState:
    """åŸºäºæ„å›¾çš„å·¥å…·é€‰æ‹©èŠ‚ç‚¹"""
    user_intent = classify_intent(state["user_input"])

    selector = DynamicToolSelector()
    available_tools = selector.select_tools(user_intent)

    # åˆ›å»ºä¸“é—¨çš„å·¥å…·èŠ‚ç‚¹
    specialized_tool_node = ToolNode(available_tools)

    # æ‰§è¡Œå·¥å…·è°ƒç”¨
    return specialized_tool_node.invoke(state)

def classify_intent(user_input: str) -> str:
    """ç®€å•çš„æ„å›¾åˆ†ç±»"""
    if any(word in user_input.lower() for word in ["è®¡ç®—", "ç®—", "æ•°å­¦"]):
        return "calculate"
    elif any(word in user_input.lower() for word in ["æœç´¢", "æŸ¥æ‰¾", "æ‰¾"]):
        return "search"
    elif any(word in user_input.lower() for word in ["ä¿å­˜", "å­˜å‚¨", "å†™å…¥"]):
        return "save"
    else:
        return "general"
```

---

## ğŸš€ éƒ¨ç½²é…ç½®æ¨¡æ¿

### Docker é…ç½®

```dockerfile
# Dockerfile
FROM python:3.11-slim

WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .

# å®‰è£… Python ä¾èµ–
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PYTHONPATH=/app
ENV PYTHONUNBUFFERED=1

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["python", "-m", "langgraph_app.server"]
```

```yaml
# docker-compose.yml
version: '3.8'

services:
  langgraph-app:
    build: .
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - LANGGRAPH_LICENSE_KEY=${LANGGRAPH_LICENSE_KEY}
      - DATABASE_URL=postgresql://user:pass@postgres:5432/langgraph
    depends_on:
      - postgres
      - redis
    volumes:
      - ./data:/app/data

  postgres:
    image: postgres:15
    environment:
      - POSTGRES_DB=langgraph
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=pass
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

volumes:
  postgres_data:
```

### LangGraph é…ç½®æ–‡ä»¶

```json
{
  "dependencies": [
    "langgraph",
    "langchain",
    "langchain-openai",
    "langchain-community"
  ],
  "graphs": {
    "my_agent": "./src/agent.py:graph"
  },
  "env": [
    "OPENAI_API_KEY",
    "LANGGRAPH_LICENSE_KEY"
  ],
  "python_version": "3.11",
  "pip_config_file": "./pip.conf",
  "dockerfile_lines": [
    "RUN apt-get update && apt-get install -y curl",
    "RUN pip install --upgrade pip"
  ]
}
```

### ç¯å¢ƒé…ç½®æ¨¡æ¿

```bash
# .env.example
# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4

# LangGraph Platform
LANGGRAPH_LICENSE_KEY=your_license_key_here
LANGGRAPH_API_URL=https://api.langgraph.dev

# Database Configuration
DATABASE_URL=postgresql://user:password@localhost:5432/langgraph
REDIS_URL=redis://localhost:6379/0

# Application Settings
APP_NAME=my-langgraph-app
APP_VERSION=1.0.0
DEBUG=false
LOG_LEVEL=INFO

# Security
SECRET_KEY=your_secret_key_here
ALLOWED_HOSTS=localhost,127.0.0.1

# Feature Flags
ENABLE_HUMAN_IN_THE_LOOP=true
ENABLE_MEMORY=true
ENABLE_STREAMING=true
```

### Kubernetes éƒ¨ç½²é…ç½®

```yaml
# k8s-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: langgraph-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: langgraph-app
  template:
    metadata:
      labels:
        app: langgraph-app
    spec:
      containers:
      - name: langgraph-app
        image: my-langgraph-app:latest
        ports:
        - containerPort: 8000
        env:
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: api-secrets
              key: openai-api-key
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: db-secrets
              key: database-url
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5

---
apiVersion: v1
kind: Service
metadata:
  name: langgraph-service
spec:
  selector:
    app: langgraph-app
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
  type: LoadBalancer
```

### æœåŠ¡å™¨å¯åŠ¨è„šæœ¬

```python
# server.py
import os
import uvicorn
from langgraph_app import create_app

def main():
    # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
    host = os.getenv("HOST", "0.0.0.0")
    port = int(os.getenv("PORT", 8000))
    workers = int(os.getenv("WORKERS", 1))
    log_level = os.getenv("LOG_LEVEL", "info").lower()

    # åˆ›å»ºåº”ç”¨
    app = create_app()

    # å¯åŠ¨æœåŠ¡å™¨
    uvicorn.run(
        app,
        host=host,
        port=port,
        workers=workers,
        log_level=log_level,
        access_log=True
    )

if __name__ == "__main__":
    main()
```

---

## ğŸ”— ä½¿ç”¨æŒ‡å—

### å¤åˆ¶ä»£ç æ¨¡æ¿

1. **é€‰æ‹©åˆé€‚çš„æ¨¡æ¿**ï¼šæ ¹æ®ä½ çš„éœ€æ±‚é€‰æ‹©ç›¸åº”çš„ä»£ç ç‰‡æ®µ
2. **ä¿®æ”¹é…ç½®**ï¼šæ ¹æ®ä½ çš„å…·ä½“æƒ…å†µä¿®æ”¹å‚æ•°å’Œé…ç½®
3. **æµ‹è¯•åŠŸèƒ½**ï¼šåœ¨å°èŒƒå›´å†…æµ‹è¯•ä»£ç ç‰‡æ®µçš„åŠŸèƒ½
4. **é›†æˆåˆ°é¡¹ç›®**ï¼šå°†æµ‹è¯•é€šè¿‡çš„ä»£ç é›†æˆåˆ°ä½ çš„ä¸»é¡¹ç›®ä¸­

### æœ€ä½³å®è·µ

- ğŸ“ **æ–‡æ¡£åŒ–**ï¼šä¸ºæ¯ä¸ªè‡ªå®šä¹‰èŠ‚ç‚¹æ·»åŠ æ¸…æ™°çš„æ–‡æ¡£è¯´æ˜
- ğŸ§ª **æµ‹è¯•**ï¼šä¸ºå…³é”®èŠ‚ç‚¹ç¼–å†™å•å…ƒæµ‹è¯•
- ğŸ” **ç›‘æ§**ï¼šæ·»åŠ é€‚å½“çš„æ—¥å¿—å’Œç›‘æ§
- ğŸ›¡ï¸ **é”™è¯¯å¤„ç†**ï¼šå®ç°å¥å£®çš„é”™è¯¯å¤„ç†æœºåˆ¶
- ğŸ”„ **ç‰ˆæœ¬æ§åˆ¶**ï¼šä½¿ç”¨ Git ç®¡ç†ä»£ç å˜æ›´

---

## ğŸ”— ç›¸å…³èµ„æº

- ğŸš€ [API é€ŸæŸ¥è¡¨](./APIé€ŸæŸ¥.md)
- ğŸ› [é”™è¯¯ç å¯¹ç…§](./é”™è¯¯ç å¯¹ç…§.md)
- â“ [å¸¸è§é—®é¢˜ FAQ](./FAQ.md)
- ğŸ“– [LangGraph å®˜æ–¹æ–‡æ¡£](https://langchain-ai.github.io/langgraph/)
- ğŸ’» [é¡¹ç›®ç¤ºä¾‹](../08-é¡¹ç›®æ¡ˆä¾‹/)

---

*ä»£ç ç‰‡æ®µåº“ä¼šæŒç»­æ›´æ–°ï¼Œæ¬¢è¿è´¡çŒ®æ›´å¤šæœ‰ç”¨çš„æ¨¡æ¿ï¼* ğŸ¤