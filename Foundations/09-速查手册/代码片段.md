# LangGraph 代码片段库

> 💡 **即插即用**：经过测试的代码模板，可直接复制使用

## 📚 模板分类

- [基础图结构](#基础图结构)
- [状态管理模式](#状态管理模式)
- [节点功能模板](#节点功能模板)
- [条件路由模板](#条件路由模板)
- [Memory 记忆模板](#memory-记忆模板)
- [Human-in-the-Loop 模板](#human-in-the-loop-模板)
- [流式处理模板](#流式处理模板)
- [工具集成模板](#工具集成模板)
- [部署配置模板](#部署配置模板)

---

## 🏗️ 基础图结构

### 最简单的图

```python
from typing import TypedDict
from langgraph.graph import StateGraph, START, END

class SimpleState(TypedDict):
    message: str

def process_node(state: SimpleState) -> SimpleState:
    return {"message": f"处理完成: {state['message']}"}

# 构建图
graph = StateGraph(SimpleState)
graph.add_node("process", process_node)
graph.add_edge(START, "process")
graph.add_edge("process", END)

# 编译并运行
app = graph.compile()
result = app.invoke({"message": "Hello World"})
print(result)  # {'message': '处理完成: Hello World'}
```

### 序列化处理图

```python
from typing import TypedDict

class SequentialState(TypedDict):
    input_text: str
    cleaned_text: str
    analyzed_text: str
    final_result: str

def clean_text(state: SequentialState) -> SequentialState:
    cleaned = state["input_text"].strip().lower()
    return {**state, "cleaned_text": cleaned}

def analyze_text(state: SequentialState) -> SequentialState:
    word_count = len(state["cleaned_text"].split())
    analyzed = f"分析结果: {word_count} 个词"
    return {**state, "analyzed_text": analyzed}

def generate_result(state: SequentialState) -> SequentialState:
    result = f"最终结果: {state['analyzed_text']}"
    return {**state, "final_result": result}

# 构建序列图
graph = StateGraph(SequentialState)
graph.add_node("clean", clean_text)
graph.add_node("analyze", analyze_text)
graph.add_node("generate", generate_result)

graph.add_edge(START, "clean")
graph.add_edge("clean", "analyze")
graph.add_edge("analyze", "generate")
graph.add_edge("generate", END)

app = graph.compile()
```

### 带分支和合并的图

```python
from typing import TypedDict, List

class BranchState(TypedDict):
    input_data: str
    branch_a_result: str
    branch_b_result: str
    merged_result: str

def process_branch_a(state: BranchState) -> BranchState:
    result = f"分支A处理: {state['input_data']}"
    return {**state, "branch_a_result": result}

def process_branch_b(state: BranchState) -> BranchState:
    result = f"分支B处理: {state['input_data']}"
    return {**state, "branch_b_result": result}

def merge_results(state: BranchState) -> BranchState:
    merged = f"合并结果: {state['branch_a_result']} + {state['branch_b_result']}"
    return {**state, "merged_result": merged}

# 构建分支图
graph = StateGraph(BranchState)
graph.add_node("branch_a", process_branch_a)
graph.add_node("branch_b", process_branch_b)
graph.add_node("merge", merge_results)

# 并行分支
graph.add_edge(START, "branch_a")
graph.add_edge(START, "branch_b")
# 等待两个分支完成后合并
graph.add_edge(["branch_a", "branch_b"], "merge")
graph.add_edge("merge", END)

app = graph.compile()
```

---

## 🗂️ 状态管理模式

### 基础状态结构

```python
from typing import TypedDict, Optional, List, Dict, Any

class BasicState(TypedDict):
    # 用户输入
    user_input: str

    # 处理状态
    is_processed: bool

    # 结果数据
    result: Optional[str]

    # 错误信息
    error: Optional[str]

    # 元数据
    timestamp: str
    session_id: str
```

### 消息历史状态

```python
from typing import Annotated, Sequence
from langchain_core.messages import BaseMessage
from langgraph.graph.message import add_messages

class ChatState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]
    user_id: str
    conversation_id: str

# 使用示例
from langchain_core.messages import HumanMessage, AIMessage

def chat_node(state: ChatState) -> ChatState:
    # 获取最新消息
    last_message = state["messages"][-1]

    # 生成回复
    response = f"收到消息: {last_message.content}"

    # 添加AI回复到消息历史
    return {
        "messages": [AIMessage(content=response)]
    }
```

### 复杂状态结构

```python
from typing import TypedDict, List, Dict, Any, Literal
from datetime import datetime

class ComplexState(TypedDict):
    # 用户信息
    user_id: str
    session_id: str

    # 输入数据
    query: str
    context: Dict[str, Any]

    # 处理状态
    current_step: Literal["analyzing", "searching", "generating", "reviewing"]
    step_count: int
    max_steps: int

    # 中间结果
    analysis_result: Optional[Dict[str, Any]]
    search_results: List[Dict[str, str]]

    # 最终输出
    generated_content: Optional[str]
    confidence_score: Optional[float]

    # 错误处理
    errors: List[str]
    retry_count: int

    # 元数据
    timestamp: str
    processing_time: Optional[float]
```

### 状态验证器

```python
def validate_state(state: ComplexState) -> ComplexState:
    """验证状态的有效性"""
    errors = []

    # 检查必需字段
    if not state.get("user_id"):
        errors.append("缺少 user_id")

    if not state.get("query"):
        errors.append("缺少 query")

    # 检查步数限制
    if state.get("step_count", 0) > state.get("max_steps", 25):
        errors.append("超过最大步数限制")

    # 更新错误列表
    if errors:
        current_errors = state.get("errors", [])
        return {**state, "errors": current_errors + errors}

    return state

def safe_node(func):
    """状态验证装饰器"""
    def wrapper(state: ComplexState) -> ComplexState:
        # 前置验证
        validated_state = validate_state(state)
        if validated_state.get("errors"):
            return validated_state

        # 执行节点逻辑
        try:
            result = func(validated_state)
            return validate_state(result)
        except Exception as e:
            return {
                **validated_state,
                "errors": validated_state.get("errors", []) + [str(e)]
            }

    return wrapper
```

---

## 🎯 节点功能模板

### 基础节点模板

```python
def basic_node_template(state: MyState) -> MyState:
    """基础节点模板"""
    try:
        # 1. 获取输入数据
        input_data = state["input_field"]

        # 2. 执行核心逻辑
        result = process_data(input_data)

        # 3. 返回更新后的状态
        return {
            **state,
            "output_field": result,
            "status": "success",
            "timestamp": datetime.now().isoformat()
        }

    except Exception as e:
        # 4. 错误处理
        return {
            **state,
            "error": str(e),
            "status": "failed",
            "retry_count": state.get("retry_count", 0) + 1
        }
```

### LLM 调用节点

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage

def llm_node(state: MyState) -> MyState:
    """LLM 调用节点"""
    # 初始化模型
    model = ChatOpenAI(model="gpt-4", temperature=0.7)

    # 构建消息
    messages = [
        SystemMessage(content="你是一个有用的助手"),
        HumanMessage(content=state["user_query"])
    ]

    # 调用 LLM
    try:
        response = model.invoke(messages)
        return {
            **state,
            "llm_response": response.content,
            "tokens_used": response.response_metadata.get("token_usage", {})
        }
    except Exception as e:
        return {**state, "error": f"LLM调用失败: {str(e)}"}
```

### 数据处理节点

```python
import json
from typing import Any

def data_processing_node(state: MyState) -> MyState:
    """数据处理节点"""
    raw_data = state["raw_data"]

    # 数据清洗
    cleaned_data = []
    for item in raw_data:
        if isinstance(item, dict) and "content" in item:
            cleaned_item = {
                "content": item["content"].strip(),
                "metadata": item.get("metadata", {}),
                "timestamp": item.get("timestamp", datetime.now().isoformat())
            }
            cleaned_data.append(cleaned_item)

    # 数据转换
    processed_data = {
        "total_items": len(cleaned_data),
        "items": cleaned_data,
        "summary": f"处理了 {len(cleaned_data)} 条数据"
    }

    return {
        **state,
        "processed_data": processed_data,
        "processing_status": "completed"
    }
```

### 异步节点模板

```python
import asyncio
import aiohttp

async def async_api_node(state: MyState) -> MyState:
    """异步 API 调用节点"""
    url = "https://api.example.com/data"
    params = {"query": state["search_query"]}

    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    return {
                        **state,
                        "api_result": data,
                        "api_status": "success"
                    }
                else:
                    return {
                        **state,
                        "api_error": f"API返回状态码: {response.status}",
                        "api_status": "failed"
                    }
    except Exception as e:
        return {
            **state,
            "api_error": str(e),
            "api_status": "failed"
        }

# 在图中使用异步节点
def sync_wrapper(state: MyState) -> MyState:
    """同步包装器"""
    return asyncio.run(async_api_node(state))
```

---

## 🔀 条件路由模板

### 简单二分支路由

```python
def simple_router(state: MyState) -> str:
    """简单的二分支路由"""
    if state.get("condition_field", False):
        return "success_path"
    else:
        return "error_path"

# 添加到图
graph.add_conditional_edges(
    "decision_node",
    simple_router,
    {
        "success_path": "success_node",
        "error_path": "error_node"
    }
)
```

### 多分支路由

```python
def multi_branch_router(state: MyState) -> str:
    """多分支路由逻辑"""
    score = state.get("confidence_score", 0)

    if score >= 0.8:
        return "high_confidence"
    elif score >= 0.5:
        return "medium_confidence"
    elif score >= 0.2:
        return "low_confidence"
    else:
        return "no_confidence"

graph.add_conditional_edges(
    "evaluation_node",
    multi_branch_router,
    {
        "high_confidence": "proceed_node",
        "medium_confidence": "review_node",
        "low_confidence": "retry_node",
        "no_confidence": "error_node"
    }
)
```

### 循环条件路由

```python
def loop_router(state: MyState) -> str:
    """循环控制路由"""
    current_step = state.get("current_step", 0)
    max_steps = state.get("max_steps", 5)
    has_converged = state.get("has_converged", False)

    # 检查终止条件
    if has_converged:
        return "converged"
    elif current_step >= max_steps:
        return "max_steps_reached"
    else:
        return "continue_loop"

graph.add_conditional_edges(
    "iteration_node",
    loop_router,
    {
        "continue_loop": "iteration_node",  # 循环回自身
        "converged": "success_node",
        "max_steps_reached": "timeout_node"
    }
)
```

### 基于内容的路由

```python
def content_based_router(state: MyState) -> str:
    """基于内容类型的路由"""
    content_type = state.get("content_type", "").lower()

    if "question" in content_type or "?" in state.get("user_input", ""):
        return "question_handler"
    elif "command" in content_type or state.get("user_input", "").startswith("/"):
        return "command_handler"
    elif "search" in content_type:
        return "search_handler"
    else:
        return "general_handler"

# 使用正则表达式的高级内容路由
import re

def advanced_content_router(state: MyState) -> str:
    """高级内容路由"""
    user_input = state.get("user_input", "").lower()

    # 定义模式
    patterns = {
        "search": r"(搜索|查找|找|search|find)",
        "translate": r"(翻译|translate|转换)",
        "analyze": r"(分析|analyze|解析|评估)",
        "summarize": r"(总结|摘要|概括|summarize)"
    }

    # 匹配模式
    for intent, pattern in patterns.items():
        if re.search(pattern, user_input):
            return f"{intent}_handler"

    return "default_handler"
```

---

## 🧠 Memory 记忆模板

### 短期记忆管理

```python
from langchain_core.messages import BaseMessage, RemoveMessage
from typing import Annotated, Sequence

class ShortTermMemoryState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]
    max_messages: int

def trim_messages(state: ShortTermMemoryState) -> ShortTermMemoryState:
    """修剪消息历史，保持短期记忆"""
    messages = state["messages"]
    max_messages = state.get("max_messages", 10)

    if len(messages) > max_messages:
        # 保留系统消息和最近的消息
        system_messages = [msg for msg in messages if msg.type == "system"]
        recent_messages = messages[-max_messages:]

        # 创建删除列表
        messages_to_remove = []
        for msg in messages:
            if msg not in system_messages and msg not in recent_messages:
                messages_to_remove.append(RemoveMessage(id=msg.id))

        return {"messages": messages_to_remove}

    return {}
```

### 长期记忆存储

```python
import chromadb
from typing import Dict, List

class LongTermMemoryState(TypedDict):
    user_id: str
    current_query: str
    retrieved_memories: List[Dict[str, Any]]

def store_memory(user_id: str, content: str, metadata: Dict[str, Any] = None):
    """存储长期记忆"""
    client = chromadb.PersistentClient(path="./memory_db")
    collection = client.get_or_create_collection(f"user_{user_id}_memories")

    collection.add(
        documents=[content],
        metadatas=[metadata or {}],
        ids=[f"memory_{len(collection.get()['documents'])}"]
    )

def retrieve_memories(state: LongTermMemoryState) -> LongTermMemoryState:
    """检索相关记忆"""
    user_id = state["user_id"]
    query = state["current_query"]

    try:
        client = chromadb.PersistentClient(path="./memory_db")
        collection = client.get_or_create_collection(f"user_{user_id}_memories")

        results = collection.query(
            query_texts=[query],
            n_results=5
        )

        memories = []
        for i, doc in enumerate(results["documents"][0]):
            memories.append({
                "content": doc,
                "metadata": results["metadatas"][0][i],
                "distance": results["distances"][0][i]
            })

        return {**state, "retrieved_memories": memories}

    except Exception as e:
        return {**state, "retrieved_memories": [], "memory_error": str(e)}

def update_memory_node(state: LongTermMemoryState) -> LongTermMemoryState:
    """更新记忆节点"""
    user_id = state["user_id"]
    current_interaction = state["current_query"]

    # 存储当前交互
    store_memory(
        user_id=user_id,
        content=current_interaction,
        metadata={
            "timestamp": datetime.now().isoformat(),
            "interaction_type": "query"
        }
    )

    return retrieve_memories(state)
```

### 记忆管理工具

```python
class MemoryManager:
    def __init__(self, user_id: str, max_short_term: int = 10):
        self.user_id = user_id
        self.max_short_term = max_short_term
        self.client = chromadb.PersistentClient(path="./memory_db")
        self.collection = self.client.get_or_create_collection(
            f"user_{user_id}_memories"
        )

    def should_store_long_term(self, content: str) -> bool:
        """判断是否应该存储为长期记忆"""
        # 简单的启发式规则
        important_keywords = ["重要", "记住", "保存", "下次", "总是"]
        return any(keyword in content for keyword in important_keywords)

    def store_if_important(self, content: str, metadata: Dict = None):
        """如果重要则存储"""
        if self.should_store_long_term(content):
            self.collection.add(
                documents=[content],
                metadatas=[metadata or {}],
                ids=[f"memory_{datetime.now().timestamp()}"]
            )

    def get_relevant_context(self, query: str, top_k: int = 3) -> str:
        """获取相关上下文"""
        results = self.collection.query(
            query_texts=[query],
            n_results=top_k
        )

        if results["documents"]:
            context = "\n".join(results["documents"][0])
            return f"相关记忆:\n{context}"
        return ""

# 在节点中使用记忆管理器
def memory_enhanced_node(state: MyState) -> MyState:
    manager = MemoryManager(state["user_id"])

    # 获取相关上下文
    context = manager.get_relevant_context(state["current_query"])

    # 处理查询（包含上下文）
    enhanced_query = f"{context}\n\n当前查询: {state['current_query']}"

    # 存储重要信息
    manager.store_if_important(
        state["current_query"],
        {"timestamp": datetime.now().isoformat()}
    )

    return {**state, "enhanced_context": context}
```

---

## 🤝 Human-in-the-Loop 模板

### 基础审核模板

```python
from langgraph.prebuilt import interrupt

def human_review_node(state: MyState) -> MyState:
    """基础人工审核节点"""
    content_to_review = state["generated_content"]

    # 检查是否需要人工审核
    if needs_human_review(content_to_review):
        # 暂停执行，等待人工输入
        human_feedback = interrupt(
            f"请审核以下内容:\n\n{content_to_review}\n\n"
            f"输入 'approve' 批准，'reject' 拒绝，或提供修改建议："
        )

        # 处理人工反馈
        if human_feedback.lower() == "approve":
            return {**state, "approval_status": "approved"}
        elif human_feedback.lower() == "reject":
            return {**state, "approval_status": "rejected"}
        else:
            return {
                **state,
                "approval_status": "needs_revision",
                "revision_instructions": human_feedback
            }

    # 自动批准
    return {**state, "approval_status": "auto_approved"}

def needs_human_review(content: str) -> bool:
    """判断是否需要人工审核"""
    sensitive_keywords = ["敏感", "机密", "个人信息", "账号", "密码"]
    return any(keyword in content for keyword in sensitive_keywords)
```

### 交互式输入收集

```python
def collect_user_info(state: MyState) -> MyState:
    """交互式收集用户信息"""
    collected_info = {}

    # 收集姓名
    if not state.get("user_name"):
        name = interrupt("请输入您的姓名：")
        collected_info["user_name"] = name

    # 收集邮箱
    if not state.get("user_email"):
        email = interrupt("请输入您的邮箱：")
        collected_info["user_email"] = email

    # 收集偏好
    if not state.get("preferences"):
        preferences = interrupt(
            "请选择您的偏好（用逗号分隔）:\n"
            "1. 技术类\n"
            "2. 商业类\n"
            "3. 娱乐类\n"
            "4. 教育类\n"
            "请输入数字："
        )
        collected_info["preferences"] = preferences

    return {**state, **collected_info}
```

### 分步骤确认模板

```python
def multi_step_confirmation(state: MyState) -> MyState:
    """多步骤确认流程"""
    plan = state["execution_plan"]

    # 显示执行计划
    plan_text = "\n".join([f"{i+1}. {step}" for i, step in enumerate(plan)])

    confirmation = interrupt(
        f"即将执行以下步骤:\n\n{plan_text}\n\n"
        f"输入 'confirm' 确认执行，'modify' 修改计划，'cancel' 取消："
    )

    if confirmation.lower() == "confirm":
        return {**state, "execution_status": "confirmed"}
    elif confirmation.lower() == "modify":
        # 收集修改建议
        modifications = interrupt("请描述您希望如何修改计划：")
        return {
            **state,
            "execution_status": "needs_modification",
            "modification_request": modifications
        }
    else:
        return {**state, "execution_status": "cancelled"}
```

### 动态表单生成

```python
def dynamic_form_node(state: MyState) -> MyState:
    """动态表单节点"""
    form_config = state["form_config"]
    form_data = {}

    for field in form_config["fields"]:
        field_name = field["name"]
        field_type = field["type"]
        field_prompt = field["prompt"]
        required = field.get("required", False)

        # 收集字段值
        while True:
            value = interrupt(f"{field_prompt}: ")

            # 验证输入
            if required and not value.strip():
                interrupt("此字段为必填项，请重新输入。")
                continue

            if field_type == "email" and value and "@" not in value:
                interrupt("请输入有效的邮箱地址。")
                continue

            if field_type == "number":
                try:
                    value = int(value) if value else None
                except ValueError:
                    interrupt("请输入有效的数字。")
                    continue

            form_data[field_name] = value
            break

    return {**state, "form_data": form_data}
```

---

## 🌊 流式处理模板

### 基础流式输出

```python
def create_streaming_app():
    """创建支持流式输出的应用"""

    def streaming_node(state: MyState) -> MyState:
        # 模拟长时间处理
        for i in range(5):
            time.sleep(1)
            # 这里可以输出中间结果
            yield {**state, "progress": f"步骤 {i+1}/5 完成"}

        return {**state, "final_result": "处理完成"}

    graph = StateGraph(MyState)
    graph.add_node("stream", streaming_node)
    graph.add_edge(START, "stream")
    graph.add_edge("stream", END)

    return graph.compile()

# 使用流式应用
app = create_streaming_app()

# 流式执行
for chunk in app.stream({"input": "test data"}):
    print(f"收到更新: {chunk}")
```

### LLM Token 流

```python
from langchain_openai import ChatOpenAI

def streaming_llm_node(state: MyState) -> MyState:
    """流式 LLM 调用节点"""
    model = ChatOpenAI(
        model="gpt-4",
        streaming=True,
        temperature=0.7
    )

    messages = [
        ("system", "你是一个有用的助手"),
        ("human", state["user_query"])
    ]

    # 流式调用
    response_chunks = []
    for chunk in model.stream(messages):
        if chunk.content:
            response_chunks.append(chunk.content)
            # 实时输出 token
            print(chunk.content, end="", flush=True)

    full_response = "".join(response_chunks)
    return {**state, "llm_response": full_response}

# 使用 astream_events 获取详细的流式事件
async def detailed_streaming():
    async for event in app.astream_events(
        {"user_query": "解释量子计算"},
        version="v1"
    ):
        if event["event"] == "on_llm_stream":
            token = event["data"]["chunk"].content
            if token:
                print(token, end="", flush=True)
```

### 自定义流式数据

```python
from langgraph.graph import add_messages

def custom_streaming_node(state: MyState) -> MyState:
    """发送自定义流式数据的节点"""

    # 发送自定义更新
    def send_custom_update(data: Dict[str, Any]):
        # 这个函数模拟发送自定义数据
        # 在实际实现中，LangGraph 会处理这些更新
        pass

    # 模拟处理步骤
    steps = ["初始化", "数据加载", "处理", "验证", "完成"]

    for i, step in enumerate(steps):
        time.sleep(0.5)

        # 发送进度更新
        send_custom_update({
            "type": "progress",
            "step": step,
            "progress": (i + 1) / len(steps),
            "message": f"正在执行: {step}"
        })

    return {**state, "status": "completed"}

# 流模式配置
def stream_with_custom_mode():
    for chunk in app.stream(
        {"input": "process data"},
        stream_mode="custom"  # 接收自定义流数据
    ):
        if chunk.get("type") == "progress":
            print(f"进度: {chunk['progress']:.1%} - {chunk['message']}")
```

### 多模式流式处理

```python
def multi_mode_streaming_demo():
    """演示不同流模式的使用"""
    input_data = {"query": "分析数据"}

    print("=== Values 模式 (完整状态) ===")
    for chunk in app.stream(input_data, stream_mode="values"):
        print(f"状态更新: {chunk}")

    print("\n=== Updates 模式 (增量更新) ===")
    for chunk in app.stream(input_data, stream_mode="updates"):
        print(f"节点更新: {chunk}")

    print("\n=== Debug 模式 (调试信息) ===")
    for chunk in app.stream(input_data, stream_mode="debug"):
        print(f"调试信息: {chunk}")

# 异步多模式流
async def async_multi_mode_streaming():
    """异步多模式流式处理"""
    input_data = {"query": "异步分析"}

    # 并发执行多种流模式
    import asyncio

    async def stream_values():
        async for chunk in app.astream(input_data, stream_mode="values"):
            print(f"Values: {chunk}")

    async def stream_updates():
        async for chunk in app.astream(input_data, stream_mode="updates"):
            print(f"Updates: {chunk}")

    # 并发执行
    await asyncio.gather(stream_values(), stream_updates())
```

---

## 🛠️ 工具集成模板

### 基础工具定义

```python
from langchain_core.tools import tool
from langgraph.prebuilt import ToolNode

@tool
def calculator(expression: str) -> str:
    """计算数学表达式

    Args:
        expression: 数学表达式，如 '2 + 3 * 4'
    """
    try:
        result = eval(expression)
        return f"计算结果: {result}"
    except Exception as e:
        return f"计算错误: {str(e)}"

@tool
def search_web(query: str) -> str:
    """搜索网络信息

    Args:
        query: 搜索查询词
    """
    # 模拟网络搜索
    return f"搜索 '{query}' 的结果: [模拟搜索结果]"

@tool
def save_file(filename: str, content: str) -> str:
    """保存文件

    Args:
        filename: 文件名
        content: 文件内容
    """
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(content)
        return f"文件 {filename} 保存成功"
    except Exception as e:
        return f"保存失败: {str(e)}"

# 创建工具节点
tools = [calculator, search_web, save_file]
tool_node = ToolNode(tools)
```

### 智能体工具调用

```python
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent

def create_tool_agent():
    """创建带工具的智能体"""

    # 初始化模型
    model = ChatOpenAI(model="gpt-4", temperature=0)

    # 绑定工具
    model_with_tools = model.bind_tools(tools)

    # 创建 ReAct 智能体
    agent = create_react_agent(model, tools)

    return agent

# 使用工具智能体
def tool_agent_demo():
    agent = create_tool_agent()

    # 测试数学计算
    result = agent.invoke({
        "messages": [("user", "计算 15 * 8 + 23")]
    })
    print(result["messages"][-1].content)

    # 测试搜索功能
    result = agent.invoke({
        "messages": [("user", "搜索Python教程")]
    })
    print(result["messages"][-1].content)
```

### 工具错误处理

```python
@tool
def robust_api_call(url: str, method: str = "GET") -> str:
    """健壮的 API 调用工具

    Args:
        url: API 端点 URL
        method: HTTP 方法 (GET, POST, etc.)
    """
    import requests
    from requests.adapters import HTTPAdapter
    from urllib3.util.retry import Retry

    # 配置重试策略
    session = requests.Session()
    retry_strategy = Retry(
        total=3,
        backoff_factor=1,
        status_forcelist=[429, 500, 502, 503, 504],
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)

    try:
        response = session.request(method, url, timeout=10)
        response.raise_for_status()
        return f"API调用成功: {response.status_code}"
    except requests.exceptions.RequestException as e:
        return f"API调用失败: {str(e)}"

def create_error_handling_tool_node():
    """创建带错误处理的工具节点"""

    def safe_tool_execution(state: MyState) -> MyState:
        """安全的工具执行包装器"""
        try:
            # 执行工具调用
            tool_result = tool_node.invoke(state)
            return {**state, **tool_result}
        except Exception as e:
            # 工具执行失败的降级处理
            return {
                **state,
                "tool_error": str(e),
                "fallback_response": "工具暂时不可用，请稍后重试"
            }

    return safe_tool_execution
```

### 动态工具选择

```python
class DynamicToolSelector:
    def __init__(self):
        self.tools = {
            "math": calculator,
            "search": search_web,
            "file": save_file
        }

    def select_tools(self, intent: str) -> List:
        """根据意图选择合适的工具"""
        tool_mapping = {
            "calculate": ["math"],
            "search": ["search"],
            "save": ["file"],
            "general": ["math", "search", "file"]
        }

        selected_tool_names = tool_mapping.get(intent, ["general"])
        return [self.tools[name] for name in selected_tool_names if name in self.tools]

def intent_based_tool_node(state: MyState) -> MyState:
    """基于意图的工具选择节点"""
    user_intent = classify_intent(state["user_input"])

    selector = DynamicToolSelector()
    available_tools = selector.select_tools(user_intent)

    # 创建专门的工具节点
    specialized_tool_node = ToolNode(available_tools)

    # 执行工具调用
    return specialized_tool_node.invoke(state)

def classify_intent(user_input: str) -> str:
    """简单的意图分类"""
    if any(word in user_input.lower() for word in ["计算", "算", "数学"]):
        return "calculate"
    elif any(word in user_input.lower() for word in ["搜索", "查找", "找"]):
        return "search"
    elif any(word in user_input.lower() for word in ["保存", "存储", "写入"]):
        return "save"
    else:
        return "general"
```

---

## 🚀 部署配置模板

### Docker 配置

```dockerfile
# Dockerfile
FROM python:3.11-slim

WORKDIR /app

# 安装系统依赖
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/*

# 复制依赖文件
COPY requirements.txt .

# 安装 Python 依赖
RUN pip install --no-cache-dir -r requirements.txt

# 复制应用代码
COPY . .

# 设置环境变量
ENV PYTHONPATH=/app
ENV PYTHONUNBUFFERED=1

# 暴露端口
EXPOSE 8000

# 启动命令
CMD ["python", "-m", "langgraph_app.server"]
```

```yaml
# docker-compose.yml
version: '3.8'

services:
  langgraph-app:
    build: .
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - LANGGRAPH_LICENSE_KEY=${LANGGRAPH_LICENSE_KEY}
      - DATABASE_URL=postgresql://user:pass@postgres:5432/langgraph
    depends_on:
      - postgres
      - redis
    volumes:
      - ./data:/app/data

  postgres:
    image: postgres:15
    environment:
      - POSTGRES_DB=langgraph
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=pass
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

volumes:
  postgres_data:
```

### LangGraph 配置文件

```json
{
  "dependencies": [
    "langgraph",
    "langchain",
    "langchain-openai",
    "langchain-community"
  ],
  "graphs": {
    "my_agent": "./src/agent.py:graph"
  },
  "env": [
    "OPENAI_API_KEY",
    "LANGGRAPH_LICENSE_KEY"
  ],
  "python_version": "3.11",
  "pip_config_file": "./pip.conf",
  "dockerfile_lines": [
    "RUN apt-get update && apt-get install -y curl",
    "RUN pip install --upgrade pip"
  ]
}
```

### 环境配置模板

```bash
# .env.example
# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4

# LangGraph Platform
LANGGRAPH_LICENSE_KEY=your_license_key_here
LANGGRAPH_API_URL=https://api.langgraph.dev

# Database Configuration
DATABASE_URL=postgresql://user:password@localhost:5432/langgraph
REDIS_URL=redis://localhost:6379/0

# Application Settings
APP_NAME=my-langgraph-app
APP_VERSION=1.0.0
DEBUG=false
LOG_LEVEL=INFO

# Security
SECRET_KEY=your_secret_key_here
ALLOWED_HOSTS=localhost,127.0.0.1

# Feature Flags
ENABLE_HUMAN_IN_THE_LOOP=true
ENABLE_MEMORY=true
ENABLE_STREAMING=true
```

### Kubernetes 部署配置

```yaml
# k8s-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: langgraph-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: langgraph-app
  template:
    metadata:
      labels:
        app: langgraph-app
    spec:
      containers:
      - name: langgraph-app
        image: my-langgraph-app:latest
        ports:
        - containerPort: 8000
        env:
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: api-secrets
              key: openai-api-key
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: db-secrets
              key: database-url
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5

---
apiVersion: v1
kind: Service
metadata:
  name: langgraph-service
spec:
  selector:
    app: langgraph-app
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
  type: LoadBalancer
```

### 服务器启动脚本

```python
# server.py
import os
import uvicorn
from langgraph_app import create_app

def main():
    # 从环境变量获取配置
    host = os.getenv("HOST", "0.0.0.0")
    port = int(os.getenv("PORT", 8000))
    workers = int(os.getenv("WORKERS", 1))
    log_level = os.getenv("LOG_LEVEL", "info").lower()

    # 创建应用
    app = create_app()

    # 启动服务器
    uvicorn.run(
        app,
        host=host,
        port=port,
        workers=workers,
        log_level=log_level,
        access_log=True
    )

if __name__ == "__main__":
    main()
```

---

## 🔗 使用指南

### 复制代码模板

1. **选择合适的模板**：根据你的需求选择相应的代码片段
2. **修改配置**：根据你的具体情况修改参数和配置
3. **测试功能**：在小范围内测试代码片段的功能
4. **集成到项目**：将测试通过的代码集成到你的主项目中

### 最佳实践

- 📝 **文档化**：为每个自定义节点添加清晰的文档说明
- 🧪 **测试**：为关键节点编写单元测试
- 🔍 **监控**：添加适当的日志和监控
- 🛡️ **错误处理**：实现健壮的错误处理机制
- 🔄 **版本控制**：使用 Git 管理代码变更

---

## 🔗 相关资源

- 🚀 [API 速查表](./API速查.md)
- 🐛 [错误码对照](./错误码对照.md)
- ❓ [常见问题 FAQ](./FAQ.md)
- 📖 [LangGraph 官方文档](https://langchain-ai.github.io/langgraph/)
- 💻 [项目示例](../08-项目案例/)

---

*代码片段库会持续更新，欢迎贡献更多有用的模板！* 🤝